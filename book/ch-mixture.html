<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 19 Mixture models | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 19 Mixture models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 19 Mixture models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2022-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-MPT.html"/>
<link rel="next" href="ch-lognormalrace.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script>
// FOLD code from 
// https://github.com/bblodfon/rtemps/blob/master/docs/bookdown-lite/hide_code.html
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {

    // if code block has been labeled with class `fold-show`, show the code on init!
    var classList = $(this).attr('class').split(/\s+/);
    for (var i = 0; i < classList.length; i++) {
    if (classList[i] === 'fold-show') {
        show = true;
      }
    }

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // hack: return show to false, otherwise all next codeBlocks will be shown!
    show = false;

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science (DRAFT)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#developing-the-right-mindset-for-this-book"><i class="fa fa-check"></i>Developing the right mindset for this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-read-this-book-anyway"><i class="fa fa-check"></i>Why read this book anyway?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#some-conventions-used-in-this-book"><i class="fa fa-check"></i>Some conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-materials"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-needed"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#introprob"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#condprob"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec-binomialcloze"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the binomial distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch-intro.html"><a href="ch-intro.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch-intro.html"><a href="ch-intro.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#continuous-random-variables-an-example-using-the-normal-distribution"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the normal distribution</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ch-intro.html"><a href="ch-intro.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-intro.html"><a href="ch-intro.html#truncating-a-normal-distribution"><i class="fa fa-check"></i><b>1.5.2</b> Truncating a normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#bivariate-and-multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#sec-contbivar"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-intro.html"><a href="ch-intro.html#sec-generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-intro.html"><a href="ch-intro.html#sec-marginal"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="ch-intro.html"><a href="ch-intro.html#summary-of-useful-r-functions-relating-to-distributions"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="ch-intro.html"><a href="ch-intro.html#summary"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="ch-intro.html"><a href="ch-intro.html#further-reading"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="ch-intro.html"><a href="ch-intro.html#sec-Foundationsexercises"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#bayes-rule"><i class="fa fa-check"></i><b>2.1</b> Bayes’ rule</a></li>
<li class="chapter" data-level="2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-analytical"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayes’ rule: An analytical example</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-introBDA.html"><a href="ch-introBDA.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-introBDA.html"><a href="ch-introBDA.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-introBDA.html"><a href="ch-introBDA.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-introBDA.html"><a href="ch-introBDA.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="ch-introBDA.html"><a href="ch-introBDA.html#further-reading-1"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="ch-introBDA.html"><a href="ch-introBDA.html#sec-BDAexercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sampling"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="3.2" data-path="ch-compbda.html"><a href="ch-compbda.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.2</b> Bayesian Regression Models using Stan: brms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-simplenormal"><i class="fa fa-check"></i><b>3.2.1</b> A simple linear model: A single subject pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-priorpred"><i class="fa fa-check"></i><b>3.3</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.4" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-sensitivity"><i class="fa fa-check"></i><b>3.4</b> The influence of priors: sensitivity analysis</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ch-compbda.html"><a href="ch-compbda.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.4.1</b> Flat, uninformative priors</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-compbda.html"><a href="ch-compbda.html#regularizing-priors"><i class="fa fa-check"></i><b>3.4.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-compbda.html"><a href="ch-compbda.html#principled-priors"><i class="fa fa-check"></i><b>3.4.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-compbda.html"><a href="ch-compbda.html#informative-priors"><i class="fa fa-check"></i><b>3.4.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-revisit"><i class="fa fa-check"></i><b>3.5</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.6" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ppd"><i class="fa fa-check"></i><b>3.6</b> Posterior predictive distribution</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="ch-compbda.html"><a href="ch-compbda.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.6.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.6.2" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lnfirst"><i class="fa fa-check"></i><b>3.6.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.6.3" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-lognormal"><i class="fa fa-check"></i><b>3.6.3</b> Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="ch-compbda.html"><a href="ch-compbda.html#list-of-the-most-important-commands"><i class="fa fa-check"></i><b>3.7</b> List of the most important commands</a></li>
<li class="chapter" data-level="3.8" data-path="ch-compbda.html"><a href="ch-compbda.html#summary-2"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="ch-compbda.html"><a href="ch-compbda.html#sec-ch3furtherreading"><i class="fa fa-check"></i><b>3.9</b> Further reading</a></li>
<li class="chapter" data-level="3.10" data-path="ch-compbda.html"><a href="ch-compbda.html#ex:compbda"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupil"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-trial"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect response times?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-reg.html"><a href="ch-reg.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-reg.html"><a href="ch-reg.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.2.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-reg.html"><a href="ch-reg.html#sec-logistic"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-reg.html"><a href="ch-reg.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-reg.html"><a href="ch-reg.html#sec-priorslogisticregression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-reg.html"><a href="ch-reg.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-reg.html"><a href="ch-reg.html#sec-comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-reg.html"><a href="ch-reg.html#descriptive-adequacy-1"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-reg.html"><a href="ch-reg.html#summary-3"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="ch-reg.html"><a href="ch-reg.html#sec-ch4furtherreading"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="ch-reg.html"><a href="ch-reg.html#sec-LMexercises"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#exchangeability-and-hierarchical-models"><i class="fa fa-check"></i><b>5.1</b> Exchangeability and hierarchical models</a></li>
<li class="chapter" data-level="5.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-N400hierarchical"><i class="fa fa-check"></i><b>5.2</b> A hierarchical model with a normal likelihood: The N400 effect</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-Mcp"><i class="fa fa-check"></i><b>5.2.1</b> Complete pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.2.2</b> No pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-uncorrelated"><i class="fa fa-check"></i><b>5.2.3</b> Varying intercepts and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-mcvivs"><i class="fa fa-check"></i><b>5.2.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-sih"><i class="fa fa-check"></i><b>5.2.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-distrmodel"><i class="fa fa-check"></i><b>5.2.6</b> Beyond the maximal model–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-stroop"><i class="fa fa-check"></i><b>5.3</b> A hierarchical log-normal model: The Stroop effect</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.3.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort"><i class="fa fa-check"></i><b>5.4</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#further-reading-2"><i class="fa fa-check"></i><b>5.6</b> Further reading</a></li>
<li class="chapter" data-level="5.7" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html#sec-HLMexercises"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-priors.html"><a href="ch-priors.html#sec-simpleexamplepriors"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-priors.html"><a href="ch-priors.html#an-example-english-relative-clauses"><i class="fa fa-check"></i><b>6.1.1</b> An example: English relative clauses</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-intercept"><i class="fa fa-check"></i><b>6.1.2</b> Eliciting a prior for the intercept</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-a-prior-for-the-slope"><i class="fa fa-check"></i><b>6.1.3</b> Eliciting a prior for the slope</a></li>
<li class="chapter" data-level="6.1.4" data-path="ch-priors.html"><a href="ch-priors.html#sec-varcomppriors"><i class="fa fa-check"></i><b>6.1.4</b> Eliciting priors for the variance components</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-priors.html"><a href="ch-priors.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="ch-priors.html"><a href="ch-priors.html#deriving-priors-from-meta-analyses"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="ch-priors.html"><a href="ch-priors.html#using-previous-experiments-posteriors-as-priors-for-a-new-study"><i class="fa fa-check"></i><b>6.4</b> Using previous experiments’ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="ch-priors.html"><a href="ch-priors.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="ch-priors.html"><a href="ch-priors.html#further-reading-3"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-workflow.html"><a href="ch-workflow.html#model-building"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="ch-workflow.html"><a href="ch-workflow.html#principled-questions-on-a-model"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-workflow.html"><a href="ch-workflow.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-workflow.html"><a href="ch-workflow.html#exemplary-data-analysis"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ch-workflow.html"><a href="ch-workflow.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-workflow.html"><a href="ch-workflow.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-workflow.html"><a href="ch-workflow.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-workflow.html"><a href="ch-workflow.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-workflow.html"><a href="ch-workflow.html#summary-6"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="ch-workflow.html"><a href="ch-workflow.html#further-reading-4"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-contr.html"><a href="ch-contr.html#basic-concepts-illustrated-using-a-two-level-factor"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-contr.html"><a href="ch-contr.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-contr.html"><a href="ch-contr.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="ch-contr.html"><a href="ch-contr.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="ch-contr.html"><a href="ch-contr.html#sec-cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix-illustrated-with-a-three-level-factor"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-contr.html"><a href="ch-contr.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-contr.html"><a href="ch-contr.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-contr.html"><a href="ch-contr.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-contr.html"><a href="ch-contr.html#sec-4levelFactor"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ch-contr.html"><a href="ch-contr.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-contr.html"><a href="ch-contr.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-contr.html"><a href="ch-contr.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="ch-contr.html"><a href="ch-contr.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="ch-contr.html"><a href="ch-contr.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-contr.html"><a href="ch-contr.html#nonOrthogonal"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-contr.html"><a href="ch-contr.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-contr.html"><a href="ch-contr.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-contr.html"><a href="ch-contr.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-contr.html"><a href="ch-contr.html#computing-condition-means-from-estimated-contrasts"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="ch-contr.html"><a href="ch-contr.html#summary-7"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="ch-contr.html"><a href="ch-contr.html#further-reading-5"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="ch-contr.html"><a href="ch-contr.html#sec-Contrastsexercises"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-MR-ANOVA"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-contrast-covariate"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-interactions-NLM"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#further-reading-6"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html#sec-Contrasts2x2exercises"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-introstan.html"><a href="ch-introstan.html#stan-syntax"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-firststan"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-clozestan"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="ch-introstan.html"><a href="ch-introstan.html#regression-models-in-stan"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-introstan.html"><a href="ch-introstan.html#sec-logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-introstan.html"><a href="ch-introstan.html#summary-9"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="ch-introstan.html"><a href="ch-introstan.html#further-reading-7"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="ch-introstan.html"><a href="ch-introstan.html#exercises"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparametrization</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-hierstan"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-complexstan.html"><a href="ch-complexstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#sec-crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-complexstan.html"><a href="ch-complexstan.html#summary-10"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="ch-complexstan.html"><a href="ch-complexstan.html#further-reading-8"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="ch-complexstan.html"><a href="ch-complexstan.html#exercises-1"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-custom.html"><a href="ch-custom.html#sec-change"><i class="fa fa-check"></i><b>12.1</b> A change of variables with the reciprocal normal distribution</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-custom.html"><a href="ch-custom.html#scaling-a-probability-density-with-the-jacobian-adjustment"><i class="fa fa-check"></i><b>12.1.1</b> Scaling a probability density with the Jacobian adjustment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-custom.html"><a href="ch-custom.html#sec-validSBC"><i class="fa fa-check"></i><b>12.2</b> Validation of a computed posterior distribution</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-custom.html"><a href="ch-custom.html#the-simulation-based-calibration-procedure"><i class="fa fa-check"></i><b>12.2.1</b> The simulation-based calibration procedure</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-custom.html"><a href="ch-custom.html#simulation-based-calibration-revealing-a-problem"><i class="fa fa-check"></i><b>12.2.2</b> Simulation-based calibration revealing a problem</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-custom.html"><a href="ch-custom.html#issues-and-limitation-of-simulation-based-calibration"><i class="fa fa-check"></i><b>12.2.3</b> Issues and limitation of simulation-based calibration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-custom.html"><a href="ch-custom.html#a-custom-distribution-re-implementing-the-exponential-distribution-manually"><i class="fa fa-check"></i><b>12.3</b> A custom distribution: Re-implementing the exponential distribution manually</a></li>
<li class="chapter" data-level="12.4" data-path="ch-custom.html"><a href="ch-custom.html#summary-11"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
<li class="chapter" data-level="12.5" data-path="ch-custom.html"><a href="ch-custom.html#further-reading-9"><i class="fa fa-check"></i><b>12.5</b> Further reading</a></li>
<li class="chapter" data-level="12.6" data-path="ch-custom.html"><a href="ch-custom.html#sec-customexercises"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Other useful models</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-remame.html"><a href="ch-remame.html#meta-analysis"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-remame.html"><a href="ch-remame.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-remame.html"><a href="ch-remame.html#measurement-error-models"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-remame.html"><a href="ch-remame.html#accounting-for-measurement-error-in-individual-differences-in-working-memory-capacity-and-reading-fluency"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in individual differences in working memory capacity and reading fluency</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-remame.html"><a href="ch-remame.html#summary-12"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="ch-remame.html"><a href="ch-remame.html#further-reading-10"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="ch-remame.html"><a href="ch-remame.html#sec-REMAMEexercises"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="14" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>14</b> Introduction to model comparison</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-comparison.html"><a href="ch-comparison.html#further-reading-11"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>15</b> Bayes factors</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-bf.html"><a href="ch-bf.html#hypothesis-testing-using-the-bayes-factor"><i class="fa fa-check"></i><b>15.1</b> Hypothesis testing using the Bayes factor</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-bf.html"><a href="ch-bf.html#marginal-likelihood"><i class="fa fa-check"></i><b>15.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factor"><i class="fa fa-check"></i><b>15.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-N400BF"><i class="fa fa-check"></i><b>15.2</b> Examining the N400 effect with Bayes factor</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-bf.html"><a href="ch-bf.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>15.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFnonnested"><i class="fa fa-check"></i><b>15.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-bf.html"><a href="ch-bf.html#the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest"><i class="fa fa-check"></i><b>15.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="15.4" data-path="ch-bf.html"><a href="ch-bf.html#sec-stanBF"><i class="fa fa-check"></i><b>15.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="15.5" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-and-in-practice"><i class="fa fa-check"></i><b>15.5</b> Bayes factors in theory and in practice</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ch-bf.html"><a href="ch-bf.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>15.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="15.5.2" data-path="ch-bf.html"><a href="ch-bf.html#sec-BFvar"><i class="fa fa-check"></i><b>15.5.2</b> Bayes factors in practice: Variability with the data</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ch-bf.html"><a href="ch-bf.html#summary-13"><i class="fa fa-check"></i><b>15.6</b> Summary</a></li>
<li class="chapter" data-level="15.7" data-path="ch-bf.html"><a href="ch-bf.html#further-reading-12"><i class="fa fa-check"></i><b>15.7</b> Further reading</a></li>
<li class="chapter" data-level="15.8" data-path="ch-bf.html"><a href="ch-bf.html#exercises-2"><i class="fa fa-check"></i><b>15.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>16</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ch-cv.html"><a href="ch-cv.html#expected-log-predictive-density-of-a-model"><i class="fa fa-check"></i><b>16.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="16.2" data-path="ch-cv.html"><a href="ch-cv.html#k-fold-and-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="16.3" data-path="ch-cv.html"><a href="ch-cv.html#testing-the-n400-effect-using-cross-validation"><i class="fa fa-check"></i><b>16.3</b> Testing the N400 effect using cross-validation</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>16.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="16.3.2" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>16.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="16.3.3" data-path="ch-cv.html"><a href="ch-cv.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>16.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ch-cv.html"><a href="ch-cv.html#sec-logcv"><i class="fa fa-check"></i><b>16.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="16.5" data-path="ch-cv.html"><a href="ch-cv.html#issues-with-cross-validation"><i class="fa fa-check"></i><b>16.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="16.6" data-path="ch-cv.html"><a href="ch-cv.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>16.6</b> Cross-validation in Stan</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="ch-cv.html"><a href="ch-cv.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>16.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ch-cv.html"><a href="ch-cv.html#summary-14"><i class="fa fa-check"></i><b>16.7</b> Summary</a></li>
<li class="chapter" data-level="16.8" data-path="ch-cv.html"><a href="ch-cv.html#further-reading-13"><i class="fa fa-check"></i><b>16.8</b> Further reading</a></li>
<li class="chapter" data-level="16.9" data-path="ch-cv.html"><a href="ch-cv.html#exercises-3"><i class="fa fa-check"></i><b>16.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="17" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>17</b> Introduction to computational cognitive modeling</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ch-cogmod.html"><a href="ch-cogmod.html#further-reading-14"><i class="fa fa-check"></i><b>17.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>18</b> Multinomial processing trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-multiple-categorical-responses"><i class="fa fa-check"></i><b>18.1</b> Modeling multiple categorical responses</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mult"><i class="fa fa-check"></i><b>18.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="18.1.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-cat"><i class="fa fa-check"></i><b>18.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="ch-MPT.html"><a href="ch-MPT.html#modeling-picture-naming-abilities-in-aphasia-with-mpt-models"><i class="fa fa-check"></i><b>18.2</b> Modeling picture naming abilities in aphasia with MPT models</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="ch-MPT.html"><a href="ch-MPT.html#calculation-of-the-probabilities-in-the-mpt-branches"><i class="fa fa-check"></i><b>18.2.1</b> Calculation of the probabilities in the MPT branches</a></li>
<li class="chapter" data-level="18.2.2" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-mpt-data"><i class="fa fa-check"></i><b>18.2.2</b> A simple MPT model</a></li>
<li class="chapter" data-level="18.2.3" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-reg"><i class="fa fa-check"></i><b>18.2.3</b> An MPT assuming by-item variability</a></li>
<li class="chapter" data-level="18.2.4" data-path="ch-MPT.html"><a href="ch-MPT.html#sec-MPT-h"><i class="fa fa-check"></i><b>18.2.4</b> A hierarchical MPT</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="ch-MPT.html"><a href="ch-MPT.html#further-reading-15"><i class="fa fa-check"></i><b>18.3</b> Further reading</a></li>
<li class="chapter" data-level="18.4" data-path="ch-MPT.html"><a href="ch-MPT.html#exercises-4"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>19</b> Mixture models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ch-mixture.html"><a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account"><i class="fa fa-check"></i><b>19.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="ch-mixture.html"><a href="ch-mixture.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>19.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="19.1.2" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-simplefastguess"><i class="fa fa-check"></i><b>19.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.3" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-multmix"><i class="fa fa-check"></i><b>19.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="19.1.4" data-path="ch-mixture.html"><a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>19.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="19.1.5" data-path="ch-mixture.html"><a href="ch-mixture.html#sec-fastguessh"><i class="fa fa-check"></i><b>19.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ch-mixture.html"><a href="ch-mixture.html#summary-15"><i class="fa fa-check"></i><b>19.2</b> Summary</a></li>
<li class="chapter" data-level="19.3" data-path="ch-mixture.html"><a href="ch-mixture.html#further-reading-16"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="ch-mixture.html"><a href="ch-mixture.html#exercises-5"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>20</b> A simple accumulator model to account for choice response time</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#modeling-a-lexical-decision-task"><i class="fa fa-check"></i><b>20.1</b> Modeling a lexical decision task</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-acccoding"><i class="fa fa-check"></i><b>20.1.1</b> Modeling the lexical decision task with the log-normal race model</a></li>
<li class="chapter" data-level="20.1.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-genaccum"><i class="fa fa-check"></i><b>20.1.2</b> A generative model for a race between accumulators</a></li>
<li class="chapter" data-level="20.1.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#fitting-the-log-normal-race-model"><i class="fa fa-check"></i><b>20.1.3</b> Fitting the log-normal race model</a></li>
<li class="chapter" data-level="20.1.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-lognormalh"><i class="fa fa-check"></i><b>20.1.4</b> A hierarchical implementation of the log-normal race model</a></li>
<li class="chapter" data-level="20.1.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#sec-contaminant"><i class="fa fa-check"></i><b>20.1.5</b> Dealing with contaminant responses</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#posterior-predictive-check-with-the-quantile-probability-plots"><i class="fa fa-check"></i><b>20.2</b> Posterior predictive check with the quantile probability plots</a></li>
<li class="chapter" data-level="20.3" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#summary-16"><i class="fa fa-check"></i><b>20.3</b> Summary</a></li>
<li class="chapter" data-level="20.4" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#further-reading-17"><i class="fa fa-check"></i><b>20.4</b> Further reading</a></li>
<li class="chapter" data-level="20.5" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html#exercises-6"><i class="fa fa-check"></i><b>20.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-mixture" class="section level1 hasAnchor" number="19">
<h1><span class="header-section-number">Chapter 19</span> Mixture models<a href="ch-mixture.html#ch-mixture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Mixture models integrate multiple data generating processes into a single model. This is especially useful in cases where we cannot fully identify from the data alone which observations belong to which process.
Mixture models are important in cognitive science because many theories of cognition assume that the behavior of subjects in certain tasks is determined by an interplay of different cognitive processes <span class="citation">(e.g., response times in schizophrenia in <a href="#ref-levy1993eye" role="doc-biblioref">D. L. Levy et al. 1993</a>; retrieval from memory in sentence processing in <a href="#ref-Mcelree2000" role="doc-biblioref">McElree 2000</a>; <a href="#ref-nicenboimModelsRetrievalSentence2018" role="doc-biblioref">Nicenboim and Vasishth 2018</a>; fast choices in <a href="#ref-Ollman1966" role="doc-biblioref">Ollman 1966</a>; <a href="#ref-DutilhEtAl2011" role="doc-biblioref">Dutilh et al. 2011</a>)</span>. It is important to stress that a mixture distribution of observations is an <em>assumption</em> of the latent process developing trial by trial based on a given theory—it doesn’t necessarily represent the true generative process. The role of Bayesian modeling is to help us understand the extent to which this assumption is well-founded, by using posterior predictive checks and comparing different models.</p>
<p>We focus here on the case where we have only two components; each component represents a distinct cognitive process based on the domain knowledge of the researcher. The vector <span class="math inline">\(\mathbf{z}\)</span> serves as an <em>indicator variable</em> that indicates which of the mixture components an observation <span class="math inline">\(y_n\)</span> belongs to (<span class="math inline">\(n=1,\dots,N\)</span> is the number of data points). We assume two components, and thus each <span class="math inline">\(z_n\)</span> can be either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> (this will allows us to generate <span class="math inline">\(z_n\)</span> with a Bernoulli distribution). We also assume two different generative processes, <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span>, which generate different distributions of the observations based on a vector of parameters indicated by <span class="math inline">\(\Theta_{1}\)</span> and <span class="math inline">\(\Theta_{2}\)</span>, respectively. These two processes occur with probability <span class="math inline">\(\theta\)</span> and <span class="math inline">\(1-\theta\)</span>, and each observation is generated as follows:</p>
<p><span class="math display" id="eq:mixz">\[\begin{equation}
\begin{aligned}
z_n \sim \mathit{Bernoulli}(\theta)\\
y_n \sim
\begin{cases}
p_1(\Theta_1), &amp; \text{ if } z_n =1 \\
p_2(\Theta_2), &amp; \text{ if } z_n=0
\end{cases}
\end{aligned}
\tag{19.1}
\end{equation}\]</span></p>
<p>We focus on only two components because this type of models is already barely identifiable in most applied situations. However, if
the number of components in the mixture is finite and also determined by the researcher, the approach presented here can in principle be extended to any number of mixtures by replacing the Bernoulli distribution by a categorical one.</p>
<p>In order to fit this model, we need to estimate the posterior of each of the parameters contained in the vectors <span class="math inline">\(\Theta_{1}\)</span> and <span class="math inline">\(\Theta_{2}\)</span> (intercepts, slopes, group-level effects, etc.), the probability <span class="math inline">\(\theta\)</span>, and the indicator variable that corresponds to each observation <span class="math inline">\(z_n\)</span>. One issue that presents here is that <span class="math inline">\(z_n\)</span> must be a discrete parameter, and Stan only allows continuous parameters. This is because Stan’s algorithm requires the derivatives of the (log) posterior distribution with respect to all parameters, and discrete parameters are not differentiable (since they have “breaks”). In probabilistic programming languages like WinBUGS and JAGS <span class="citation">(<a href="#ref-lunn2012bugs" role="doc-biblioref">D. Lunn et al. 2012</a>; <a href="#ref-plummer2016jags" role="doc-biblioref">Plummer 2016</a>)</span>, discrete parameters are possible to use; but not in Stan. In Stan, we can circumvent this issue by marginalizing out the indicator variable <span class="math inline">\(z\)</span>.<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a> If <span class="math inline">\(p_1\)</span> appears in the mixture with probability <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(p_2\)</span> with probability <span class="math inline">\(1-\theta\)</span>, then the joint likelihood is defined as a function of <span class="math inline">\(\Theta\)</span> (which concatenates both <span class="math inline">\(\Theta_1\)</span> and <span class="math inline">\(\Theta_2\)</span>), and importantly <span class="math inline">\(z_n\)</span> “disappears”:</p>
<p><span class="math display">\[\begin{equation}
p(y_n | \Theta) = \theta \cdot p_1(y_n| \Theta_1) + (1-\theta) \cdot p_2(y_n | \Theta_2)
\end{equation}\]</span></p>
<!-- Marginalization is a special case of a more general technique often referred to as *Rao-Blackwellization*.  -->
<p>The intuition behind this formula is that each likelihood function, <span class="math inline">\(p_1\)</span>, <span class="math inline">\(p_2\)</span> is weighted by its probability of being the relevant generative process. For our purposes, it suffices to say that marginalization works; the reader interested in the mathematics behind marginalization is directed to the further reading section at the end of the chapter.<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a></p>
<p>Even though Stan cannot fit a model with the discrete indicator of the latent class <span class="math inline">\(\mathbf{z}\)</span> that we used in equation <a href="ch-mixture.html#eq:mixz">(19.1)</a>, this equation will prove very useful when we want to generate synthetic data.</p>
<!-- schiz data examples -->
<!-- http://www.stat.columbia.edu/~gelman/book/data/schiz.asc -->
<p>In the following sections, we model a well-known phenomenon (i.e., the speed-accuracy trade-off) assuming an underlying finite mixture process. We start from the verbal description of the model, and we then implement the model in Stan step by step.</p>
<div id="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account" class="section level2 hasAnchor" number="19.1">
<h2><span class="header-section-number">19.1</span> A mixture model of the speed-accuracy trade-off: The fast-guess model account<a href="ch-mixture.html#a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we are faced with multiple choices that require an immediate decision, we can speed up the decision at the expense of accuracy and become more accurate at the expense of speed; this is called the speed-accuracy trade-off <span class="citation">(<a href="#ref-wickelgren1977speed" role="doc-biblioref">Wickelgren 1977</a>)</span><!-- (Bogacz, Wagenmakers, Forstmann, & Nieuwenhuis, 2010; Schouten & Bekker, 1967; Wickelgren, 1977) -->. The most popular class of models that can incorporate both response times and accuracy, and give an account for the speed-accuracy trade-off is the class of sequential sampling models, which include the drift diffusion model <span class="citation">(<a href="#ref-Ratcliff1978" role="doc-biblioref">Ratcliff 1978</a>)</span>, the linear ballistic accumulator <span class="citation">(<a href="#ref-brownSimplestCompleteModel2008" role="doc-biblioref">S. D. Brown and Heathcote 2008</a>)</span>, and the log-normal race model <span class="citation">(<a href="#ref-HeathcoteLove2012" role="doc-biblioref">Heathcote and Love 2012</a>; <a href="#ref-RouderEtAl2015" role="doc-biblioref">Rouder et al. 2015</a>)</span>, which we discuss in chapter <a href="ch-lognormalrace.html#ch-lognormalrace">20</a>; for a review see <span class="citation">Ratcliff et al. (<a href="#ref-Ratcliff2016" role="doc-biblioref">2016</a>)</span>.</p>
<p>However, an alternative model that has been proposed in the past is Ollman’s simple fast-guess model <span class="citation">(<a href="#ref-Ollman1966" role="doc-biblioref">Ollman 1966</a>; <a href="#ref-Yellott1967" role="doc-biblioref">Yellott 1967</a>, <a href="#ref-Yellott1971" role="doc-biblioref">1971</a>)</span>.<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a> Although it has mostly fallen out of favor <span class="citation">(but see <a href="#ref-DutilhEtAl2011" role="doc-biblioref">Dutilh et al. 2011</a> for a more modern variant of this model)</span>, it presents a very simple framework using finite mixture modeling that can also account for the speed-accuracy trade-off. In the next sections, we’ll use this model to exemplify the use of finite mixtures to represent different cognitive processes.</p>
<div id="the-global-motion-detection-task" class="section level3 hasAnchor" number="19.1.1">
<h3><span class="header-section-number">19.1.1</span> The global motion detection task<a href="ch-mixture.html#the-global-motion-detection-task" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One way to examine the behavior of human and primate subjects when faced with two-alternative forced choices is the detection of the global motion of a random dot kinematogram <span class="citation">(<a href="#ref-britten_shadlen_newsome_movshon_1993" role="doc-biblioref">Britten et al. 1993</a>)</span>. In this task, a subject sees a number of random dots on the screen from which a proportion of them move in a single direction (e.g., right) and the rest move in random directions. The subject’s goal is to estimate the overall direction of the movement. One of the reasons for the popularity of this task is that it permits the fine-tuning of the difficulty of trials <span class="citation">(<a href="#ref-Dutilh2019quality" role="doc-biblioref">Dutilh et al. 2019</a>)</span>: The task is harder when the proportion of dots that move coherently (the level of <em>coherence</em>) is lower; see Figure <a href="ch-mixture.html#fig:globalmotionpng">19.1</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:globalmotionpng"></span>
<img src="cc_figure/globalmotion_lr.PNG" alt="Three levels of difficulty of the global motion detection task. The figures show a consistent movement to the right with three levels of coherence (10%, 50%, and 100%). The subjects see the dots moving in the direction indicated by the arrows. The subjects do not see the arrows and all the dots look identical in the actual task. Adapted from Han et al. (2018); licensed under CC BY 4.0." width="100%" />
<p class="caption">
FIGURE 19.1: Three levels of difficulty of the global motion detection task. The figures show a consistent movement to the right with three levels of coherence (10%, 50%, and 100%). The subjects see the dots moving in the direction indicated by the arrows. The subjects do not see the arrows and all the dots look identical in the actual task. Adapted from <span class="citation">Han et al. (<a href="#ref-DingEtAl" role="doc-biblioref">2018</a>)</span>; licensed under CC BY 4.0.
</p>
</div>
<p>Ollman’s <span class="citation">(<a href="#ref-Ollman1966" role="doc-biblioref">1966</a>)</span> fast-guess model assumes that the behavior in this task (and in any other choice task) is governed by two distinct cognitive processes: (i) a guessing mode, and (ii) a task-engaged mode. In the guessing mode, responses are fast and accuracy is at chance level. In the task-engaged mode, responses are slower and accuracy approaches 100%. This means that intermediate values of response times and accuracy can only be achieved by mixing responses from the two modes. Further assumptions of this model are that response times depend on the difficulty of the choice, and that the probability of being on one of the two states depend on the speed incentives during the instructions. (To simplify matters, we ignore the possibility of the accuracy of the choice being also affected by the difficulty of the choice. Also, we ignore the possibility that subjects might be biased to one specific response in the guessing mode, but see exercise <a href="ch-mixture.html#exr:mixbias">19.3</a>.)</p>
<div id="data-set" class="section level4 hasAnchor" number="19.1.1.1">
<h4><span class="header-section-number">19.1.1.1</span> Data set<a href="ch-mixture.html#data-set" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We implement the assumptions behind Ollman’s fast-guess model and examine its fit to data of a global motion detection task from <span class="citation">Dutilh et al. (<a href="#ref-Dutilh2019quality" role="doc-biblioref">2019</a>)</span>.</p>
<p>The data set from <span class="citation">Dutilh et al. (<a href="#ref-Dutilh2019quality" role="doc-biblioref">2019</a>)</span> contains ~2800 trials of each of the 20 subjects participating in a global motion detection task and can be found as <code>df_dots</code> in the <code>bcogsci</code> package. There were two level of coherence yielding hard and easy trials (<code>diff</code>), and the trials where done under instructions that emphasized either accuracy or speed (<code>emphasis</code>). More information about the data set can be found by accessing the documentation for the data set (by typing <code>?df_dots</code> in the R console).</p>
<div class="sourceCode" id="cb1073"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1073-1"><a href="ch-mixture.html#cb1073-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;df_dots&quot;</span>) </span>
<span id="cb1073-2"><a href="ch-mixture.html#cb1073-2" aria-hidden="true" tabindex="-1"></a>df_dots</span></code></pre></div>
<pre><code>## # A tibble: 56,097 × 12
##    subj diff  emphasis    rt   acc fix_dur stim  resp  trial block
##   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;
## 1     1 easy  speed      482     1   0.738 R     R         1     6
## 2     1 hard  speed      602     1   0.784 R     R         2     6
## 3     1 hard  speed      381     1   0.651 R     R         3     6
##   block_trial bias 
##         &lt;int&gt; &lt;chr&gt;
## 1           1 no   
## 2           2 no   
## 3           3 no   
## # … with 56,094 more rows</code></pre>
<p>We might think that if the fast-guess model were true, we would see a bimodal distribution, when we plot a histogram of the data. Unfortunately, when two similar distributions are mixed, we won’t see any apparent bimodality; see Figure <a href="ch-mixture.html#fig:dfdots">19.2</a>.</p>

<div class="sourceCode" id="cb1075"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb1075-1"><a href="ch-mixture.html#cb1075-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1075-2"><a href="ch-mixture.html#cb1075-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_dots, <span class="fu">aes</span>(rt)) <span class="sc">+</span></span>
<span id="cb1075-3"><a href="ch-mixture.html#cb1075-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:dfdots"></span>
<img src="bookdown_files/figure-html/dfdots-1.svg" alt="Distribution of response times in the data of the global motion detection task in Dutilh et al. (2019)." width="672" />
<p class="caption">
FIGURE 19.2: Distribution of response times in the data of the global motion detection task in <span class="citation">Dutilh et al. (<a href="#ref-Dutilh2019quality" role="doc-biblioref">2019</a>)</span>.
</p>
</div>
<p>However, Figure <a href="ch-mixture.html#fig:dfaccrt">19.3</a> reveals that incorrect responses are generally faster, and this is especially true when the instructions emphasized accuracy.</p>

<div class="sourceCode" id="cb1076"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb1076-1"><a href="ch-mixture.html#cb1076-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1076-2"><a href="ch-mixture.html#cb1076-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_dots, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(acc), <span class="at">y =</span> rt)) <span class="sc">+</span></span>
<span id="cb1076-3"><a href="ch-mixture.html#cb1076-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">position =</span> <span class="fu">position_jitter</span>(<span class="at">width =</span> .<span class="dv">4</span>, <span class="at">height =</span> <span class="dv">0</span>),</span>
<span id="cb1076-4"><a href="ch-mixture.html#cb1076-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">alpha =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb1076-5"><a href="ch-mixture.html#cb1076-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(diff <span class="sc">~</span> emphasis) <span class="sc">+</span></span>
<span id="cb1076-6"><a href="ch-mixture.html#cb1076-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="sc">+</span></span>
<span id="cb1076-7"><a href="ch-mixture.html#cb1076-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:dfaccrt"></span>
<img src="bookdown_files/figure-html/dfaccrt-1.svg" alt="Distribution of response times by accuracy in the data of the global motion detection task in Dutilh et al. (2019)." width="672" />
<p class="caption">
FIGURE 19.3: Distribution of response times by accuracy in the data of the global motion detection task in <span class="citation">Dutilh et al. (<a href="#ref-Dutilh2019quality" role="doc-biblioref">2019</a>)</span>.
</p>
</div>
<!-- using a plotting style -->
<!-- that has become important in RT research, -->
<!-- called a “latency-probability” plot (LP plot: -->
<!-- Audley & Pike, 1965). Latency probability -->
<!-- plots show mean RT as a function of the -->
<!-- probability of a response. Points on the left -->
<!-- of the graph represent the lower probability -->
<!-- (error) responses and complementary points -->
<!-- on the right of the graph represent the higher -->
<!-- probability (correct) responses from the same -->
<!-- experimental conditions. Sometimes, LP -->
<!-- plots are expanded to show more than just -->
<!-- the mean RT, by plotting several quantiles -->
<!-- of the RT distributions–these are called -->
<!-- “quantile-probability,” or QP, plots. chapter -->
</div>
</div>
<div id="sec-simplefastguess" class="section level3 hasAnchor" number="19.1.2">
<h3><span class="header-section-number">19.1.2</span> A very simple implementation of the fast-guess model<a href="ch-mixture.html#sec-simplefastguess" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The description of the model makes it clear that an ideal subject who never guesses has a response time that depends only on the difficulty of the trial. As we did in previous chapters, we assume that response times are log-normally distributed, and for simplicity we start by modeling the behavior of a single subject:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\alpha + \beta \cdot x_n, \sigma)
\end{equation}\]</span></p>
<p>In the previous equation, <span class="math inline">\(x\)</span> is larger for difficult trials. If we center <span class="math inline">\(x\)</span>, <span class="math inline">\(\alpha\)</span> represents the average logarithmic transformed response times for a subject engaged in the task, and <span class="math inline">\(\beta\)</span> is the effect of trial difficulty on log-response time. We assume a non-deterministic process, with a noise parameter <span class="math inline">\(\sigma\)</span>. See also Box <a href="ch-reg.html#thm:lognormal">4.3</a> for more information about log-normally distributed response times.</p>
<p>Alternatively, a subject that guesses in every trial would show a response time distribution that is independent of the difficulty of the trial:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim \mathit{LogNormal}(\gamma, \sigma_2)
\end{equation}\]</span></p>
<p>Here <span class="math inline">\(\gamma\)</span> represents the the average logarithmic transformed response time when a subject only guesses. We assume that responses from the guessing mode might have a different noise component than from the task-engaged mode.</p>
<p>The fast-guess model makes the assumption that during a task, a single subject would behave in these two ways: They would be engaged in the task a proportion of the trials and would guess on the rest of the trials. This means that for a single subject, there is an underlying probability of being engaged in the task, <span class="math inline">\(p_{task}\)</span>, that determines whether they are actually choosing (<span class="math inline">\(z=1\)</span>) or guessing (<span class="math inline">\(z=0\)</span>):</p>
<p><span class="math display">\[\begin{equation}
z_n \sim \mathit{Bernoulli}(p_{task})
\end{equation}\]</span></p>
<p>The value of the parameter <span class="math inline">\(z\)</span> in every trial determines the behavior of the subject. This means that the distribution that we observe is a mixture of the two distributions presented before:</p>
<p><span class="math display" id="eq:dismix">\[\begin{equation}
rt_n \sim
\begin{cases}
\mathit{LogNormal}(\alpha + \beta \cdot x_n, \sigma), &amp; \text{ if } z_n =1 \\
\mathit{LogNormal}(\gamma, \sigma_2), &amp; \text{ if } z_n=0
\end{cases}
\tag{19.2}
\end{equation}\]</span></p>
<p>In order to have a Bayesian implementation, we also need to define some priors. We use priors that encode what we know about reaction time experiments. These priors are slightly more informative than the ones that we used in section <a href="ch-reg.html#sec-trial">4.2</a>, but they still can be considered regularizing priors. One can verify this by performing prior predictive checks. As we increase the complexity of our models, it’s worth to spend some time designing more realistic priors. These will speed up computation and in some cases they will be crucial to solve convergence problems.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim \mathit{Normal}(6, 1)\\
\beta &amp;\sim \mathit{Normal}(0, .1)\\
\sigma &amp;\sim \mathit{Normal}_+(.5, .2)
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\gamma &amp;\sim \mathit{Normal}(6, 1)\\
\sigma_2 &amp;\sim \mathit{Normal}_+(.5, .2)
\end{aligned}
\end{equation}\]</span></p>
<p>For now, we allow all values for the probability of having an engaged response having equal likelihood; we achieve this by setting the following prior to <span class="math inline">\(p_{task}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
p_{task} \sim \mathit{Beta}(1, 1)
\end{equation}\]</span></p>
<p>This represents a flat, uniformative prior over the probability parameter <span class="math inline">\(p_{task}\)</span>.</p>
<p>Before we fit our model to the real data, we generate synthetic data to make sure that our model is working as expected.</p>
<p>We first define the number of observations, predictors, and fixed point values for each of the parameters. We assume <span class="math inline">\(1000\)</span> observations and two levels of difficulty <span class="math inline">\(-0.5\)</span> and <span class="math inline">\(0.5\)</span>. The point values chosen for the parameters are relatively realistic (based on our previous experience on reaction time experiments). Although in the priors we try to encode the range of possible values for the parameters, in this simulation we assume only one instance of this possible range:</p>
<div class="sourceCode" id="cb1077"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1077-1"><a href="ch-mixture.html#cb1077-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1077-2"><a href="ch-mixture.html#cb1077-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span>.<span class="dv">5</span>, N<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(.<span class="dv">5</span>, N<span class="sc">/</span><span class="dv">2</span>)) <span class="co"># level of difficulty</span></span>
<span id="cb1077-3"><a href="ch-mixture.html#cb1077-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters true values</span></span>
<span id="cb1077-4"><a href="ch-mixture.html#cb1077-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">5.8</span></span>
<span id="cb1077-5"><a href="ch-mixture.html#cb1077-5" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb1077-6"><a href="ch-mixture.html#cb1077-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> .<span class="dv">4</span></span>
<span id="cb1077-7"><a href="ch-mixture.html#cb1077-7" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb1077-8"><a href="ch-mixture.html#cb1077-8" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">5.2</span></span>
<span id="cb1077-9"><a href="ch-mixture.html#cb1077-9" aria-hidden="true" tabindex="-1"></a>p_task <span class="ot">&lt;-</span> .<span class="dv">8</span></span>
<span id="cb1077-10"><a href="ch-mixture.html#cb1077-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Median time</span></span>
<span id="cb1077-11"><a href="ch-mixture.html#cb1077-11" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="st">&quot;engaged&quot;</span> <span class="ot">=</span> <span class="fu">exp</span>(alpha), <span class="st">&quot;guessing&quot;</span> <span class="ot">=</span> <span class="fu">exp</span>(gamma))</span></code></pre></div>
<pre><code>##  engaged guessing 
##      330      181</code></pre>
<p>For generate a mixture of response times, we use the indicator of a latent class, <code>z</code>.</p>
<div class="sourceCode" id="cb1079"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1079-1"><a href="ch-mixture.html#cb1079-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(<span class="at">n =</span> N, <span class="at">prob =</span> p_task)</span>
<span id="cb1079-2"><a href="ch-mixture.html#cb1079-2" aria-hidden="true" tabindex="-1"></a>rt <span class="ot">&lt;-</span> <span class="fu">if_else</span>(z <span class="sc">==</span> <span class="dv">1</span>,</span>
<span id="cb1079-3"><a href="ch-mixture.html#cb1079-3" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N,</span>
<span id="cb1079-4"><a href="ch-mixture.html#cb1079-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">meanlog =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x,</span>
<span id="cb1079-5"><a href="ch-mixture.html#cb1079-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sdlog =</span> sigma),</span>
<span id="cb1079-6"><a href="ch-mixture.html#cb1079-6" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N, </span>
<span id="cb1079-7"><a href="ch-mixture.html#cb1079-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">meanlog =</span> gamma,</span>
<span id="cb1079-8"><a href="ch-mixture.html#cb1079-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sdlog =</span> sigma2))</span>
<span id="cb1079-9"><a href="ch-mixture.html#cb1079-9" aria-hidden="true" tabindex="-1"></a>df_dots_simdata1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">trial =</span> <span class="dv">1</span><span class="sc">:</span>N, <span class="at">x =</span> x, <span class="at">rt =</span> rt)</span></code></pre></div>
<p>We verify that our simulated data is realistic, that is, it’s on the same range as the original data; see Figure <a href="ch-mixture.html#fig:simmix">19.4</a>.</p>

<div class="sourceCode" id="cb1080"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1080-1"><a href="ch-mixture.html#cb1080-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_dots_simdata1, <span class="fu">aes</span>(rt)) <span class="sc">+</span></span>
<span id="cb1080-2"><a href="ch-mixture.html#cb1080-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:simmix"></span>
<img src="bookdown_files/figure-html/simmix-1.svg" alt="Response times in the simulated data (df_dots_simdata1) that follows the fast-guess model." width="672" />
<p class="caption">
FIGURE 19.4: Response times in the simulated data (<code>df_dots_simdata1</code>) that follows the fast-guess model.
</p>
</div>
<p>To implement the mixture model defined in Equation <a href="ch-compbda.html#eq:logpriorsunif">(3.10)</a> in Stan, the discrete parameter <span class="math inline">\(z\)</span> needs to be marginalized:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(rt_n | \Theta) &amp;= p_{task} \cdot LogNormal(rt_n | \alpha + \beta \cdot x_n, \sigma) +\\
    &amp; (1 - p_{task}) \cdot LogNormal(rt_n | \gamma, \sigma_2)
\end{aligned}
\end{equation}\]</span></p>
<p>In addition, Stan requires the likelihood to be defined in log-space:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\log(p(rt | \Theta)) &amp;= \log(p_{task} \cdot LogNormal(rt_n | \alpha + \beta \cdot x_n, \sigma) +\\
    &amp; (1 - p_{task}) \cdot LogNormal(rt_n | \gamma, \sigma_2))
\end{aligned}
\end{equation}\]</span></p>
<p>A “naive” implementation in Stan would look like the following (recall that <code>_lpdf</code> functions provide log-transformed densities)</p>
<pre><code>target += log(
        p_task * exp(lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma)) +
              (1-p_task) * exp(lognormal_lpdf(rt[n] | gamma, sigma2)));</code></pre>
<p>However, we need to take into account that <span class="math inline">\(log(A \pm B)\)</span> can be numerically unstable (i.e., prone to under/overflow), when <span class="math inline">\(A\)</span> is much larger than <span class="math inline">\(B\)</span> or vice-versa. Stan provides several functions to deal with different special cases of logarithms of sums and differences. Here we need <code>log_sum_exp(x, y)</code> that corresponds to <code>log(exp(x) + exp(y))</code> and <code>log1m(x)</code> that corresponds to <code>log(1-x)</code>.</p>
<p>First, we need to take into account that the first summand of the logarithm, <code>p_task * exp(lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma))</code> corresponds to <code>exp(x)</code>, and the second one, <code>(1-p_task) * exp(lognormal_lpdf(rt[n] | gamma, sigma2))</code> to <code>exp(y)</code> in <code>log_sum_exp(x, y)</code>. This means that we need to first apply the logarithm to each of them to use them as arguments of <code>log_sum_exp(x, y)</code>:</p>
<pre><code>target += log_sum_exp(
              log(p_task) + lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma),
              log(1-p_task) + lognormal_lpdf(rt[n] | gamma, sigma2));</code></pre>
<p>Now we can just replace <code>log(1-p_task)</code> by the more stable <code>log1m(p_task)</code>:</p>
<pre><code>target += log_sum_exp(
               log(p_task) + lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma),
               log1m(p_task) + lognormal_lpdf(rt[n] | gamma, sigma2));</code></pre>
<p>The complete model (<code>mixture_rt.stan</code>) is shown below:</p>
<div class="sourceCode" id="cb1084"><pre class="sourceCode stan fold-show"><code class="sourceCode stan"><span id="cb1084-1"><a href="ch-mixture.html#cb1084-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1084-2"><a href="ch-mixture.html#cb1084-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N;</span>
<span id="cb1084-3"><a href="ch-mixture.html#cb1084-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb1084-4"><a href="ch-mixture.html#cb1084-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] rt;</span>
<span id="cb1084-5"><a href="ch-mixture.html#cb1084-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1084-6"><a href="ch-mixture.html#cb1084-6" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1084-7"><a href="ch-mixture.html#cb1084-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb1084-8"><a href="ch-mixture.html#cb1084-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb1084-9"><a href="ch-mixture.html#cb1084-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma;</span>
<span id="cb1084-10"><a href="ch-mixture.html#cb1084-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> gamma; <span class="co">//guessing</span></span>
<span id="cb1084-11"><a href="ch-mixture.html#cb1084-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma2;</span>
<span id="cb1084-12"><a href="ch-mixture.html#cb1084-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_task;</span>
<span id="cb1084-13"><a href="ch-mixture.html#cb1084-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1084-14"><a href="ch-mixture.html#cb1084-14" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1084-15"><a href="ch-mixture.html#cb1084-15" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the task component</span></span>
<span id="cb1084-16"><a href="ch-mixture.html#cb1084-16" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1084-17"><a href="ch-mixture.html#cb1084-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta | <span class="dv">0</span>, .<span class="dv">1</span>);</span>
<span id="cb1084-18"><a href="ch-mixture.html#cb1084-18" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1084-19"><a href="ch-mixture.html#cb1084-19" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1084-20"><a href="ch-mixture.html#cb1084-20" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the guessing component</span></span>
<span id="cb1084-21"><a href="ch-mixture.html#cb1084-21" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(gamma | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1084-22"><a href="ch-mixture.html#cb1084-22" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma2 | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1084-23"><a href="ch-mixture.html#cb1084-23" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1084-24"><a href="ch-mixture.html#cb1084-24" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_task | <span class="dv">1</span>, <span class="dv">1</span>);</span>
<span id="cb1084-25"><a href="ch-mixture.html#cb1084-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N)</span>
<span id="cb1084-26"><a href="ch-mixture.html#cb1084-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">target +=</span> log_sum_exp(log(p_task) +</span>
<span id="cb1084-27"><a href="ch-mixture.html#cb1084-27" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma),</span>
<span id="cb1084-28"><a href="ch-mixture.html#cb1084-28" aria-hidden="true" tabindex="-1"></a>                          log1m(p_task) +</span>
<span id="cb1084-29"><a href="ch-mixture.html#cb1084-29" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | gamma, sigma2));</span>
<span id="cb1084-30"><a href="ch-mixture.html#cb1084-30" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Call the Stan model <code>mixture_rt.stan</code>, and fit it to the simulated data. First, we set up the simulated data as a list structure:</p>
<div class="sourceCode" id="cb1085"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1085-1"><a href="ch-mixture.html#cb1085-1" aria-hidden="true" tabindex="-1"></a>ls_dots_simdata <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">N =</span> N, <span class="at">rt =</span> rt, <span class="at">x =</span> x) </span></code></pre></div>
<p>Then fit the model:</p>
<!-- eval = !file.exists("dataR/fit_mix_rt.RDS") -->
<div class="sourceCode" id="cb1086"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1086-1"><a href="ch-mixture.html#cb1086-1" aria-hidden="true" tabindex="-1"></a>mixture_rt <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb1086-2"><a href="ch-mixture.html#cb1086-2" aria-hidden="true" tabindex="-1"></a>                        <span class="st">&quot;mixture_rt.stan&quot;</span>,</span>
<span id="cb1086-3"><a href="ch-mixture.html#cb1086-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb1086-4"><a href="ch-mixture.html#cb1086-4" aria-hidden="true" tabindex="-1"></a>fit_mix_rt <span class="ot">&lt;-</span> <span class="fu">stan</span>(mixture_rt, <span class="at">data =</span> ls_dots_simdata)   </span></code></pre></div>
<pre><code>## Warning: The largest R-hat is 1.74, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## https://mc-stan.org/misc/warnings.html#r-hat</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>There are a lot of warnings, the Rhats are too large, and number of effective samples is too low:</p>
<div class="sourceCode" id="cb1090"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1090-1"><a href="ch-mixture.html#cb1090-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_mix_rt) </span></code></pre></div>
<pre><code>##            mean     2.5%    97.5% n_eff Rhat
## alpha      5.61     4.94     5.94     3 1.89
## beta       0.06    -0.07     0.18    18 1.08
## sigma      0.44     0.27     0.59     3 1.74
## gamma      5.64     5.08     5.93     3 1.86
## sigma2     0.46     0.33     0.58     3 1.93
## p_task     0.45     0.08     0.87    10 1.15
## lp__   -6385.06 -6390.07 -6382.02    12 1.12</code></pre>
<p>The traceplots show clearly that the chains aren’t mixing; see Figure <a href="ch-mixture.html#fig:traceplotsmix1">19.5</a>.</p>

<div class="sourceCode" id="cb1092"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1092-1"><a href="ch-mixture.html#cb1092-1" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(fit_mix_rt) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceplotsmix1"></span>
<img src="bookdown_files/figure-html/traceplotsmix1-1.svg" alt="Traceplots from the model mixture_rt.stan fit to simulated data." width="672" />
<p class="caption">
FIGURE 19.5: Traceplots from the model <code>mixture_rt.stan</code> fit to simulated data.
</p>
</div>
<p>The problem with this model is that the mixture components (i.e., the fast-guesses and the engaged mode) are underlyingly exchangeable and thus not identifiable. Each chain doesn’t know how each component was identified by the rest of the chains. However, we do have information that can identify the components: According to the theoretical model, we know that the average response in the engaged mode, represented by <span class="math inline">\(\alpha\)</span>, should be slower than the average response in the guessing mode, <span class="math inline">\(\gamma\)</span>.</p>
<p>Even though the theoretical model assumes that guesses are faster than engaged responses, this is not explicit in our computational model. That is, our model lacks some of the theoretical information that we have, namely that the distribution of engaged response times should be slower than the distribution of guesses times. This can be encoded with a strong prior for <span class="math inline">\(\gamma\)</span>, where we assume that its prior distribution is truncated on an upper bound by the value of <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\gamma \sim \mathit{Normal}(6, 1), \text{for } \gamma &lt; \alpha
\end{equation}\]</span></p>
<p>This would be enough to make the model identifiable.</p>
<p>Another softer constraint that we could add to our implementation is the assumption that subjects are generally more likely to be trying to do the task than just guessing. If this assumption is correct, we also improve the accuracy of our estimation of the posterior of the model. (The opposite is also true: If subjects are not trying to do the task, this assumption will be unwarranted and our prior information will lead us farther from to the “true” values of the parameters). The following prior has the probability density concentrated nearer to <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[\begin{equation}
p_{task} \sim \mathit{Beta}(8, 2)
\end{equation}\]</span></p>
<p>Plotting this prior confirms where most of the probability mass lies; see Figure <a href="ch-mixture.html#fig:dbeta">19.6</a>.</p>

<div class="sourceCode" id="cb1093"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb1093-1"><a href="ch-mixture.html#cb1093-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1093-2"><a href="ch-mixture.html#cb1093-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="cf">function</span>(x) <span class="fu">dbeta</span>(x, <span class="dv">8</span>, <span class="dv">2</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:dbeta"></span>
<img src="bookdown_files/figure-html/dbeta-1.svg" alt="Density plot for the prior of \(p_{task}\), \(Beta(8,2)\)" width="672" />
<p class="caption">
FIGURE 19.6: Density plot for the prior of <span class="math inline">\(p_{task}\)</span>, <span class="math inline">\(Beta(8,2)\)</span>
</p>
</div>
<p>The Stan code for this model is shown below as <code>mixture_rt2.stan</code>.</p>
<div class="sourceCode" id="cb1094"><pre class="sourceCode stan fold-show"><code class="sourceCode stan"><span id="cb1094-1"><a href="ch-mixture.html#cb1094-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1094-2"><a href="ch-mixture.html#cb1094-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N;</span>
<span id="cb1094-3"><a href="ch-mixture.html#cb1094-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb1094-4"><a href="ch-mixture.html#cb1094-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] rt;</span>
<span id="cb1094-5"><a href="ch-mixture.html#cb1094-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1094-6"><a href="ch-mixture.html#cb1094-6" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1094-7"><a href="ch-mixture.html#cb1094-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb1094-8"><a href="ch-mixture.html#cb1094-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb1094-9"><a href="ch-mixture.html#cb1094-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma;</span>
<span id="cb1094-10"><a href="ch-mixture.html#cb1094-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">upper</span> = alpha&gt; gamma; <span class="co">//guessing</span></span>
<span id="cb1094-11"><a href="ch-mixture.html#cb1094-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma2;</span>
<span id="cb1094-12"><a href="ch-mixture.html#cb1094-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_task;</span>
<span id="cb1094-13"><a href="ch-mixture.html#cb1094-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1094-14"><a href="ch-mixture.html#cb1094-14" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1094-15"><a href="ch-mixture.html#cb1094-15" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the task component</span></span>
<span id="cb1094-16"><a href="ch-mixture.html#cb1094-16" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1094-17"><a href="ch-mixture.html#cb1094-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta | <span class="dv">0</span>, .<span class="dv">3</span>);</span>
<span id="cb1094-18"><a href="ch-mixture.html#cb1094-18" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1094-19"><a href="ch-mixture.html#cb1094-19" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1094-20"><a href="ch-mixture.html#cb1094-20" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the guessing component</span></span>
<span id="cb1094-21"><a href="ch-mixture.html#cb1094-21" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(gamma | <span class="dv">6</span>, <span class="dv">1</span>) -</span>
<span id="cb1094-22"><a href="ch-mixture.html#cb1094-22" aria-hidden="true" tabindex="-1"></a>    normal_lcdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1094-23"><a href="ch-mixture.html#cb1094-23" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma2 | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1094-24"><a href="ch-mixture.html#cb1094-24" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1094-25"><a href="ch-mixture.html#cb1094-25" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_task | <span class="dv">8</span>, <span class="dv">2</span>);</span>
<span id="cb1094-26"><a href="ch-mixture.html#cb1094-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N)</span>
<span id="cb1094-27"><a href="ch-mixture.html#cb1094-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">target +=</span> log_sum_exp(log(p_task) +</span>
<span id="cb1094-28"><a href="ch-mixture.html#cb1094-28" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma),</span>
<span id="cb1094-29"><a href="ch-mixture.html#cb1094-29" aria-hidden="true" tabindex="-1"></a>                          log1m(p_task) +</span>
<span id="cb1094-30"><a href="ch-mixture.html#cb1094-30" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | gamma, sigma2)) ;</span>
<span id="cb1094-31"><a href="ch-mixture.html#cb1094-31" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Once we change the higher boundary of <code>gamma</code> in the <code>parameters</code> block, we also need to truncate the distribution in the <code>model</code> block by correcting the PDF with its CDF. This correction is carried out using the CDF because we are truncating the distribution at the right-hand side; recall that earlier we used the complement of the CDF when we truncate a distribution at the left-hand side); also see Box <a href="ch-reg.html#thm:truncation">4.1</a>.</p>
<pre><code>  target += normal_lpdf(gamma | 6, 1) -
    normal_lcdf(alpha | 6, 1);</code></pre>
<p>Fit this model (call it <code>mixture_rt2.stan</code>) to the same simulated data set that we used before:</p>
<div class="sourceCode" id="cb1096"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1096-1"><a href="ch-mixture.html#cb1096-1" aria-hidden="true" tabindex="-1"></a>mixture_rt2 <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb1096-2"><a href="ch-mixture.html#cb1096-2" aria-hidden="true" tabindex="-1"></a>                        <span class="st">&quot;mixture_rt2.stan&quot;</span>,</span>
<span id="cb1096-3"><a href="ch-mixture.html#cb1096-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb1096-4"><a href="ch-mixture.html#cb1096-4" aria-hidden="true" tabindex="-1"></a>fit_mix_rt2 <span class="ot">&lt;-</span> <span class="fu">stan</span>(mixture_rt2, <span class="at">data =</span> ls_dots_simdata)  </span></code></pre></div>
<p>Now the summaries and traceplots look fine; see Figure <a href="ch-mixture.html#fig:traceplotsmix2">19.7</a>.</p>

<div class="sourceCode" id="cb1097"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1097-1"><a href="ch-mixture.html#cb1097-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_mix_rt2) </span></code></pre></div>
<pre><code>##            mean     2.5%    97.5% n_eff Rhat
## alpha      5.78     5.72     5.85   994 1.01
## beta       0.02    -0.04     0.08  2452 1.00
## sigma      0.38     0.34     0.42  1037 1.01
## gamma      5.07     4.61     5.48   729 1.01
## sigma2     0.45     0.25     0.61   790 1.00
## p_task     0.81     0.57     0.95   789 1.01
## lp__   -6331.84 -6335.92 -6329.39  1405 1.00</code></pre>
<div class="sourceCode" id="cb1099"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1099-1"><a href="ch-mixture.html#cb1099-1" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(fit_mix_rt2) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceplotsmix2"></span>
<img src="bookdown_files/figure-html/traceplotsmix2-1.svg" alt="Traceplots from the model mixture_rt2.stan fit to simulated data." width="672" />
<p class="caption">
FIGURE 19.7: Traceplots from the model <code>mixture_rt2.stan</code> fit to simulated data.
</p>
</div>
</div>
<div id="sec-multmix" class="section level3 hasAnchor" number="19.1.3">
<h3><span class="header-section-number">19.1.3</span> A multivariate implementation of the fast-guess model<a href="ch-mixture.html#sec-multmix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A problem with the previous implementation of the fast-guess model is that we ignore the accuracy information in the data. We can implement a closer version of the verbal description of the model:
In particular, we also want to model the fact that accuracy is at chance level in the fast-guessing mode and that accuracy approaches 100% during the task-engaged mode.</p>
<p>This means that the mixture affects two pairs of distributions:</p>
<p><span class="math display">\[\begin{equation}
z_n \sim \mathit{Bernoulli}(p_{task})
\end{equation}\]</span></p>
<p>A response time distribution</p>
<p><span class="math display" id="eq:dismix2">\[\begin{equation}
rt_n \sim
\begin{cases}
\mathit{LogNormal}(\alpha + \beta \cdot x_n, \sigma), &amp; \text{ if } z_n =1 \\
\mathit{LogNormal}(\gamma, \sigma_2), &amp; \text{ if } z_n=0
\end{cases}
\tag{19.3}
\end{equation}\]</span></p>
<p>and an accuracy distribution</p>
<p><span class="math display" id="eq:dismix3">\[\begin{equation}
acc_n \sim
\begin{cases}
\mathit{Bernoulli}(p_{correct}), &amp; \text{ if } z_n =1 \\
\mathit{Bernoulli}(.5), &amp; \text{ if } z_n=0
\end{cases}
\tag{19.4}
\end{equation}\]</span></p>
<p>We have a new parameter <span class="math inline">\(p_{correct}\)</span>, which represent the probability of making a correct answer in the engaged mode. The verbal description says that it is closer to 100%, and here we have the freedom to choose whatever prior we believe represents for us values that are close to 100% accuracy. We translate this belief into a prior as follows; our prior choice is relatively informative but does not impose a hard constraint; if a subject consistently shows relatively low (or high) accuracy, <span class="math inline">\(p_{correct}\)</span> will change accordingly:</p>
<p><span class="math display">\[\begin{equation}
p_{correct} \sim \mathit{Beta}(995, 5)
\end{equation}\]</span></p>
<p>In our simulated data, we assume that the global motion detection task is done by a very accurate subject, with an accuracy of 99.9%.</p>
<p>First, simulate reaction times as before.</p>
<div class="sourceCode" id="cb1100"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1100-1"><a href="ch-mixture.html#cb1100-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1100-2"><a href="ch-mixture.html#cb1100-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span>.<span class="dv">5</span>, N<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(.<span class="dv">5</span>, N<span class="sc">/</span><span class="dv">2</span>)) <span class="co"># difficulty</span></span>
<span id="cb1100-3"><a href="ch-mixture.html#cb1100-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">5.8</span></span>
<span id="cb1100-4"><a href="ch-mixture.html#cb1100-4" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb1100-5"><a href="ch-mixture.html#cb1100-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> .<span class="dv">4</span></span>
<span id="cb1100-6"><a href="ch-mixture.html#cb1100-6" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb1100-7"><a href="ch-mixture.html#cb1100-7" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">5.2</span> <span class="co"># fast guess location</span></span>
<span id="cb1100-8"><a href="ch-mixture.html#cb1100-8" aria-hidden="true" tabindex="-1"></a>p_task <span class="ot">&lt;-</span> .<span class="dv">8</span> <span class="co"># prob of being on task</span></span>
<span id="cb1100-9"><a href="ch-mixture.html#cb1100-9" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(<span class="at">n =</span> N, <span class="at">prob =</span> p_task)</span>
<span id="cb1100-10"><a href="ch-mixture.html#cb1100-10" aria-hidden="true" tabindex="-1"></a>rt <span class="ot">&lt;-</span> <span class="fu">if_else</span>(z <span class="sc">==</span> <span class="dv">1</span>,</span>
<span id="cb1100-11"><a href="ch-mixture.html#cb1100-11" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N,</span>
<span id="cb1100-12"><a href="ch-mixture.html#cb1100-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">meanlog =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x,</span>
<span id="cb1100-13"><a href="ch-mixture.html#cb1100-13" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sdlog =</span> sigma),</span>
<span id="cb1100-14"><a href="ch-mixture.html#cb1100-14" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N, </span>
<span id="cb1100-15"><a href="ch-mixture.html#cb1100-15" aria-hidden="true" tabindex="-1"></a>                    <span class="at">meanlog =</span> gamma,</span>
<span id="cb1100-16"><a href="ch-mixture.html#cb1100-16" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sdlog =</span> sigma2))</span></code></pre></div>
<p>Simulate accuracy and include both reaction times and accuracy in the simulated data set.</p>
<div class="sourceCode" id="cb1101"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1101-1"><a href="ch-mixture.html#cb1101-1" aria-hidden="true" tabindex="-1"></a>p_correct <span class="ot">&lt;-</span> .<span class="dv">999</span></span>
<span id="cb1101-2"><a href="ch-mixture.html#cb1101-2" aria-hidden="true" tabindex="-1"></a>acc <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(z, <span class="fu">rbern</span>(<span class="at">n =</span> N, p_correct),</span>
<span id="cb1101-3"><a href="ch-mixture.html#cb1101-3" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">rbern</span>(<span class="at">n =</span> N, .<span class="dv">5</span>))</span>
<span id="cb1101-4"><a href="ch-mixture.html#cb1101-4" aria-hidden="true" tabindex="-1"></a>df_dots_simdata3 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">trial =</span> <span class="dv">1</span><span class="sc">:</span>N,</span>
<span id="cb1101-5"><a href="ch-mixture.html#cb1101-5" aria-hidden="true" tabindex="-1"></a>                           <span class="at">x =</span> x,</span>
<span id="cb1101-6"><a href="ch-mixture.html#cb1101-6" aria-hidden="true" tabindex="-1"></a>                           <span class="at">rt =</span> rt,</span>
<span id="cb1101-7"><a href="ch-mixture.html#cb1101-7" aria-hidden="true" tabindex="-1"></a>                           <span class="at">acc =</span> acc) <span class="sc">%&gt;%</span></span>
<span id="cb1101-8"><a href="ch-mixture.html#cb1101-8" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">mutate</span>(<span class="at">diff =</span> <span class="fu">if_else</span>(x <span class="sc">==</span> .<span class="dv">5</span>, <span class="st">&quot;hard&quot;</span>, <span class="st">&quot;easy&quot;</span>))</span></code></pre></div>
<p>Plot the simulated data in Figure <a href="ch-mixture.html#fig:simdata3">19.8</a>. This time we can see the effect of task difficulty on the simulated response times and accuracy:</p>

<div class="sourceCode" id="cb1102"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1102-1"><a href="ch-mixture.html#cb1102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_dots_simdata3, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(acc), <span class="at">y =</span> rt)) <span class="sc">+</span></span>
<span id="cb1102-2"><a href="ch-mixture.html#cb1102-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">position =</span> <span class="fu">position_jitter</span>(<span class="at">width =</span> .<span class="dv">4</span>, <span class="at">height =</span> <span class="dv">0</span>),</span>
<span id="cb1102-3"><a href="ch-mixture.html#cb1102-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">alpha =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb1102-4"><a href="ch-mixture.html#cb1102-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(diff <span class="sc">~</span> .) <span class="sc">+</span></span>
<span id="cb1102-5"><a href="ch-mixture.html#cb1102-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="sc">+</span></span>
<span id="cb1102-6"><a href="ch-mixture.html#cb1102-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:simdata3"></span>
<img src="bookdown_files/figure-html/simdata3-1.svg" alt="Response times by accuracy accounting for task difficulty in the simulated data (df_dots_simdata3) that follows the fast-guess model." width="672" />
<p class="caption">
FIGURE 19.8: Response times by accuracy accounting for task difficulty in the simulated data (<code>df_dots_simdata3</code>) that follows the fast-guess model.
</p>
</div>
<p>Next, we need to marginalize out the discrete parameters from both pairs of distributions.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(rt, acc | \Theta) = &amp; p_{task} \cdot \\
&amp; LogNormal(rt_n | \alpha + \beta \cdot x_n, \sigma) \cdot \\
&amp; Bernoulli(acc_n | p_{correct}) \\
&amp; +\\
&amp; (1 - p_{task}) \cdot \\
&amp; LogNormal(rt_n | \gamma, \sigma_2) \cdot\\
&amp; Bernoulli(acc_n | .5)
\end{aligned}
\end{equation}\]</span></p>
<p>In log-space:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\log(p(rt, acc | \Theta)) =  \log(\exp(&amp;\\
&amp; \log(p_{task}) +\\
  &amp;\log(LogNormal(rt_n | \alpha + \beta * x_n, \sigma)) + \\
  &amp;\log(Bernoulli(acc_n | p_{correct})))\\
  +&amp;\\
\exp(&amp;\\
&amp; \log(1 - p_{task}) + \\
&amp; \log(LogNormal(rt_n |\gamma, \sigma_2)) + \\
&amp; \log(Bernoulli(acc_n | .5)))\\
    )&amp; \\
\end{aligned}
\end{equation}\]</span></p>
<p>Our model translates into the following Stan code (<code>mixture_rtacc.stan</code>):</p>
<div class="sourceCode" id="cb1103"><pre class="sourceCode stan fold-show"><code class="sourceCode stan"><span id="cb1103-1"><a href="ch-mixture.html#cb1103-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1103-2"><a href="ch-mixture.html#cb1103-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N;</span>
<span id="cb1103-3"><a href="ch-mixture.html#cb1103-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb1103-4"><a href="ch-mixture.html#cb1103-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] rt;</span>
<span id="cb1103-5"><a href="ch-mixture.html#cb1103-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> acc[N];</span>
<span id="cb1103-6"><a href="ch-mixture.html#cb1103-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1103-7"><a href="ch-mixture.html#cb1103-7" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1103-8"><a href="ch-mixture.html#cb1103-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb1103-9"><a href="ch-mixture.html#cb1103-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb1103-10"><a href="ch-mixture.html#cb1103-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma;</span>
<span id="cb1103-11"><a href="ch-mixture.html#cb1103-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">upper</span> = alpha&gt; gamma; <span class="co">//guessing</span></span>
<span id="cb1103-12"><a href="ch-mixture.html#cb1103-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma2;</span>
<span id="cb1103-13"><a href="ch-mixture.html#cb1103-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_correct;</span>
<span id="cb1103-14"><a href="ch-mixture.html#cb1103-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_task;</span>
<span id="cb1103-15"><a href="ch-mixture.html#cb1103-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1103-16"><a href="ch-mixture.html#cb1103-16" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1103-17"><a href="ch-mixture.html#cb1103-17" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the task component</span></span>
<span id="cb1103-18"><a href="ch-mixture.html#cb1103-18" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1103-19"><a href="ch-mixture.html#cb1103-19" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta | <span class="dv">0</span>, .<span class="dv">3</span>);</span>
<span id="cb1103-20"><a href="ch-mixture.html#cb1103-20" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1103-21"><a href="ch-mixture.html#cb1103-21" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1103-22"><a href="ch-mixture.html#cb1103-22" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the guessing component</span></span>
<span id="cb1103-23"><a href="ch-mixture.html#cb1103-23" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(gamma | <span class="dv">6</span>, <span class="dv">1</span>) -</span>
<span id="cb1103-24"><a href="ch-mixture.html#cb1103-24" aria-hidden="true" tabindex="-1"></a>    normal_lcdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1103-25"><a href="ch-mixture.html#cb1103-25" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma2 | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1103-26"><a href="ch-mixture.html#cb1103-26" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1103-27"><a href="ch-mixture.html#cb1103-27" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_correct | <span class="dv">995</span>, <span class="dv">5</span>);</span>
<span id="cb1103-28"><a href="ch-mixture.html#cb1103-28" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_task | <span class="dv">8</span>, <span class="dv">2</span>);</span>
<span id="cb1103-29"><a href="ch-mixture.html#cb1103-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N)</span>
<span id="cb1103-30"><a href="ch-mixture.html#cb1103-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">target +=</span> log_sum_exp(log(p_task) +</span>
<span id="cb1103-31"><a href="ch-mixture.html#cb1103-31" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma) +</span>
<span id="cb1103-32"><a href="ch-mixture.html#cb1103-32" aria-hidden="true" tabindex="-1"></a>                          bernoulli_lpmf(acc[n] | p_correct),</span>
<span id="cb1103-33"><a href="ch-mixture.html#cb1103-33" aria-hidden="true" tabindex="-1"></a>                          log1m(p_task) +</span>
<span id="cb1103-34"><a href="ch-mixture.html#cb1103-34" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | gamma, sigma2) +</span>
<span id="cb1103-35"><a href="ch-mixture.html#cb1103-35" aria-hidden="true" tabindex="-1"></a>                          bernoulli_lpmf(acc[n] | .<span class="dv">5</span>));</span>
<span id="cb1103-36"><a href="ch-mixture.html#cb1103-36" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Next, set up the data in list format:</p>
<div class="sourceCode" id="cb1104"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1104-1"><a href="ch-mixture.html#cb1104-1" aria-hidden="true" tabindex="-1"></a>ls_dots_simdata <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">N =</span> N,</span>
<span id="cb1104-2"><a href="ch-mixture.html#cb1104-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">rt =</span> rt,</span>
<span id="cb1104-3"><a href="ch-mixture.html#cb1104-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">x =</span> x,</span>
<span id="cb1104-4"><a href="ch-mixture.html#cb1104-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">acc =</span> acc)</span></code></pre></div>
<p>Then fit the model:</p>
<div class="sourceCode" id="cb1105"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1105-1"><a href="ch-mixture.html#cb1105-1" aria-hidden="true" tabindex="-1"></a>mixture_rtacc <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb1105-2"><a href="ch-mixture.html#cb1105-2" aria-hidden="true" tabindex="-1"></a>                        <span class="st">&quot;mixture_rtacc.stan&quot;</span>,</span>
<span id="cb1105-3"><a href="ch-mixture.html#cb1105-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb1105-4"><a href="ch-mixture.html#cb1105-4" aria-hidden="true" tabindex="-1"></a>fit_mix_rtacc <span class="ot">&lt;-</span> <span class="fu">stan</span>(mixture_rtacc, <span class="at">data =</span> ls_dots_simdata)  </span></code></pre></div>
<p>We see that our model can be fit to both response times and accuracy, and its parameters estimates have sensible values (given the fixed parameters we used to generate our simulated data).</p>
<div class="sourceCode" id="cb1106"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1106-1"><a href="ch-mixture.html#cb1106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_mix_rtacc) </span></code></pre></div>
<pre><code>##               mean     2.5%    97.5% n_eff Rhat
## alpha         5.79     5.76     5.82  4238    1
## beta          0.02    -0.04     0.08  5445    1
## sigma         0.38     0.36     0.41  5539    1
## gamma         5.17     5.07     5.27  3742    1
## sigma2        0.50     0.43     0.57  3999    1
## p_correct     0.99     0.99     1.00  4608    1
## p_task        0.80     0.76     0.84  5089    1
## lp__      -6607.59 -6612.33 -6604.88  1864    1</code></pre>
<p>We will evaluate the recovery of the parameters more carefully when we deal with the hierarchical version of the fast-guess model in section <a href="ch-mixture.html#sec-fastguessh">19.1.5</a>. Before we extend this model hierarchically, let us also take into account the instructions given to the subjects.</p>
</div>
<div id="an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account" class="section level3 hasAnchor" number="19.1.4">
<h3><span class="header-section-number">19.1.4</span> An implementation of the fast-guess model that takes instructions into account<a href="ch-mixture.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The actual global motion detection experiment that we started with has another manipulation that can help us to evaluate better the fast-guess model. In some trials, the instructions emphasized accuracy (e.g., “Be as accurate as possible.”) and in others speed (e.g., “Be as fast as possible.”). The fast-guess model also assumes that the probability of being in one of the two states depends on the speed incentives given during the instructions. This entails that now <span class="math inline">\(p_{task}\)</span> depends on the instructions <span class="math inline">\(x_2\)</span>, where we encode a speed incentive with <span class="math inline">\(-0.5\)</span> and an accuracy incentive with <span class="math inline">\(0.5\)</span>. Essentially, we need to fit the following regression:</p>
<p><span class="math display">\[\begin{equation}
\alpha_{task} + x_2 \cdot \beta_{task}
\end{equation}\]</span></p>
<p>As we did with MPT models in the previous chapter (in section <a href="ch-MPT.html#sec-MPT-reg">18.2.3</a>), we need to bound the previous regression between 0 and 1; we achieve this using the logistic or inverse logit function:</p>
<p><span class="math display">\[\begin{equation}
p_{task} = logit^{-1}(\alpha_{task} + x_2 \cdot \beta_{task})
\end{equation}\]</span></p>
<p>This means that we need to interpret <span class="math inline">\(\alpha_{task} + x_2 \cdot \beta_{task}\)</span> in log-odds, which is bounded by <span class="math inline">\((-\infty, \infty)\)</span> rather than as a probability; see also section <a href="ch-MPT.html#sec-MPT-reg">18.2.3</a> in the previous chapter.</p>
<p>The likelihood (defined before in section <a href="ch-mixture.html#sec-multmix">19.1.3</a>) remains the same:</p>
<p><span class="math display">\[\begin{equation}
z_n \sim \mathit{Bernoulli}(p_{task})
\end{equation}\]</span></p>
<p>A response time distribution is defined:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim
\begin{cases}
\mathit{LogNormal}(\alpha + \beta \cdot x_n, \sigma), &amp; \text{ if } z_n =1 \\
\mathit{LogNormal}(\gamma, \sigma_2), &amp; \text{ if } z_n=0
\end{cases}
\end{equation}\]</span></p>
<p>and an accuracy distribution is defined as well:</p>
<p><span class="math display">\[\begin{equation}
acc_n \sim
\begin{cases}
\mathit{Bernoulli}(p_{correct}), &amp; \text{ if } z_n =1 \\
\mathit{Bernoulli}(.5), &amp; \text{ if } z_n=0
\end{cases}
\end{equation}\]</span></p>
<p>The only further change in our model is that rather than a prior on <span class="math inline">\(p_{task}\)</span>, we now need priors for <span class="math inline">\(\alpha_{task}\)</span> and <span class="math inline">\(\beta_{task}\)</span>, which are on the log-odds scale.</p>
<p>For <span class="math inline">\(\beta_{task}\)</span>, we assume an effect that can rather large and we won’t assume a direction a priori (for now):</p>
<p><span class="math display">\[\begin{equation}
\beta_{task} \sim \mathit{Normal}(0, 1)
\end{equation}\]</span></p>
<p>This means that the subject could be affected by the instructions in the expected way with an increased probability to be task-engaged (leading to better accuracy) when the instructions emphasize accuracy (<span class="math inline">\(\beta_{task} &gt;0\)</span>), or the subject might be behaving in an unexpected way with a decreased probability to be task-engaged (leading to worse accuracy) when the instructions emphasize accuracy (<span class="math inline">\(\beta_{task} &lt;0\)</span>). The latter situation, <span class="math inline">\(\beta_{task} &lt;0\)</span>, could represent the instructions being misunderstood. It’s certainly possible to include priors that encode the expected direction of the effect instead: <span class="math inline">\(\mathit{Normal}_{+}(0,1)\)</span>. Unless there is a compelling reason to constrain the prior in this way, in general we will, following Cromwell’s rule, leave open the possibility of the <span class="math inline">\(\beta\)</span> parameter having negative values.</p>
<p>How can we choose a prior for <span class="math inline">\(\alpha_{task}\)</span> that encodes the same information that we had in the previous model in <span class="math inline">\(p_{task}\)</span>? One possibility is to create an auxiliary parameter <span class="math inline">\(p_{btask}\)</span>, that represents the baseline probability of being engaged in the task, with the same prior that we use in the previous section, and then transform it to an unconstrained space for our regression with the logit function:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp;p_{btask} \sim \mathit{\mathit{Beta}}(8, 2)\\
&amp;\alpha_{task} = logit(p_{btask})
\end{aligned}
\end{equation}\]</span></p>
<p>To verify that our priors make sense, we plot the difference in prior predicted probability of being engaged in the task under the two emphasis conditions in Figure <a href="ch-mixture.html#fig:emphasis">19.9</a>.</p>
<div class="sourceCode" id="cb1108"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1108-1"><a href="ch-mixture.html#cb1108-1" aria-hidden="true" tabindex="-1"></a>Ns <span class="ot">&lt;-</span> <span class="dv">1000</span> <span class="co"># number of samples for the plot</span></span>
<span id="cb1108-2"><a href="ch-mixture.html#cb1108-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Priors</span></span>
<span id="cb1108-3"><a href="ch-mixture.html#cb1108-3" aria-hidden="true" tabindex="-1"></a>p_btask <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="at">n =</span> Ns, <span class="at">shape1 =</span> <span class="dv">8</span>, <span class="at">shape2 =</span> <span class="dv">2</span>)</span>
<span id="cb1108-4"><a href="ch-mixture.html#cb1108-4" aria-hidden="true" tabindex="-1"></a>beta_task <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> Ns, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1108-5"><a href="ch-mixture.html#cb1108-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicted probability of being engaged</span></span>
<span id="cb1108-6"><a href="ch-mixture.html#cb1108-6" aria-hidden="true" tabindex="-1"></a>p_task_easy <span class="ot">&lt;-</span> <span class="fu">plogis</span>(<span class="fu">qlogis</span>(p_btask) <span class="sc">+</span> .<span class="dv">5</span> <span class="sc">*</span> beta_task)</span>
<span id="cb1108-7"><a href="ch-mixture.html#cb1108-7" aria-hidden="true" tabindex="-1"></a>p_task_hard <span class="ot">&lt;-</span> <span class="fu">plogis</span>(<span class="fu">qlogis</span>(p_btask) <span class="sc">+</span> <span class="sc">-</span>.<span class="dv">5</span> <span class="sc">*</span> beta_task)</span>
<span id="cb1108-8"><a href="ch-mixture.html#cb1108-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicted difference</span></span>
<span id="cb1108-9"><a href="ch-mixture.html#cb1108-9" aria-hidden="true" tabindex="-1"></a>diff_p_pred <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">diff =</span> p_task_easy <span class="sc">-</span> p_task_hard)</span></code></pre></div>

<div class="sourceCode" id="cb1109"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1109-1"><a href="ch-mixture.html#cb1109-1" aria-hidden="true" tabindex="-1"></a>diff_p_pred <span class="sc">%&gt;%</span></span>
<span id="cb1109-2"><a href="ch-mixture.html#cb1109-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(diff)) <span class="sc">+</span></span>
<span id="cb1109-3"><a href="ch-mixture.html#cb1109-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:emphasis"></span>
<img src="bookdown_files/figure-html/emphasis-1.svg" alt="Difference in prior predicted probability of being engaged in the task under the two emphasis conditions for the simulated data (diff_p_pred) that follows the fast-guess model." width="672" />
<p class="caption">
FIGURE 19.9: Difference in prior predicted probability of being engaged in the task under the two emphasis conditions for the simulated data (<code>diff_p_pred</code>) that follows the fast-guess model.
</p>
</div>
<p>Figure <a href="ch-mixture.html#fig:emphasis">19.9</a> shows that we are predicting a priori that the difference in <span class="math inline">\(p_{task}\)</span> will be mostly smaller than <span class="math inline">\(\pm 0.3\)</span>, which seems to make sense intuitively. If we had more information about the likely range of variation, we could of course have adapted the prior to reflect that belief.</p>
<p>We are ready to generate a new data set, by deciding on some fixed values for <span class="math inline">\(\beta_{task}\)</span> and <span class="math inline">\(p_{btask}\)</span>.</p>
<div class="sourceCode" id="cb1110"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1110-1"><a href="ch-mixture.html#cb1110-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1110-2"><a href="ch-mixture.html#cb1110-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span>.<span class="dv">5</span>, N<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(.<span class="dv">5</span>, N<span class="sc">/</span><span class="dv">2</span>)) <span class="co"># difficulty</span></span>
<span id="cb1110-3"><a href="ch-mixture.html#cb1110-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">5</span>, .<span class="dv">5</span>), N<span class="sc">/</span><span class="dv">2</span>) <span class="co"># instructions</span></span>
<span id="cb1110-4"><a href="ch-mixture.html#cb1110-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that the predictors are crossed:</span></span>
<span id="cb1110-5"><a href="ch-mixture.html#cb1110-5" aria-hidden="true" tabindex="-1"></a>predictors <span class="ot">&lt;-</span> <span class="fu">tibble</span>(x, x2)</span>
<span id="cb1110-6"><a href="ch-mixture.html#cb1110-6" aria-hidden="true" tabindex="-1"></a><span class="fu">xtabs</span>(<span class="sc">~</span> x <span class="sc">+</span> x2, predictors)</span></code></pre></div>
<pre><code>##       x2
## x      -0.5 0.5
##   -0.5  250 250
##   0.5   250 250</code></pre>
<div class="sourceCode" id="cb1112"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1112-1"><a href="ch-mixture.html#cb1112-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">5.8</span></span>
<span id="cb1112-2"><a href="ch-mixture.html#cb1112-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb1112-3"><a href="ch-mixture.html#cb1112-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> .<span class="dv">4</span></span>
<span id="cb1112-4"><a href="ch-mixture.html#cb1112-4" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb1112-5"><a href="ch-mixture.html#cb1112-5" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">5.2</span> </span>
<span id="cb1112-6"><a href="ch-mixture.html#cb1112-6" aria-hidden="true" tabindex="-1"></a>p_correct <span class="ot">&lt;-</span> .<span class="dv">999</span></span>
<span id="cb1112-7"><a href="ch-mixture.html#cb1112-7" aria-hidden="true" tabindex="-1"></a><span class="co"># New true values:</span></span>
<span id="cb1112-8"><a href="ch-mixture.html#cb1112-8" aria-hidden="true" tabindex="-1"></a>p_btask <span class="ot">&lt;-</span> .<span class="dv">85</span></span>
<span id="cb1112-9"><a href="ch-mixture.html#cb1112-9" aria-hidden="true" tabindex="-1"></a>beta_task <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb1112-10"><a href="ch-mixture.html#cb1112-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data:</span></span>
<span id="cb1112-11"><a href="ch-mixture.html#cb1112-11" aria-hidden="true" tabindex="-1"></a>alpha_task <span class="ot">&lt;-</span> <span class="fu">qlogis</span>(p_btask)</span>
<span id="cb1112-12"><a href="ch-mixture.html#cb1112-12" aria-hidden="true" tabindex="-1"></a>p_task <span class="ot">&lt;-</span> <span class="fu">plogis</span>(alpha_task <span class="sc">+</span> x2 <span class="sc">*</span> beta_task)</span>
<span id="cb1112-13"><a href="ch-mixture.html#cb1112-13" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N, <span class="at">prob =</span> p_task)</span>
<span id="cb1112-14"><a href="ch-mixture.html#cb1112-14" aria-hidden="true" tabindex="-1"></a>rt <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(z,</span>
<span id="cb1112-15"><a href="ch-mixture.html#cb1112-15" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N, <span class="at">meanlog =</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x, <span class="at">sdlog =</span> sigma),</span>
<span id="cb1112-16"><a href="ch-mixture.html#cb1112-16" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N, <span class="at">meanlog =</span> gamma, <span class="at">sdlog =</span> sigma2))</span>
<span id="cb1112-17"><a href="ch-mixture.html#cb1112-17" aria-hidden="true" tabindex="-1"></a>acc <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(z, <span class="fu">rbern</span>(N, p_correct),</span>
<span id="cb1112-18"><a href="ch-mixture.html#cb1112-18" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">rbern</span>(N, .<span class="dv">5</span>))</span>
<span id="cb1112-19"><a href="ch-mixture.html#cb1112-19" aria-hidden="true" tabindex="-1"></a>df_dots_simdata4 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">trial =</span> <span class="dv">1</span><span class="sc">:</span>N,</span>
<span id="cb1112-20"><a href="ch-mixture.html#cb1112-20" aria-hidden="true" tabindex="-1"></a>                           <span class="at">x =</span> x,</span>
<span id="cb1112-21"><a href="ch-mixture.html#cb1112-21" aria-hidden="true" tabindex="-1"></a>                           <span class="at">rt =</span> rt,</span>
<span id="cb1112-22"><a href="ch-mixture.html#cb1112-22" aria-hidden="true" tabindex="-1"></a>                           <span class="at">acc =</span> acc,</span>
<span id="cb1112-23"><a href="ch-mixture.html#cb1112-23" aria-hidden="true" tabindex="-1"></a>                           <span class="at">x2 =</span> x2) <span class="sc">%&gt;%</span></span>
<span id="cb1112-24"><a href="ch-mixture.html#cb1112-24" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mutate</span>(<span class="at">diff =</span> <span class="fu">if_else</span>(x <span class="sc">==</span> .<span class="dv">5</span>, <span class="st">&quot;hard&quot;</span>, <span class="st">&quot;easy&quot;</span>),</span>
<span id="cb1112-25"><a href="ch-mixture.html#cb1112-25" aria-hidden="true" tabindex="-1"></a>                 <span class="at">emphasis =</span> <span class="fu">ifelse</span>(x2 <span class="sc">==</span> .<span class="dv">5</span>, <span class="st">&quot;accuracy&quot;</span>, <span class="st">&quot;speed&quot;</span>))</span></code></pre></div>
<p>We can generate a plot now where both the difficulty of the task and the instructions are manipulated; see Figure <a href="ch-mixture.html#fig:taskinstr">19.10</a>.</p>

<div class="sourceCode" id="cb1113"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1113-1"><a href="ch-mixture.html#cb1113-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_dots_simdata4, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(acc), <span class="at">y =</span> rt)) <span class="sc">+</span></span>
<span id="cb1113-2"><a href="ch-mixture.html#cb1113-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">position =</span> <span class="fu">position_jitter</span>(<span class="at">width =</span> .<span class="dv">4</span>, <span class="at">height =</span> <span class="dv">0</span>),</span>
<span id="cb1113-3"><a href="ch-mixture.html#cb1113-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">alpha =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb1113-4"><a href="ch-mixture.html#cb1113-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(diff <span class="sc">~</span> emphasis) <span class="sc">+</span></span>
<span id="cb1113-5"><a href="ch-mixture.html#cb1113-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="sc">+</span></span>
<span id="cb1113-6"><a href="ch-mixture.html#cb1113-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Response time&quot;</span>) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:taskinstr"></span>
<img src="bookdown_files/figure-html/taskinstr-1.svg" alt="Response times and accuracy by the difficulty of the task and the instructions type for the simulated data (df_dots_simdata4) that follows the fast-guess model." width="672" />
<p class="caption">
FIGURE 19.10: Response times and accuracy by the difficulty of the task and the instructions type for the simulated data (<code>df_dots_simdata4</code>) that follows the fast-guess model.
</p>
</div>
<p>In the Stan implementation, <code>log_inv_logit(x)</code> is applying the logistic (or inverse logit) function to <code>x</code> to transform it into a probability and then applying the logarithm; <code>log1m_inv_logit(x)</code> is applying the logistic function to <code>x</code>, and then applying the logarithm to its complement <span class="math inline">\((1 - p)\)</span>. We do this because rather than having <code>p_task</code> in probability space, we have <code>lodds_task</code> in log-odds space:</p>
<pre><code>real lodds_task = logit(p_btask) + x2[n] * beta_task;</code></pre>
<p>The parameter <code>lodds_task</code> estimates the mixing probabilities in log-odds:</p>
<pre><code>target += log_sum_exp(log_inv_logit(lodds_task) + ..., 
          log1m_inv_logit(lodds_task) + ...)</code></pre>
<p>We also add a <code>generated quantities</code> block that can be used for further (prior or posterior) predictive checks. In this block we do use <code>z</code> as an indicator of the latent class (task-engaged mode or fast-guessing mode), since we do not estimate <code>z</code>, but rather generate it based on the parameter’s posteriors.</p>
<p>We use the dummy variable <code>onlyprior</code> to indicate whether we use the data or we only sample from the priors. One can always do the predictive checks in R, transforming the code that we wrote for the simulation into a function, and writing the priors in R. However, it can be simpler to take advantage of Stan output format and rewrite the code in Stan. One downside of this is that the <code>stanfit</code> object that stores the model output can become too large for the memory of the computer.</p>
<div class="sourceCode" id="cb1116"><pre class="sourceCode stan fold-show"><code class="sourceCode stan"><span id="cb1116-1"><a href="ch-mixture.html#cb1116-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1116-2"><a href="ch-mixture.html#cb1116-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N;</span>
<span id="cb1116-3"><a href="ch-mixture.html#cb1116-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb1116-4"><a href="ch-mixture.html#cb1116-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] rt;</span>
<span id="cb1116-5"><a href="ch-mixture.html#cb1116-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> acc[N];</span>
<span id="cb1116-6"><a href="ch-mixture.html#cb1116-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x2; <span class="co">//speed or accuracy emphasis</span></span>
<span id="cb1116-7"><a href="ch-mixture.html#cb1116-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; onlyprior;</span>
<span id="cb1116-8"><a href="ch-mixture.html#cb1116-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1116-9"><a href="ch-mixture.html#cb1116-9" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1116-10"><a href="ch-mixture.html#cb1116-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb1116-11"><a href="ch-mixture.html#cb1116-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb1116-12"><a href="ch-mixture.html#cb1116-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma;</span>
<span id="cb1116-13"><a href="ch-mixture.html#cb1116-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">upper</span> = alpha&gt; gamma; <span class="co">//guessing</span></span>
<span id="cb1116-14"><a href="ch-mixture.html#cb1116-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma2;</span>
<span id="cb1116-15"><a href="ch-mixture.html#cb1116-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_correct;</span>
<span id="cb1116-16"><a href="ch-mixture.html#cb1116-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_btask;</span>
<span id="cb1116-17"><a href="ch-mixture.html#cb1116-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta_task;</span>
<span id="cb1116-18"><a href="ch-mixture.html#cb1116-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1116-19"><a href="ch-mixture.html#cb1116-19" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1116-20"><a href="ch-mixture.html#cb1116-20" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the task component</span></span>
<span id="cb1116-21"><a href="ch-mixture.html#cb1116-21" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1116-22"><a href="ch-mixture.html#cb1116-22" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta | <span class="dv">0</span>, .<span class="dv">1</span>);</span>
<span id="cb1116-23"><a href="ch-mixture.html#cb1116-23" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1116-24"><a href="ch-mixture.html#cb1116-24" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1116-25"><a href="ch-mixture.html#cb1116-25" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the guessing component</span></span>
<span id="cb1116-26"><a href="ch-mixture.html#cb1116-26" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(gamma | <span class="dv">6</span>, <span class="dv">1</span>)-</span>
<span id="cb1116-27"><a href="ch-mixture.html#cb1116-27" aria-hidden="true" tabindex="-1"></a>  normal_lcdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1116-28"><a href="ch-mixture.html#cb1116-28" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma2 | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1116-29"><a href="ch-mixture.html#cb1116-29" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1116-30"><a href="ch-mixture.html#cb1116-30" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta_task | <span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb1116-31"><a href="ch-mixture.html#cb1116-31" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_correct | <span class="dv">995</span>, <span class="dv">5</span>);</span>
<span id="cb1116-32"><a href="ch-mixture.html#cb1116-32" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_btask | <span class="dv">8</span>, <span class="dv">2</span>);</span>
<span id="cb1116-33"><a href="ch-mixture.html#cb1116-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(onlyprior != <span class="dv">1</span>)</span>
<span id="cb1116-34"><a href="ch-mixture.html#cb1116-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N){</span>
<span id="cb1116-35"><a href="ch-mixture.html#cb1116-35" aria-hidden="true" tabindex="-1"></a>      <span class="dt">real</span> lodds_task = logit(p_btask) + x2[n] * beta_task;</span>
<span id="cb1116-36"><a href="ch-mixture.html#cb1116-36" aria-hidden="true" tabindex="-1"></a>      <span class="kw">target +=</span> log_sum_exp(log_inv_logit(lodds_task)+</span>
<span id="cb1116-37"><a href="ch-mixture.html#cb1116-37" aria-hidden="true" tabindex="-1"></a>                            lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma) +</span>
<span id="cb1116-38"><a href="ch-mixture.html#cb1116-38" aria-hidden="true" tabindex="-1"></a>                            bernoulli_lpmf(acc[n] | p_correct),</span>
<span id="cb1116-39"><a href="ch-mixture.html#cb1116-39" aria-hidden="true" tabindex="-1"></a>                            log1m_inv_logit(lodds_task) +</span>
<span id="cb1116-40"><a href="ch-mixture.html#cb1116-40" aria-hidden="true" tabindex="-1"></a>                            lognormal_lpdf(rt[n] | gamma, sigma2) +</span>
<span id="cb1116-41"><a href="ch-mixture.html#cb1116-41" aria-hidden="true" tabindex="-1"></a>                            bernoulli_lpmf(acc[n] | .<span class="dv">5</span>));</span>
<span id="cb1116-42"><a href="ch-mixture.html#cb1116-42" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1116-43"><a href="ch-mixture.html#cb1116-43" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1116-44"><a href="ch-mixture.html#cb1116-44" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb1116-45"><a href="ch-mixture.html#cb1116-45" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> rt_pred[N];</span>
<span id="cb1116-46"><a href="ch-mixture.html#cb1116-46" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> acc_pred[N];</span>
<span id="cb1116-47"><a href="ch-mixture.html#cb1116-47" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> z[N]; </span>
<span id="cb1116-48"><a href="ch-mixture.html#cb1116-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N){</span>
<span id="cb1116-49"><a href="ch-mixture.html#cb1116-49" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> lodds_task = logit(p_btask) + x2[n] * beta_task;</span>
<span id="cb1116-50"><a href="ch-mixture.html#cb1116-50" aria-hidden="true" tabindex="-1"></a>    z[n] = bernoulli_rng(inv_logit(lodds_task));</span>
<span id="cb1116-51"><a href="ch-mixture.html#cb1116-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(z[n]==<span class="dv">1</span>){</span>
<span id="cb1116-52"><a href="ch-mixture.html#cb1116-52" aria-hidden="true" tabindex="-1"></a>      rt_pred[n] = lognormal_rng(alpha + x[n] * beta, sigma);</span>
<span id="cb1116-53"><a href="ch-mixture.html#cb1116-53" aria-hidden="true" tabindex="-1"></a>      acc_pred[n] = bernoulli_rng(p_correct);</span>
<span id="cb1116-54"><a href="ch-mixture.html#cb1116-54" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span>{</span>
<span id="cb1116-55"><a href="ch-mixture.html#cb1116-55" aria-hidden="true" tabindex="-1"></a>      rt_pred[n] = lognormal_rng(gamma, sigma2);</span>
<span id="cb1116-56"><a href="ch-mixture.html#cb1116-56" aria-hidden="true" tabindex="-1"></a>      acc_pred[n] = bernoulli_rng(.<span class="dv">5</span>);</span>
<span id="cb1116-57"><a href="ch-mixture.html#cb1116-57" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1116-58"><a href="ch-mixture.html#cb1116-58" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1116-59"><a href="ch-mixture.html#cb1116-59" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We save the code as <code>mixture_rtacc2.stan</code>, and before fitting it to the simulated data, we perform prior predictive checks.</p>
<div id="prior-predictive-checks-1" class="section level4 hasAnchor" number="19.1.4.1">
<h4><span class="header-section-number">19.1.4.1</span> Prior predictive checks<a href="ch-mixture.html#prior-predictive-checks-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Generate prior predictive distributions, by setting <code>onlyprior</code> to <code>1</code>.</p>
<div class="sourceCode" id="cb1117"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1117-1"><a href="ch-mixture.html#cb1117-1" aria-hidden="true" tabindex="-1"></a>ls_dots_simdata <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">N =</span> N,</span>
<span id="cb1117-2"><a href="ch-mixture.html#cb1117-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">rt =</span> rt,</span>
<span id="cb1117-3"><a href="ch-mixture.html#cb1117-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">x =</span> x,</span>
<span id="cb1117-4"><a href="ch-mixture.html#cb1117-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">x2 =</span> x2,</span>
<span id="cb1117-5"><a href="ch-mixture.html#cb1117-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">acc =</span> acc,</span>
<span id="cb1117-6"><a href="ch-mixture.html#cb1117-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">onlyprior =</span> <span class="dv">1</span>) </span>
<span id="cb1117-7"><a href="ch-mixture.html#cb1117-7" aria-hidden="true" tabindex="-1"></a>mixture_rtacc2 <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;stan_models&quot;</span>, </span>
<span id="cb1117-8"><a href="ch-mixture.html#cb1117-8" aria-hidden="true" tabindex="-1"></a>                        <span class="st">&quot;mixture_rtacc2.stan&quot;</span>, </span>
<span id="cb1117-9"><a href="ch-mixture.html#cb1117-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb1117-10"><a href="ch-mixture.html#cb1117-10" aria-hidden="true" tabindex="-1"></a>fit_mix_rtacc2_priors <span class="ot">&lt;-</span> <span class="fu">stan</span>(mixture_rtacc2,</span>
<span id="cb1117-11"><a href="ch-mixture.html#cb1117-11" aria-hidden="true" tabindex="-1"></a>                         <span class="at">data =</span> ls_dots_simdata,</span>
<span id="cb1117-12"><a href="ch-mixture.html#cb1117-12" aria-hidden="true" tabindex="-1"></a>                         <span class="at">chains =</span> <span class="dv">1</span>, <span class="at">iter =</span> <span class="dv">2000</span>)   </span></code></pre></div>
<p>We plot prior predictive distributions of response times as follows in Figure <a href="ch-mixture.html#fig:ppcmix4">19.11</a>, by setting <code>y = rt</code> using <code>ppd_dens_overlay()</code>.</p>

<div class="sourceCode" id="cb1118"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1118-1"><a href="ch-mixture.html#cb1118-1" aria-hidden="true" tabindex="-1"></a>rt_pred <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_mix_rtacc2_priors)<span class="sc">$</span>rt_pred</span>
<span id="cb1118-2"><a href="ch-mixture.html#cb1118-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ppd_dens_overlay</span>(rt_pred[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,]) <span class="sc">+</span></span>
<span id="cb1118-3"><a href="ch-mixture.html#cb1118-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>)) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ppcmix4"></span>
<img src="bookdown_files/figure-html/ppcmix4-1.svg" alt="Prior predictive distributions of response times (using mixture_rtacc2.stan) that follows the fast-guess model." width="672" />
<p class="caption">
FIGURE 19.11: Prior predictive distributions of response times (using <code>mixture_rtacc2.stan</code>) that follows the fast-guess model.
</p>
</div>
<p>Some of the predictive data sets contain responses that are too large, and some of the have too much probability mass close to zero, but there is nothing clearly wrong in the prior predictive distributions (considering that the model hasn’t “seen” the data).</p>
<p>If we want to plot the prior predicted distribution of differences in response time conditioning on task difficulty, we need to define a new function. Then we use the <code>bayesplot</code> function <code>ppc_stat()</code> that takes as an argument of <code>stat</code> any summary function; see Figure <a href="ch-mixture.html#fig:ppdiff">19.12</a>.</p>

<div class="sourceCode" id="cb1119"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1119-1"><a href="ch-mixture.html#cb1119-1" aria-hidden="true" tabindex="-1"></a>meanrt_diff <span class="ot">&lt;-</span> <span class="cf">function</span>(rt){</span>
<span id="cb1119-2"><a href="ch-mixture.html#cb1119-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(rt[x <span class="sc">==</span> .<span class="dv">5</span>]) <span class="sc">-</span></span>
<span id="cb1119-3"><a href="ch-mixture.html#cb1119-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(rt[x <span class="sc">==</span> <span class="sc">-</span>.<span class="dv">5</span>])</span>
<span id="cb1119-4"><a href="ch-mixture.html#cb1119-4" aria-hidden="true" tabindex="-1"></a>} </span>
<span id="cb1119-5"><a href="ch-mixture.html#cb1119-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ppd_stat</span>(rt_pred, <span class="at">stat =</span> meanrt_diff) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ppdiff"></span>
<img src="bookdown_files/figure-html/ppdiff-1.svg" alt="Prior predicted distribution (using mixture_rtacc2.stan) of differences in response time conditioning on task difficulty." width="672" />
<p class="caption">
FIGURE 19.12: Prior predicted distribution (using <code>mixture_rtacc2.stan</code>) of differences in response time conditioning on task difficulty.
</p>
</div>
<!-- We can inspect the prior predictive distribution of accuracy by grouping accuracy by emphasis: -->
<p>We find that the range of response times look reasonable. There are, however, always more checks that can be done, for example, plotting other summary statistics, or predictions conditioned on other aspects of the data.</p>
</div>
<div id="fit-to-simulated-data" class="section level4 hasAnchor" number="19.1.4.2">
<h4><span class="header-section-number">19.1.4.2</span> Fit to simulated data<a href="ch-mixture.html#fit-to-simulated-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Fit it to data, by setting <code>onlyprior = 0</code>:</p>
<div class="sourceCode" id="cb1120"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1120-1"><a href="ch-mixture.html#cb1120-1" aria-hidden="true" tabindex="-1"></a>ls_dots_simdata <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">N =</span> N,</span>
<span id="cb1120-2"><a href="ch-mixture.html#cb1120-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">rt =</span> rt,</span>
<span id="cb1120-3"><a href="ch-mixture.html#cb1120-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">x =</span> x,</span>
<span id="cb1120-4"><a href="ch-mixture.html#cb1120-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">x2 =</span> x2,</span>
<span id="cb1120-5"><a href="ch-mixture.html#cb1120-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">acc =</span> acc,</span>
<span id="cb1120-6"><a href="ch-mixture.html#cb1120-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">onlyprior =</span> <span class="dv">0</span>) </span></code></pre></div>
<div class="sourceCode" id="cb1121"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1121-1"><a href="ch-mixture.html#cb1121-1" aria-hidden="true" tabindex="-1"></a>fit_mix_rtacc2 <span class="ot">&lt;-</span> <span class="fu">stan</span>(mixture_rtacc2, <span class="at">data =</span> ls_dots_simdata)</span></code></pre></div>
<div class="sourceCode" id="cb1122"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1122-1"><a href="ch-mixture.html#cb1122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_mix_rtacc2,</span>
<span id="cb1122-2"><a href="ch-mixture.html#cb1122-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,</span>
<span id="cb1122-3"><a href="ch-mixture.html#cb1122-3" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;p_correct&quot;</span>, <span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>)) </span></code></pre></div>
<pre><code>##           mean 2.5% 97.5% n_eff Rhat
## alpha     5.80 5.77  5.83  5889    1
## beta      0.06 0.00  0.12  7200    1
## sigma     0.39 0.37  0.41  5873    1
## gamma     5.21 5.09  5.32  4366    1
## sigma2    0.53 0.46  0.61  4721    1
## p_correct 0.99 0.99  1.00  4756    1
## p_btask   0.85 0.82  0.88  4521    1
## beta_task 1.12 0.64  1.63  5780    1</code></pre>
<p>We see that we fit the model without problems. Before we evaluate the recovery of the parameters more carefully, we implement a hierarchical version of the fast-guess model.</p>
</div>
</div>
<div id="sec-fastguessh" class="section level3 hasAnchor" number="19.1.5">
<h3><span class="header-section-number">19.1.5</span> A hierarchical implementation of the fast-guess model<a href="ch-mixture.html#sec-fastguessh" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far we have evaluated the behavior of one simulated subject.
We discussed before (in the context of distributional regression models, in section <a href="ch-hierarchical.html#sec-distrmodel">5.2.6</a>, and in the MPT modeling chapter, <a href="ch-MPT.html#ch-MPT">18</a>) that every parameter in a model can, in principle, be made hierarchical. This doesn’t guarantee, however, that we’ll learn anything from the data for those parameters or that our model will converge! The best advice here is to start simple with simulated data. Despite the fact that convergence with simulated data does not guarantee the convergence of the same model with real data, the reverse is in general true.</p>
<p>For our hierarchical version, we assume that both response times and the effect of task difficulty vary by subject, and that different subjects have different guessing times. This entails the following change to the response time distribution:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim
\begin{cases}
\mathit{LogNormal}(\alpha + u_{subj[n],1} +  x_n \cdot  (\beta +  u_{subj[n], 2}), \sigma), &amp; \text{ if } z_n =1 \\
\mathit{LogNormal}(\gamma + u_{subj[n], 3}, \sigma_2), &amp; \text{ if } z_n=0
\end{cases}
\end{equation}\]</span></p>
<p>We assume that the three vectors of <span class="math inline">\(u\)</span> (adjustment to the intercept and slope of the task-engaged distribution, and the adjustment to the guessing time distribution) follow a multivariate normal distribution centered on zero. For simplicity and lack of any prior knowledge about this experiment design and method, we assume the same (weakly informative) prior distribution for the three variance components and the same regularizing LKJ prior for the three correlations between the adjustments (<span class="math inline">\(\rho_{u_{1,2}}, \rho_{u_{1,3}}, \rho_{u_{2,3}}\)</span>):</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\boldsymbol{u} &amp;\sim\mathcal{N}(0, \Sigma_u)\\
\tau_{u_{1..3}} &amp; \sim \mathit{ \mathit{Normal}}_+(0, .5)\\
\rho_u &amp;\sim \mathit{LKJcorr}(2)
\end{aligned}
\end{equation}\]</span></p>
<p>Before we fit the model to the real data set, we simulate data again; this time we simulate 100 trials of each of 20 subjects.</p>
<div class="sourceCode" id="cb1124"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1124-1"><a href="ch-mixture.html#cb1124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the fake stimuli</span></span>
<span id="cb1124-2"><a href="ch-mixture.html#cb1124-2" aria-hidden="true" tabindex="-1"></a>N_subj <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb1124-3"><a href="ch-mixture.html#cb1124-3" aria-hidden="true" tabindex="-1"></a>N_trials <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1124-4"><a href="ch-mixture.html#cb1124-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters true values</span></span>
<span id="cb1124-5"><a href="ch-mixture.html#cb1124-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">5.8</span></span>
<span id="cb1124-6"><a href="ch-mixture.html#cb1124-6" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb1124-7"><a href="ch-mixture.html#cb1124-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> .<span class="dv">4</span></span>
<span id="cb1124-8"><a href="ch-mixture.html#cb1124-8" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb1124-9"><a href="ch-mixture.html#cb1124-9" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">5.2</span></span>
<span id="cb1124-10"><a href="ch-mixture.html#cb1124-10" aria-hidden="true" tabindex="-1"></a>beta_task <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb1124-11"><a href="ch-mixture.html#cb1124-11" aria-hidden="true" tabindex="-1"></a>p_btask <span class="ot">&lt;-</span> .<span class="dv">85</span></span>
<span id="cb1124-12"><a href="ch-mixture.html#cb1124-12" aria-hidden="true" tabindex="-1"></a>alpha_task <span class="ot">&lt;-</span> <span class="fu">qlogis</span>(p_btask)</span>
<span id="cb1124-13"><a href="ch-mixture.html#cb1124-13" aria-hidden="true" tabindex="-1"></a>p_correct <span class="ot">&lt;-</span> .<span class="dv">999</span></span>
<span id="cb1124-14"><a href="ch-mixture.html#cb1124-14" aria-hidden="true" tabindex="-1"></a>tau_u <span class="ot">&lt;-</span> <span class="fu">c</span>(.<span class="dv">2</span>, .<span class="dv">005</span>, .<span class="dv">3</span>)</span>
<span id="cb1124-15"><a href="ch-mixture.html#cb1124-15" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> .<span class="dv">3</span></span></code></pre></div>
<div class="sourceCode" id="cb1125"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1125-1"><a href="ch-mixture.html#cb1125-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Build the data set here:</span></span>
<span id="cb1125-2"><a href="ch-mixture.html#cb1125-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> N_subj <span class="sc">*</span> N_trials</span>
<span id="cb1125-3"><a href="ch-mixture.html#cb1125-3" aria-hidden="true" tabindex="-1"></a>stimuli <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span>.<span class="dv">5</span>, N_trials <span class="sc">/</span> <span class="dv">2</span>),</span>
<span id="cb1125-4"><a href="ch-mixture.html#cb1125-4" aria-hidden="true" tabindex="-1"></a>                            <span class="fu">rep</span>(.<span class="dv">5</span>, N_trials <span class="sc">/</span> <span class="dv">2</span>)), N_subj),</span>
<span id="cb1125-5"><a href="ch-mixture.html#cb1125-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">x2 =</span> <span class="fu">rep</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">5</span>, .<span class="dv">5</span>), N_trials <span class="sc">/</span> <span class="dv">2</span>), N_subj),</span>
<span id="cb1125-6"><a href="ch-mixture.html#cb1125-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">subj =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>N_subj, <span class="at">each =</span> N_trials),</span>
<span id="cb1125-7"><a href="ch-mixture.html#cb1125-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">trial =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>N_trials, N_subj)</span>
<span id="cb1125-8"><a href="ch-mixture.html#cb1125-8" aria-hidden="true" tabindex="-1"></a>                  )</span>
<span id="cb1125-9"><a href="ch-mixture.html#cb1125-9" aria-hidden="true" tabindex="-1"></a>stimuli</span></code></pre></div>
<pre><code>## # A tibble: 2,000 × 4
##       x    x2  subj trial
##   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;
## 1  -0.5  -0.5     1     1
## 2  -0.5   0.5     1     2
## 3  -0.5  -0.5     1     3
## # … with 1,997 more rows</code></pre>
<div class="sourceCode" id="cb1127"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1127-1"><a href="ch-mixture.html#cb1127-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the correlation matrix for the adjustments u[1...3]:</span></span>
<span id="cb1127-2"><a href="ch-mixture.html#cb1127-2" aria-hidden="true" tabindex="-1"></a>Cor_u <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(rho, <span class="dv">9</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb1127-3"><a href="ch-mixture.html#cb1127-3" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(Cor_u) <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1127-4"><a href="ch-mixture.html#cb1127-4" aria-hidden="true" tabindex="-1"></a>Cor_u</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  1.0  0.3  0.3
## [2,]  0.3  1.0  0.3
## [3,]  0.3  0.3  1.0</code></pre>
<div class="sourceCode" id="cb1129"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1129-1"><a href="ch-mixture.html#cb1129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance covariance matrix for subjects:</span></span>
<span id="cb1129-2"><a href="ch-mixture.html#cb1129-2" aria-hidden="true" tabindex="-1"></a>Sigma_u <span class="ot">&lt;-</span> <span class="fu">diag</span>(tau_u, <span class="dv">3</span>, <span class="dv">3</span>) <span class="sc">%*%</span> Cor_u <span class="sc">%*%</span> <span class="fu">diag</span>(tau_u, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb1129-3"><a href="ch-mixture.html#cb1129-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the correlated adjustments </span></span>
<span id="cb1129-4"><a href="ch-mixture.html#cb1129-4" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> N_subj, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), Sigma_u)</span>
<span id="cb1129-5"><a href="ch-mixture.html#cb1129-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Check whether they are correctly correlated</span></span>
<span id="cb1129-6"><a href="ch-mixture.html#cb1129-6" aria-hidden="true" tabindex="-1"></a><span class="co"># (There will be some random variation,</span></span>
<span id="cb1129-7"><a href="ch-mixture.html#cb1129-7" aria-hidden="true" tabindex="-1"></a><span class="co"># but if you increase the number of observations and</span></span>
<span id="cb1129-8"><a href="ch-mixture.html#cb1129-8" aria-hidden="true" tabindex="-1"></a><span class="co"># the value of the correlation, you&#39;ll be able to obtain</span></span>
<span id="cb1129-9"><a href="ch-mixture.html#cb1129-9" aria-hidden="true" tabindex="-1"></a><span class="co"># a more exact value below)</span></span>
<span id="cb1129-10"><a href="ch-mixture.html#cb1129-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(u)</span></code></pre></div>
<pre><code>##        [,1]   [,2]  [,3]
## [1,] 1.0000 0.0622 0.332
## [2,] 0.0622 1.0000 0.161
## [3,] 0.3315 0.1608 1.000</code></pre>
<div class="sourceCode" id="cb1131"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1131-1"><a href="ch-mixture.html#cb1131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the SDs</span></span>
<span id="cb1131-2"><a href="ch-mixture.html#cb1131-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(u[, <span class="dv">1</span>]); <span class="fu">sd</span>(u[, <span class="dv">2</span>]); <span class="fu">sd</span>(u[, <span class="dv">3</span>])</span></code></pre></div>
<pre><code>## [1] 0.189</code></pre>
<pre><code>## [1] 0.00438</code></pre>
<pre><code>## [1] 0.306</code></pre>
<div class="sourceCode" id="cb1135"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1135-1"><a href="ch-mixture.html#cb1135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the simulated data</span></span>
<span id="cb1135-2"><a href="ch-mixture.html#cb1135-2" aria-hidden="true" tabindex="-1"></a>df_dots_simdata <span class="ot">&lt;-</span> stimuli <span class="sc">%&gt;%</span></span>
<span id="cb1135-3"><a href="ch-mixture.html#cb1135-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">z =</span> <span class="fu">rbern</span>(N, <span class="at">prob =</span> <span class="fu">plogis</span>(alpha_task <span class="sc">+</span> x2 <span class="sc">*</span> beta_task)),</span>
<span id="cb1135-4"><a href="ch-mixture.html#cb1135-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">rt =</span> <span class="fu">ifelse</span>(z,</span>
<span id="cb1135-5"><a href="ch-mixture.html#cb1135-5" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N, <span class="at">meanlog =</span> alpha <span class="sc">+</span> u[subj, <span class="dv">1</span>] <span class="sc">+</span></span>
<span id="cb1135-6"><a href="ch-mixture.html#cb1135-6" aria-hidden="true" tabindex="-1"></a>                             (beta <span class="sc">+</span> u[subj, <span class="dv">2</span>]) <span class="sc">*</span> x,</span>
<span id="cb1135-7"><a href="ch-mixture.html#cb1135-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sdlog =</span> sigma),</span>
<span id="cb1135-8"><a href="ch-mixture.html#cb1135-8" aria-hidden="true" tabindex="-1"></a>             <span class="fu">rlnorm</span>(N, <span class="at">meanlog =</span> gamma <span class="sc">+</span> u[subj, <span class="dv">3</span>],</span>
<span id="cb1135-9"><a href="ch-mixture.html#cb1135-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sdlog =</span> sigma2)),</span>
<span id="cb1135-10"><a href="ch-mixture.html#cb1135-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">acc =</span> <span class="fu">ifelse</span>(z, <span class="fu">rbern</span>(<span class="at">n =</span> N, p_correct),</span>
<span id="cb1135-11"><a href="ch-mixture.html#cb1135-11" aria-hidden="true" tabindex="-1"></a>              <span class="fu">rbern</span>(<span class="at">n =</span> N, .<span class="dv">5</span>)),</span>
<span id="cb1135-12"><a href="ch-mixture.html#cb1135-12" aria-hidden="true" tabindex="-1"></a>         <span class="at">diff =</span> <span class="fu">if_else</span>(x <span class="sc">==</span> .<span class="dv">5</span>, <span class="st">&quot;hard&quot;</span>, <span class="st">&quot;easy&quot;</span>),</span>
<span id="cb1135-13"><a href="ch-mixture.html#cb1135-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">emphasis =</span> <span class="fu">ifelse</span>(x2 <span class="sc">==</span> .<span class="dv">5</span>, <span class="st">&quot;accuracy&quot;</span>, <span class="st">&quot;speed&quot;</span>))</span></code></pre></div>
<p>Verify that the distribution of the simulated response times conditional on the simulated accuracy and the experimental manipulations make sense with the following plot shown in Figure <a href="ch-mixture.html#fig:simdatartacc">19.13</a>.</p>

<div class="sourceCode" id="cb1136"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1136-1"><a href="ch-mixture.html#cb1136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_dots_simdata, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(acc), <span class="at">y =</span> rt)) <span class="sc">+</span></span>
<span id="cb1136-2"><a href="ch-mixture.html#cb1136-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">position =</span> <span class="fu">position_jitter</span>(<span class="at">width =</span> .<span class="dv">4</span>, </span>
<span id="cb1136-3"><a href="ch-mixture.html#cb1136-3" aria-hidden="true" tabindex="-1"></a>                                        <span class="at">height =</span> <span class="dv">0</span>), <span class="at">alpha =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb1136-4"><a href="ch-mixture.html#cb1136-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(diff <span class="sc">~</span> emphasis) <span class="sc">+</span></span>
<span id="cb1136-5"><a href="ch-mixture.html#cb1136-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="sc">+</span></span>
<span id="cb1136-6"><a href="ch-mixture.html#cb1136-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:simdatartacc"></span>
<img src="bookdown_files/figure-html/simdatartacc-1.svg" alt="Distribution of response times conditional on the simulated accuracy and the experimental manipulations for the simulated hierarchical data (df_dots_simdata) that follows the fast-guess model." width="672" />
<p class="caption">
FIGURE 19.13: Distribution of response times conditional on the simulated accuracy and the experimental manipulations for the simulated hierarchical data (<code>df_dots_simdata</code>) that follows the fast-guess model.
</p>
</div>
<p>We implement the model in Stan as follows in <code>mixture_h.stan</code>. The hierarchical extension uses the Cholesky factorization for the group-level effects (as we did in <a href="ch-complexstan.html#sec-corrstan">11.1.3</a>).</p>
<div class="sourceCode" id="cb1137"><pre class="sourceCode stan fold-show"><code class="sourceCode stan"><span id="cb1137-1"><a href="ch-mixture.html#cb1137-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1137-2"><a href="ch-mixture.html#cb1137-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N;</span>
<span id="cb1137-3"><a href="ch-mixture.html#cb1137-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb1137-4"><a href="ch-mixture.html#cb1137-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] rt;</span>
<span id="cb1137-5"><a href="ch-mixture.html#cb1137-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> acc[N];</span>
<span id="cb1137-6"><a href="ch-mixture.html#cb1137-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x2; <span class="co">//speed or accuracy emphasis</span></span>
<span id="cb1137-7"><a href="ch-mixture.html#cb1137-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N_subj;</span>
<span id="cb1137-8"><a href="ch-mixture.html#cb1137-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>, <span class="kw">upper</span> = N_subj&gt; subj[N];</span>
<span id="cb1137-9"><a href="ch-mixture.html#cb1137-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1137-10"><a href="ch-mixture.html#cb1137-10" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1137-11"><a href="ch-mixture.html#cb1137-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb1137-12"><a href="ch-mixture.html#cb1137-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb1137-13"><a href="ch-mixture.html#cb1137-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma;</span>
<span id="cb1137-14"><a href="ch-mixture.html#cb1137-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">upper</span> = alpha&gt; gamma; <span class="co">//guessing</span></span>
<span id="cb1137-15"><a href="ch-mixture.html#cb1137-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma2;</span>
<span id="cb1137-16"><a href="ch-mixture.html#cb1137-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_correct;</span>
<span id="cb1137-17"><a href="ch-mixture.html#cb1137-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_btask;</span>
<span id="cb1137-18"><a href="ch-mixture.html#cb1137-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta_task;</span>
<span id="cb1137-19"><a href="ch-mixture.html#cb1137-19" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt;[<span class="dv">3</span>]  tau_u;   </span>
<span id="cb1137-20"><a href="ch-mixture.html#cb1137-20" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[<span class="dv">3</span>, N_subj] z_u;</span>
<span id="cb1137-21"><a href="ch-mixture.html#cb1137-21" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cholesky_factor_corr</span>[<span class="dv">3</span>] L_u;</span>
<span id="cb1137-22"><a href="ch-mixture.html#cb1137-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1137-23"><a href="ch-mixture.html#cb1137-23" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb1137-24"><a href="ch-mixture.html#cb1137-24" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N_subj, <span class="dv">3</span>] u;</span>
<span id="cb1137-25"><a href="ch-mixture.html#cb1137-25" aria-hidden="true" tabindex="-1"></a>  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;</span>
<span id="cb1137-26"><a href="ch-mixture.html#cb1137-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1137-27"><a href="ch-mixture.html#cb1137-27" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1137-28"><a href="ch-mixture.html#cb1137-28" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the task component</span></span>
<span id="cb1137-29"><a href="ch-mixture.html#cb1137-29" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1137-30"><a href="ch-mixture.html#cb1137-30" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta | <span class="dv">0</span>, .<span class="dv">1</span>);</span>
<span id="cb1137-31"><a href="ch-mixture.html#cb1137-31" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1137-32"><a href="ch-mixture.html#cb1137-32" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1137-33"><a href="ch-mixture.html#cb1137-33" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the guessing component</span></span>
<span id="cb1137-34"><a href="ch-mixture.html#cb1137-34" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(gamma | <span class="dv">6</span>, <span class="dv">1</span>) - </span>
<span id="cb1137-35"><a href="ch-mixture.html#cb1137-35" aria-hidden="true" tabindex="-1"></a>    normal_lcdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1137-36"><a href="ch-mixture.html#cb1137-36" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma2 | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1137-37"><a href="ch-mixture.html#cb1137-37" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1137-38"><a href="ch-mixture.html#cb1137-38" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(tau_u | <span class="dv">0</span>, .<span class="dv">5</span>)</span>
<span id="cb1137-39"><a href="ch-mixture.html#cb1137-39" aria-hidden="true" tabindex="-1"></a>    - <span class="dv">3</span>* normal_lccdf(<span class="dv">0</span> | <span class="dv">0</span>, .<span class="dv">5</span>);</span>
<span id="cb1137-40"><a href="ch-mixture.html#cb1137-40" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta_task | <span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb1137-41"><a href="ch-mixture.html#cb1137-41" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_correct | <span class="dv">995</span>, <span class="dv">5</span>);</span>
<span id="cb1137-42"><a href="ch-mixture.html#cb1137-42" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_btask | <span class="dv">8</span>, <span class="dv">2</span>);</span>
<span id="cb1137-43"><a href="ch-mixture.html#cb1137-43" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> lkj_corr_cholesky_lpdf(L_u | <span class="dv">2</span>);</span>
<span id="cb1137-44"><a href="ch-mixture.html#cb1137-44" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> std_normal_lpdf(to_vector(z_u));</span>
<span id="cb1137-45"><a href="ch-mixture.html#cb1137-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1137-46"><a href="ch-mixture.html#cb1137-46" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N){</span>
<span id="cb1137-47"><a href="ch-mixture.html#cb1137-47" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> lodds_task = logit(p_btask) + x2[n] * beta_task;</span>
<span id="cb1137-48"><a href="ch-mixture.html#cb1137-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">target +=</span> log_sum_exp(log_inv_logit(lodds_task) +</span>
<span id="cb1137-49"><a href="ch-mixture.html#cb1137-49" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | alpha + u[subj[n], <span class="dv">1</span>] +</span>
<span id="cb1137-50"><a href="ch-mixture.html#cb1137-50" aria-hidden="true" tabindex="-1"></a>                                         x[n] * (beta + u[subj[n], <span class="dv">2</span>]), sigma) +</span>
<span id="cb1137-51"><a href="ch-mixture.html#cb1137-51" aria-hidden="true" tabindex="-1"></a>                          bernoulli_lpmf(acc[n] | p_correct),</span>
<span id="cb1137-52"><a href="ch-mixture.html#cb1137-52" aria-hidden="true" tabindex="-1"></a>                          log1m_inv_logit(lodds_task) +</span>
<span id="cb1137-53"><a href="ch-mixture.html#cb1137-53" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | gamma + u[subj[n], <span class="dv">3</span>], sigma2) +</span>
<span id="cb1137-54"><a href="ch-mixture.html#cb1137-54" aria-hidden="true" tabindex="-1"></a>                          bernoulli_lpmf(acc[n] |.<span class="dv">5</span>));</span>
<span id="cb1137-55"><a href="ch-mixture.html#cb1137-55" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1137-56"><a href="ch-mixture.html#cb1137-56" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1137-57"><a href="ch-mixture.html#cb1137-57" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb1137-58"><a href="ch-mixture.html#cb1137-58" aria-hidden="true" tabindex="-1"></a>  <span class="dt">corr_matrix</span>[<span class="dv">3</span>] rho_u = L_u * L_u&#39;;</span>
<span id="cb1137-59"><a href="ch-mixture.html#cb1137-59" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Save the model code and fit it to the simulated data:</p>
<div class="sourceCode" id="cb1138"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1138-1"><a href="ch-mixture.html#cb1138-1" aria-hidden="true" tabindex="-1"></a>ls_dots_simdata <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">N =</span> N,</span>
<span id="cb1138-2"><a href="ch-mixture.html#cb1138-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">rt =</span> df_dots_simdata<span class="sc">$</span>rt,</span>
<span id="cb1138-3"><a href="ch-mixture.html#cb1138-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">x =</span> df_dots_simdata<span class="sc">$</span>x,</span>
<span id="cb1138-4"><a href="ch-mixture.html#cb1138-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">x2 =</span> df_dots_simdata<span class="sc">$</span>x2,</span>
<span id="cb1138-5"><a href="ch-mixture.html#cb1138-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">acc =</span> df_dots_simdata<span class="sc">$</span>acc,</span>
<span id="cb1138-6"><a href="ch-mixture.html#cb1138-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">subj =</span> df_dots_simdata<span class="sc">$</span>subj,</span>
<span id="cb1138-7"><a href="ch-mixture.html#cb1138-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">N_subj =</span> N_subj)</span>
<span id="cb1138-8"><a href="ch-mixture.html#cb1138-8" aria-hidden="true" tabindex="-1"></a>mixture_h <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb1138-9"><a href="ch-mixture.html#cb1138-9" aria-hidden="true" tabindex="-1"></a>                        <span class="st">&quot;mixture_h.stan&quot;</span>,</span>
<span id="cb1138-10"><a href="ch-mixture.html#cb1138-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb1138-11"><a href="ch-mixture.html#cb1138-11" aria-hidden="true" tabindex="-1"></a>fit_mix_h <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">file =</span> mixture_h,</span>
<span id="cb1138-12"><a href="ch-mixture.html#cb1138-12" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> ls_dots_simdata,</span>
<span id="cb1138-13"><a href="ch-mixture.html#cb1138-13" aria-hidden="true" tabindex="-1"></a>                  <span class="at">iter =</span> <span class="dv">3000</span>,</span>
<span id="cb1138-14"><a href="ch-mixture.html#cb1138-14" aria-hidden="true" tabindex="-1"></a>                  <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> .<span class="dv">9</span>))</span></code></pre></div>
<p>Print the posterior summary:</p>
<div class="sourceCode" id="cb1139"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1139-1"><a href="ch-mixture.html#cb1139-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_mix_h, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,</span>
<span id="cb1139-2"><a href="ch-mixture.html#cb1139-2" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;p_correct&quot;</span>,<span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>, <span class="st">&quot;tau_u&quot;</span>,</span>
<span id="cb1139-3"><a href="ch-mixture.html#cb1139-3" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;rho_u[1,2]&quot;</span>, <span class="st">&quot;rho_u[1,3]&quot;</span>, <span class="st">&quot;rho_u[2,3]&quot;</span>))</span></code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## alpha       5.73  5.65  5.82   821 1.00
## beta        0.05  0.00  0.09  6061 1.00
## sigma       0.40  0.38  0.42  8705 1.00
## gamma       5.16  4.93  5.38  2383 1.00
## sigma2      0.55  0.49  0.61  8497 1.00
## p_correct   1.00  0.99  1.00  8264 1.00
## p_btask     0.85  0.83  0.88  8341 1.00
## beta_task   0.07 -0.24  0.39  9331 1.00
## tau_u[1]    0.19  0.13  0.28  1549 1.00
## tau_u[2]    0.06  0.00  0.14  1499 1.00
## tau_u[3]    0.46  0.31  0.67  2519 1.00
## rho_u[1,2]  0.04 -0.60  0.64  7077 1.00
## rho_u[1,3]  0.04 -0.40  0.46  2717 1.00
## rho_u[2,3] -0.08 -0.71  0.58   615 1.01</code></pre>
<p>We see that we can fit the hierarchical extension of our model to simulated data. Next we’ll evaluate whether we can recover the true values of the parameters.</p>
<div id="recovery-of-the-parameters" class="section level4 hasAnchor" number="19.1.5.1">
<h4><span class="header-section-number">19.1.5.1</span> Recovery of the parameters<a href="ch-mixture.html#recovery-of-the-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>By “recovering” the true values of the parameters, we mean that the true values are somewhere inside the bulk of the posterior distribution of the model.</p>
<p>We use <code>mcmc_recover_hist</code> to compare the posterior distributions of the relevant parameters of the model with their true values in Figure <a href="ch-mixture.html#fig:recovermixt">19.14</a>.</p>

<div class="sourceCode" id="cb1141"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1141-1"><a href="ch-mixture.html#cb1141-1" aria-hidden="true" tabindex="-1"></a>df_fit_mix_h <span class="ot">&lt;-</span> fit_mix_h <span class="sc">%&gt;%</span> <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb1141-2"><a href="ch-mixture.html#cb1141-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,</span>
<span id="cb1141-3"><a href="ch-mixture.html#cb1141-3" aria-hidden="true" tabindex="-1"></a>           <span class="st">&quot;p_correct&quot;</span>,<span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>, <span class="st">&quot;tau_u[1]&quot;</span>,</span>
<span id="cb1141-4"><a href="ch-mixture.html#cb1141-4" aria-hidden="true" tabindex="-1"></a>           <span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;tau_u[3]&quot;</span>, <span class="st">&quot;rho_u[1,2]&quot;</span>, <span class="st">&quot;rho_u[1,3]&quot;</span>,</span>
<span id="cb1141-5"><a href="ch-mixture.html#cb1141-5" aria-hidden="true" tabindex="-1"></a>           <span class="st">&quot;rho_u[2,3]&quot;</span>))</span>
<span id="cb1141-6"><a href="ch-mixture.html#cb1141-6" aria-hidden="true" tabindex="-1"></a>true_values <span class="ot">&lt;-</span> <span class="fu">c</span>(alpha, beta, sigma, gamma, sigma2,</span>
<span id="cb1141-7"><a href="ch-mixture.html#cb1141-7" aria-hidden="true" tabindex="-1"></a>                p_correct, p_btask, beta_task, tau_u[<span class="dv">1</span>],</span>
<span id="cb1141-8"><a href="ch-mixture.html#cb1141-8" aria-hidden="true" tabindex="-1"></a>                tau_u[<span class="dv">2</span>], tau_u[<span class="dv">3</span>], <span class="fu">rep</span>(rho,<span class="dv">3</span>))</span>
<span id="cb1141-9"><a href="ch-mixture.html#cb1141-9" aria-hidden="true" tabindex="-1"></a><span class="fu">mcmc_recover_hist</span>(df_fit_mix_h, true_values)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:recovermixt"></span>
<img src="bookdown_files/figure-html/recovermixt-1.svg" alt="Posterior distributions of the main parameters of the mixture model fit_mix_h together with their true values." width="672" />
<p class="caption">
FIGURE 19.14: Posterior distributions of the main parameters of the mixture model <code>fit_mix_h</code> together with their true values.
</p>
</div>
<p>The model seems to be underestimating the probability of being correct of the subjects (<code>p_correct</code>) and overestimating the probability of being engaged in the task (<code>p_btask</code>). However, the numerical differences are very small. We can be relatively certain that the model is not seriously misspecified. As mentioned in previous chapters, a more principled (and computationally demanding) approach uses simulation based calibration (SBC) introduced in section <a href="ch-custom.html#sec-validSBC">12.2</a> of chapter <a href="ch-custom.html#ch-custom">12</a> <span class="citation">(and see also <a href="#ref-talts2018validating" role="doc-biblioref">Talts et al. 2018</a>; <a href="#ref-schad2020toward" role="doc-biblioref">Daniel J. Schad, Betancourt, and Vasishth 2020</a>)</span>.</p>
</div>
<div id="fitting-the-model-to-real-data" class="section level4 hasAnchor" number="19.1.5.2">
<h4><span class="header-section-number">19.1.5.2</span> Fitting the model to real data<a href="ch-mixture.html#fitting-the-model-to-real-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After verifying that our model works as expected, we are ready to fit it to real data. We code the predictors <span class="math inline">\(x\)</span> and <span class="math inline">\(x_2\)</span> as we did for the simulated data:</p>
<div class="sourceCode" id="cb1142"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1142-1"><a href="ch-mixture.html#cb1142-1" aria-hidden="true" tabindex="-1"></a>df_dots <span class="ot">&lt;-</span> df_dots <span class="sc">%&gt;%</span></span>
<span id="cb1142-2"><a href="ch-mixture.html#cb1142-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">x =</span> <span class="fu">if_else</span>(diff <span class="sc">==</span> <span class="st">&quot;easy&quot;</span>, <span class="sc">-</span>.<span class="dv">5</span>, .<span class="dv">5</span>),</span>
<span id="cb1142-3"><a href="ch-mixture.html#cb1142-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">x2 =</span> <span class="fu">if_else</span>(emphasis <span class="sc">==</span> <span class="st">&quot;accuracy&quot;</span>, .<span class="dv">5</span>, <span class="sc">-</span>.<span class="dv">5</span>))</span></code></pre></div>
<p>The main obstacle now is that fitting the entire data set takes around 12 hours! We’ll sample 600 observations of each subject as follows:</p>
<div class="sourceCode" id="cb1143"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1143-1"><a href="ch-mixture.html#cb1143-1" aria-hidden="true" tabindex="-1"></a>df_dots_data_short <span class="ot">&lt;-</span> df_dots <span class="sc">%&gt;%</span></span>
<span id="cb1143-2"><a href="ch-mixture.html#cb1143-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(subj) <span class="sc">%&gt;%</span> </span>
<span id="cb1143-3"><a href="ch-mixture.html#cb1143-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">600</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1144"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1144-1"><a href="ch-mixture.html#cb1144-1" aria-hidden="true" tabindex="-1"></a>ls_dots_data_short <span class="ot">&lt;-</span></span>
<span id="cb1144-2"><a href="ch-mixture.html#cb1144-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">N =</span> <span class="fu">nrow</span>(df_dots_data_short),</span>
<span id="cb1144-3"><a href="ch-mixture.html#cb1144-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">rt =</span> df_dots_data_short<span class="sc">$</span>rt,</span>
<span id="cb1144-4"><a href="ch-mixture.html#cb1144-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> df_dots_data_short<span class="sc">$</span>x,</span>
<span id="cb1144-5"><a href="ch-mixture.html#cb1144-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">x2 =</span> df_dots_data_short<span class="sc">$</span>x2,</span>
<span id="cb1144-6"><a href="ch-mixture.html#cb1144-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">acc =</span> df_dots_data_short<span class="sc">$</span>acc,</span>
<span id="cb1144-7"><a href="ch-mixture.html#cb1144-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">subj =</span> <span class="fu">as.numeric</span>(df_dots_data_short<span class="sc">$</span>subj),</span>
<span id="cb1144-8"><a href="ch-mixture.html#cb1144-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">N_subj =</span> <span class="fu">length</span>(<span class="fu">unique</span>(df_dots_data_short<span class="sc">$</span>subj)))</span>
<span id="cb1144-9"><a href="ch-mixture.html#cb1144-9" aria-hidden="true" tabindex="-1"></a>fit_mix_data <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">file =</span> mixture_h,</span>
<span id="cb1144-10"><a href="ch-mixture.html#cb1144-10" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> ls_dots_data_short,</span>
<span id="cb1144-11"><a href="ch-mixture.html#cb1144-11" aria-hidden="true" tabindex="-1"></a>                     <span class="at">chains =</span> <span class="dv">4</span>,</span>
<span id="cb1144-12"><a href="ch-mixture.html#cb1144-12" aria-hidden="true" tabindex="-1"></a>                     <span class="at">iter =</span> <span class="dv">2000</span>,</span>
<span id="cb1144-13"><a href="ch-mixture.html#cb1144-13" aria-hidden="true" tabindex="-1"></a>                     <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span>.<span class="dv">9</span>,</span>
<span id="cb1144-14"><a href="ch-mixture.html#cb1144-14" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">max_treedepth =</span> <span class="dv">12</span>))</span></code></pre></div>
<p>The model has not converged at all!</p>
<div class="sourceCode" id="cb1145"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1145-1"><a href="ch-mixture.html#cb1145-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_mix_data,</span>
<span id="cb1145-2"><a href="ch-mixture.html#cb1145-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,</span>
<span id="cb1145-3"><a href="ch-mixture.html#cb1145-3" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;p_correct&quot;</span>,<span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>, <span class="st">&quot;tau_u&quot;</span>,</span>
<span id="cb1145-4"><a href="ch-mixture.html#cb1145-4" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;rho_u[1,2]&quot;</span>, <span class="st">&quot;rho_u[1,3]&quot;</span>, <span class="st">&quot;rho_u[2,3]&quot;</span>))</span></code></pre></div>
<pre><code>##            mean  2.5% 97.5% n_eff  Rhat
## alpha      6.36  6.28  6.47     3  1.55
## beta       0.11  0.07  0.17     2  2.44
## sigma      0.24  0.22  0.29     2 11.65
## gamma      6.24  6.07  6.35     2  2.48
## sigma2     0.38  0.21  0.44     2 18.19
## p_correct  0.98  0.93  1.00     2 11.74
## p_btask    0.75  0.68  0.92     2  8.77
## beta_task  2.00  0.78  5.71     2 10.99
## tau_u[1]   0.14  0.10  0.20    40  1.06
## tau_u[2]   0.05  0.03  0.08    16  1.08
## tau_u[3]   0.18  0.12  0.27     7  1.18
## rho_u[1,2] 0.46  0.00  0.82    23  1.06
## rho_u[1,3] 0.16 -0.28  0.55   518  1.02
## rho_u[2,3] 0.02 -0.46  0.52    13  1.11</code></pre>
<p>The traceplots in Figure <a href="ch-mixture.html#fig:traceplotmulti">19.15</a> show that the chains are not mixing at all. It seems that the posterior is multimodal, and there are at least two combinations of parameters that would fit the data equally well.</p>

<div class="sourceCode" id="cb1147"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1147-1"><a href="ch-mixture.html#cb1147-1" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(fit_mix_data)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceplotmulti"></span>
<img src="bookdown_files/figure-html/traceplotmulti-1.svg" alt="Traceplots from the hierarchical model (mixture_h.stan) fit to (a subset) of the real data. The traceplot shows clearly that the posterior has at least two modes." width="672" />
<p class="caption">
FIGURE 19.15: Traceplots from the hierarchical model (<code>mixture_h.stan</code>) fit to (a subset) of the real data. The traceplot shows clearly that the posterior has at least two modes.
</p>
</div>
<p>What should we do now? It can be a good idea to back off and simplify the model. Once the simplified model converges, we can think about adding further complexity. The verbal description of our model says that the accuracy in the task-engaged mode should be close to 100%. To simplify the model, we’ll assume that it’s exactly 100%. This entails the following:</p>
<p><span class="math display">\[\begin{equation}
p_{correct} = 1
\end{equation}\]</span></p>
<p>We adapt our Stan code in <code>mixture_h2.stan</code> reflecting that <code>p_correct</code> is no longer a parameter and it’s now in a block called <code>transformed data</code>. There we assign to <code>p_correct</code> the value of <code>1</code>.</p>
<div class="sourceCode" id="cb1148"><pre class="sourceCode stan fold-show"><code class="sourceCode stan"><span id="cb1148-1"><a href="ch-mixture.html#cb1148-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1148-2"><a href="ch-mixture.html#cb1148-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N;</span>
<span id="cb1148-3"><a href="ch-mixture.html#cb1148-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb1148-4"><a href="ch-mixture.html#cb1148-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] rt;</span>
<span id="cb1148-5"><a href="ch-mixture.html#cb1148-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> acc[N];</span>
<span id="cb1148-6"><a href="ch-mixture.html#cb1148-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x2; <span class="co">//speed or accuracy emphasis</span></span>
<span id="cb1148-7"><a href="ch-mixture.html#cb1148-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>&gt; N_subj;</span>
<span id="cb1148-8"><a href="ch-mixture.html#cb1148-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span> = <span class="dv">1</span>, <span class="kw">upper</span> = N_subj&gt; subj[N];</span>
<span id="cb1148-9"><a href="ch-mixture.html#cb1148-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1148-10"><a href="ch-mixture.html#cb1148-10" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb1148-11"><a href="ch-mixture.html#cb1148-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> p_correct = <span class="dv">1</span>;</span>
<span id="cb1148-12"><a href="ch-mixture.html#cb1148-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1148-13"><a href="ch-mixture.html#cb1148-13" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1148-14"><a href="ch-mixture.html#cb1148-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb1148-15"><a href="ch-mixture.html#cb1148-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb1148-16"><a href="ch-mixture.html#cb1148-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma;</span>
<span id="cb1148-17"><a href="ch-mixture.html#cb1148-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">upper</span> = alpha&gt; gamma; <span class="co">//guessing</span></span>
<span id="cb1148-18"><a href="ch-mixture.html#cb1148-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; sigma2;</span>
<span id="cb1148-19"><a href="ch-mixture.html#cb1148-19" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>, <span class="kw">upper</span> = <span class="dv">1</span>&gt; p_btask;</span>
<span id="cb1148-20"><a href="ch-mixture.html#cb1148-20" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta_task;</span>
<span id="cb1148-21"><a href="ch-mixture.html#cb1148-21" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt;[<span class="dv">3</span>]  tau_u;   </span>
<span id="cb1148-22"><a href="ch-mixture.html#cb1148-22" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[<span class="dv">3</span>, N_subj] z_u;</span>
<span id="cb1148-23"><a href="ch-mixture.html#cb1148-23" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cholesky_factor_corr</span>[<span class="dv">3</span>] L_u;</span>
<span id="cb1148-24"><a href="ch-mixture.html#cb1148-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1148-25"><a href="ch-mixture.html#cb1148-25" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb1148-26"><a href="ch-mixture.html#cb1148-26" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N_subj, <span class="dv">3</span>] u;</span>
<span id="cb1148-27"><a href="ch-mixture.html#cb1148-27" aria-hidden="true" tabindex="-1"></a>  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;</span>
<span id="cb1148-28"><a href="ch-mixture.html#cb1148-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1148-29"><a href="ch-mixture.html#cb1148-29" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1148-30"><a href="ch-mixture.html#cb1148-30" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the task component</span></span>
<span id="cb1148-31"><a href="ch-mixture.html#cb1148-31" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1148-32"><a href="ch-mixture.html#cb1148-32" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta | <span class="dv">0</span>, .<span class="dv">1</span>);</span>
<span id="cb1148-33"><a href="ch-mixture.html#cb1148-33" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1148-34"><a href="ch-mixture.html#cb1148-34" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1148-35"><a href="ch-mixture.html#cb1148-35" aria-hidden="true" tabindex="-1"></a>  <span class="co">// priors for the guessing component</span></span>
<span id="cb1148-36"><a href="ch-mixture.html#cb1148-36" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(gamma | <span class="dv">6</span>, <span class="dv">1</span>) - </span>
<span id="cb1148-37"><a href="ch-mixture.html#cb1148-37" aria-hidden="true" tabindex="-1"></a>    normal_lcdf(alpha | <span class="dv">6</span>, <span class="dv">1</span>);</span>
<span id="cb1148-38"><a href="ch-mixture.html#cb1148-38" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(sigma2 | .<span class="dv">5</span>, .<span class="dv">2</span>)</span>
<span id="cb1148-39"><a href="ch-mixture.html#cb1148-39" aria-hidden="true" tabindex="-1"></a>    - normal_lccdf(<span class="dv">0</span> | .<span class="dv">5</span>, .<span class="dv">2</span>);</span>
<span id="cb1148-40"><a href="ch-mixture.html#cb1148-40" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(tau_u | <span class="dv">0</span>, .<span class="dv">5</span>)</span>
<span id="cb1148-41"><a href="ch-mixture.html#cb1148-41" aria-hidden="true" tabindex="-1"></a>    - <span class="dv">3</span>* normal_lccdf(<span class="dv">0</span> | <span class="dv">0</span>, .<span class="dv">5</span>);</span>
<span id="cb1148-42"><a href="ch-mixture.html#cb1148-42" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> normal_lpdf(beta_task | <span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb1148-43"><a href="ch-mixture.html#cb1148-43" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> beta_lpdf(p_btask | <span class="dv">8</span>, <span class="dv">2</span>);</span>
<span id="cb1148-44"><a href="ch-mixture.html#cb1148-44" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> lkj_corr_cholesky_lpdf(L_u | <span class="dv">2</span>);</span>
<span id="cb1148-45"><a href="ch-mixture.html#cb1148-45" aria-hidden="true" tabindex="-1"></a>  <span class="kw">target +=</span> std_normal_lpdf(to_vector(z_u));</span>
<span id="cb1148-46"><a href="ch-mixture.html#cb1148-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1148-47"><a href="ch-mixture.html#cb1148-47" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N){</span>
<span id="cb1148-48"><a href="ch-mixture.html#cb1148-48" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> lodds_task = logit(p_btask) + x2[n] * beta_task;</span>
<span id="cb1148-49"><a href="ch-mixture.html#cb1148-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">target +=</span> log_sum_exp(log_inv_logit(lodds_task) +</span>
<span id="cb1148-50"><a href="ch-mixture.html#cb1148-50" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | alpha + u[subj[n], <span class="dv">1</span>] +</span>
<span id="cb1148-51"><a href="ch-mixture.html#cb1148-51" aria-hidden="true" tabindex="-1"></a>                                         x[n] * (beta + u[subj[n], <span class="dv">2</span>]), sigma) +</span>
<span id="cb1148-52"><a href="ch-mixture.html#cb1148-52" aria-hidden="true" tabindex="-1"></a>                          bernoulli_lpmf(acc[n] | p_correct),</span>
<span id="cb1148-53"><a href="ch-mixture.html#cb1148-53" aria-hidden="true" tabindex="-1"></a>                          log1m_inv_logit(lodds_task) +</span>
<span id="cb1148-54"><a href="ch-mixture.html#cb1148-54" aria-hidden="true" tabindex="-1"></a>                          lognormal_lpdf(rt[n] | gamma + u[subj[n], <span class="dv">3</span>], sigma2) +</span>
<span id="cb1148-55"><a href="ch-mixture.html#cb1148-55" aria-hidden="true" tabindex="-1"></a>                          bernoulli_lpmf(acc[n] |.<span class="dv">5</span>));</span>
<span id="cb1148-56"><a href="ch-mixture.html#cb1148-56" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1148-57"><a href="ch-mixture.html#cb1148-57" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1148-58"><a href="ch-mixture.html#cb1148-58" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb1148-59"><a href="ch-mixture.html#cb1148-59" aria-hidden="true" tabindex="-1"></a>  <span class="dt">corr_matrix</span>[<span class="dv">3</span>] rho_u = L_u * L_u&#39;;</span>
<span id="cb1148-60"><a href="ch-mixture.html#cb1148-60" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Fit the model again to the same data:</p>
<div class="sourceCode" id="cb1149"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1149-1"><a href="ch-mixture.html#cb1149-1" aria-hidden="true" tabindex="-1"></a>fit_mixs_data <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">file =</span> mixture_h,</span>
<span id="cb1149-2"><a href="ch-mixture.html#cb1149-2" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> ls_dots_data_short,</span>
<span id="cb1149-3"><a href="ch-mixture.html#cb1149-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">chains =</span> <span class="dv">4</span>,</span>
<span id="cb1149-4"><a href="ch-mixture.html#cb1149-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">iter =</span> <span class="dv">2000</span>,</span>
<span id="cb1149-5"><a href="ch-mixture.html#cb1149-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span>.<span class="dv">9</span>,</span>
<span id="cb1149-6"><a href="ch-mixture.html#cb1149-6" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">max_treedepth =</span> <span class="dv">12</span>))</span></code></pre></div>
<p>The model has now converged.</p>
<div class="sourceCode" id="cb1150"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1150-1"><a href="ch-mixture.html#cb1150-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_mixs_data,</span>
<span id="cb1150-2"><a href="ch-mixture.html#cb1150-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,</span>
<span id="cb1150-3"><a href="ch-mixture.html#cb1150-3" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>, <span class="st">&quot;tau_u&quot;</span>,</span>
<span id="cb1150-4"><a href="ch-mixture.html#cb1150-4" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;rho_u[1,2]&quot;</span>, <span class="st">&quot;rho_u[1,3]&quot;</span>, <span class="st">&quot;rho_u[2,3]&quot;</span>))</span></code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## alpha       6.34  6.29  6.40   667    1
## beta        0.10  0.07  0.12  1841    1
## sigma       0.22  0.21  0.23  4727    1
## gamma       6.29  6.21  6.35  1254    1
## sigma2      0.43  0.42  0.44  4386    1
## p_btask     0.69  0.68  0.70  4686    1
## beta_task   0.88  0.75  1.02  4608    1
## tau_u[1]    0.14  0.10  0.20   942    1
## tau_u[2]    0.05  0.03  0.08  1737    1
## tau_u[3]    0.19  0.13  0.27  1552    1
## rho_u[1,2]  0.40 -0.07  0.75  2194    1
## rho_u[1,3]  0.15 -0.28  0.53  1390    1
## rho_u[2,3] -0.03 -0.48  0.44  1071    1</code></pre>
<p>The traceplots in Figure <a href="ch-mixture.html#fig:traceplotnomulti">19.16</a> show that this times the chains are mixing well.</p>

<div class="sourceCode" id="cb1152"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb1152-1"><a href="ch-mixture.html#cb1152-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1152-2"><a href="ch-mixture.html#cb1152-2" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(fit_mixs_data)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:traceplotnomulti"></span>
<img src="bookdown_files/figure-html/traceplotnomulti-1.svg" alt="Traceplots from the simplified hierarchical model (mixture_h.stan, assuming that p_correct = 1) fit to (a subset) of the real data. The traceplot shows that chains are mixing well." width="672" />
<p class="caption">
FIGURE 19.16: Traceplots from the simplified hierarchical model (<code>mixture_h.stan</code>, assuming that <code>p_correct = 1</code>) fit to (a subset) of the real data. The traceplot shows that chains are mixing well.
</p>
</div>
<p>What can we say about the fit of the model now?</p>
<p><em>Under the assumptions that we have made</em> (e.g., there are two processing modes, response times are affected by the difficulty of the task in the task-engaged mode, accuracy is not affected by the difficulty of the task and is perfect at the task-engaged mode, etc.), we can look at the parameters and conclude the following:</p>
<!-- - Subjects seemed to have a very high accuracy once they were engaged in the task. (`p_correct` is very high). -->
<ul>
<li>The instructions seemed to have a strong effect on the processing mode of the subjects (<code>beta_task</code> is relatively high), and in the expected direction (emphasis in accuracy led to a higher probability to be in the task engaged mode).</li>
<li>The guessing mode seemed to be much noisier than the task-engaged mode (compare <code>sigma</code> with <code>sigma2</code>).</li>
<li>Slow subjects seemed to show a stronger effect of the experimental manipulation (<code>rho_u1[1,2]</code> is mostly positive).</li>
</ul>
<p>If we want to know whether our model achieves descriptive adequacy, we need to look at the posterior predictive distributions of the model. However, by using posterior predictive checks, we won’t be able to conclude that our model is not overfitting. Our success in fitting the fast-guess model to real data does not entail that the model is a good account of the data. It just means that it’s flexible enough to fit the data. One further step could be to develop a competing model, and then comparing the performances of the models, using Bayes factors or cross validation.</p>
</div>
<div id="sec-ppdmixture" class="section level4 hasAnchor" number="19.1.5.3">
<h4><span class="header-section-number">19.1.5.3</span> Posterior predictive checks<a href="ch-mixture.html#sec-ppdmixture" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For the posterior predictive checks, we can write the generated quantities block in a new file. The advantage is that we can generate as many observations as needed <em>after estimating the parameters</em>. There is no model block in the follow Stan program. We use the <code>gqs()</code> function in the <code>rstan</code> library, which allows us to use the posterior draws from a previously fitted model to generate posterior predicted data.</p>
<div class="sourceCode" id="cb1153"><pre class="sourceCode stan fold-show"><code class="sourceCode stan"></code></pre></div>
<p>Generate responses from 500 simulated experiments as follows:</p>
<div class="sourceCode" id="cb1154"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1154-1"><a href="ch-mixture.html#cb1154-1" aria-hidden="true" tabindex="-1"></a>mixture_h_gen <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;stan_models&quot;</span>,</span>
<span id="cb1154-2"><a href="ch-mixture.html#cb1154-2" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;mixture_h_gen.stan&quot;</span>,</span>
<span id="cb1154-3"><a href="ch-mixture.html#cb1154-3" aria-hidden="true" tabindex="-1"></a>                            <span class="at">package =</span> <span class="st">&quot;bcogsci&quot;</span>)</span>
<span id="cb1154-4"><a href="ch-mixture.html#cb1154-4" aria-hidden="true" tabindex="-1"></a>gen_model <span class="ot">&lt;-</span> <span class="fu">stan_model</span>(mixture_h_gen)</span>
<span id="cb1154-5"><a href="ch-mixture.html#cb1154-5" aria-hidden="true" tabindex="-1"></a>draws_par <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(fit_mixs_data)[<span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>, ,drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb1154-6"><a href="ch-mixture.html#cb1154-6" aria-hidden="true" tabindex="-1"></a>gen_mix_data <span class="ot">&lt;-</span> <span class="fu">gqs</span>(gen_model,</span>
<span id="cb1154-7"><a href="ch-mixture.html#cb1154-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> ls_dots_data_short,</span>
<span id="cb1154-8"><a href="ch-mixture.html#cb1154-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">draws =</span> draws_par)</span></code></pre></div>
<p>We first take a look at the general distribution of response times generated by the posterior predictive model and by our real data in Figure <a href="ch-mixture.html#fig:postpmix">19.17</a>.</p>

<div class="sourceCode" id="cb1155"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1155-1"><a href="ch-mixture.html#cb1155-1" aria-hidden="true" tabindex="-1"></a>rt_pred <span class="ot">&lt;-</span> <span class="fu">extract</span>(gen_mix_data)<span class="sc">$</span>rt_pred </span>
<span id="cb1155-2"><a href="ch-mixture.html#cb1155-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ppc_dens_overlay</span>(<span class="at">y =</span> ls_dots_data_short<span class="sc">$</span>rt, <span class="at">yrep =</span> rt_pred[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,]) <span class="sc">+</span></span>
<span id="cb1155-3"><a href="ch-mixture.html#cb1155-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>)) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpmix"></span>
<img src="bookdown_files/figure-html/postpmix-1.svg" alt="Posterior predictive distribution of the hierarchical fast-guess model (using mixture_h_gen.stan) in comparison with the observed response times." width="672" />
<p class="caption">
FIGURE 19.17: Posterior predictive distribution of the hierarchical fast-guess model (using <code>mixture_h_gen.stan</code>) in comparison with the observed response times.
</p>
</div>
<p>We see that the distribution of the observed response times is narrower than the predictive distribution. We are generating response times that are more spread out than the real data. <!-- has heavier tails than the predictive distribution. This means that somewhere in our model we are failing to account for response time variability. --></p>
<!-- We see that we tend to generate some responses that are too large, but the general shape of the predictive distribution of the response times is fine. -->
<!-- If we want to plot the prior predicted distribution of differences in response time conditioning on task difficulty, we need to define a new function. Then we use the `bayesplot` function `ppc_stat()` that takes as an argument of `stat` any summary function. -->
<p>Next we examine the effect of the experimental manipulation in Figure <a href="ch-mixture.html#fig:postpmanip">19.18</a>: The posterior predictive check reveals that the model underestimates the observed effect of the experimental manipulation: the observed difference between response times is well outside the bulk of the predictive distribution.</p>

<div class="sourceCode" id="cb1156"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1156-1"><a href="ch-mixture.html#cb1156-1" aria-hidden="true" tabindex="-1"></a>meanrt_diff <span class="ot">&lt;-</span> <span class="cf">function</span>(rt){</span>
<span id="cb1156-2"><a href="ch-mixture.html#cb1156-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(rt[ls_dots_data_short<span class="sc">$</span>x <span class="sc">==</span> .<span class="dv">5</span>]) <span class="sc">-</span></span>
<span id="cb1156-3"><a href="ch-mixture.html#cb1156-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(rt[ls_dots_data_short<span class="sc">$</span>x <span class="sc">==</span> <span class="sc">-</span>.<span class="dv">5</span>])</span>
<span id="cb1156-4"><a href="ch-mixture.html#cb1156-4" aria-hidden="true" tabindex="-1"></a>}  </span>
<span id="cb1156-5"><a href="ch-mixture.html#cb1156-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ppc_stat</span>(ls_dots_data_short<span class="sc">$</span>rt,</span>
<span id="cb1156-6"><a href="ch-mixture.html#cb1156-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">yrep =</span> rt_pred,</span>
<span id="cb1156-7"><a href="ch-mixture.html#cb1156-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">stat =</span> meanrt_diff) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpmanip"></span>
<img src="bookdown_files/figure-html/postpmanip-1.svg" alt="Posterior predictive distribution (using mixture_h_gen.stan) of the difference in response time due to the experimental manipulation. The vertical bar shows the observed difference in the data." width="672" />
<p class="caption">
FIGURE 19.18: Posterior predictive distribution (using <code>mixture_h_gen.stan</code>) of the difference in response time due to the experimental manipulation. The vertical bar shows the observed difference in the data.
</p>
</div>
<p>Another important posterior predictive check includes comparing the fit of the model using a quantile probability plot, which is presented in the next chapter.</p>
<p>We also look at some instances of the predictive distribution. Figure <a href="ch-mixture.html#fig:postppdobs">19.19</a> shows a simulated data set in black overlaid onto the real observations in gray. As we noticed in Figure <a href="ch-mixture.html#fig:postpmix">19.17</a>, the model is predicting less variability than what we find in the data, especially when the emphasis is on accuracy.</p>
<div class="sourceCode" id="cb1157"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb1157-1"><a href="ch-mixture.html#cb1157-1" aria-hidden="true" tabindex="-1"></a>acc_pred <span class="ot">&lt;-</span> <span class="fu">extract</span>(gen_mix_data)<span class="sc">$</span>acc_pred</span>
<span id="cb1157-2"><a href="ch-mixture.html#cb1157-2" aria-hidden="true" tabindex="-1"></a>df_dots_pred <span class="ot">&lt;-</span></span>
<span id="cb1157-3"><a href="ch-mixture.html#cb1157-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">rt =</span> ls_dots_data_short<span class="sc">$</span>rt,</span>
<span id="cb1157-4"><a href="ch-mixture.html#cb1157-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">acc =</span> ls_dots_data_short<span class="sc">$</span>acc,</span>
<span id="cb1157-5"><a href="ch-mixture.html#cb1157-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">difficulty =</span> <span class="fu">ifelse</span>(ls_dots_data_short<span class="sc">$</span>x <span class="sc">==</span> .<span class="dv">5</span>,</span>
<span id="cb1157-6"><a href="ch-mixture.html#cb1157-6" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;hard&quot;</span>, <span class="st">&quot;easy&quot;</span>),</span>
<span id="cb1157-7"><a href="ch-mixture.html#cb1157-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">emphasis =</span> <span class="fu">ifelse</span>(ls_dots_data_short<span class="sc">$</span>x2 <span class="sc">==</span> <span class="sc">-</span>.<span class="dv">5</span>,</span>
<span id="cb1157-8"><a href="ch-mixture.html#cb1157-8" aria-hidden="true" tabindex="-1"></a>                           <span class="st">&quot;speed&quot;</span>, <span class="st">&quot;accuracy&quot;</span>),</span>
<span id="cb1157-9"><a href="ch-mixture.html#cb1157-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">acc_pred1 =</span> acc_pred[<span class="dv">1</span>,],</span>
<span id="cb1157-10"><a href="ch-mixture.html#cb1157-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">rt_pred1 =</span> rt_pred[<span class="dv">1</span>,])</span></code></pre></div>

<div class="figure"><span style="display:block;" id="fig:postppdobs"></span>
<img src="bookdown_files/figure-html/postppdobs-1.svg" alt="A simulated (posterior predictive) data set in black overlaid onto the observations in gray (based on mixture_h_gen.stan)." width="672" />
<p class="caption">
FIGURE 19.19: A simulated (posterior predictive) data set in black overlaid onto the observations in gray (based on <code>mixture_h_gen.stan</code>).
</p>
</div>
<p>If we would like to compare this model with a competing one using cross-validation, we would need to calculate the point-wise log-likelihood in the generated block:</p>
<pre><code>generated quantities {
  array[N] real log_lik;
  for(n in 1:N){
    real lodds_task = logit(p_btask) + x2[n] * beta_task;
    log_lik[n] += log_sum_exp(log_inv_logit(lodds_task) +
                                lognormal_lpdf(rt[n] | alpha + u[subj[n], 1] +
                                                 x[n] * (beta + u[subj[n], 2]), sigma) +
                                bernoulli_lpmf(acc[n] | p_correct),
                              log1m_inv_logit(lodds_task) +
                                lognormal_lpdf(rt[n] | gamma + u[subj[n], 3], sigma2) +
                                bernoulli_lpmf(acc[n] |.5));
  }
}
</code></pre>
<p>It’s important to bear in mind that we can only compare models on the same dependent variable(s). That is, we would need to compare this model with another one fit to the same dependent variables and also in the same scale: accuracy (<code>0</code> or <code>1</code>) and response time in milliseconds. This means that we cannot compare our fast-guess model with an accuracy-only model, for example. It also means that to compare our fast-guess model with a model based on left/right choices (known as stimulus coding, see section <a href="ch-lognormalrace.html#sec-acccoding">20.1.1</a>) and response times, we would need to reparametrize one of the two models; see exercise <a href="ch-lognormalrace.html#exr:lnldt">20.3</a> in the next chapter.</p>
<p>To conclude, the fast-guess model shows a relatively decent fit to the data and is able to account for the speed-accuracy trade-off. The model shows some inaccuracies that could lead to its revision and improvement. To what extent the inaccuracies are acceptable or not depends on (i) the empirical finding that we want to account for (for example, we can already assume that the model will struggle to fit data sets that show slow errors); and (ii) its comparison with a competing account.</p>
<!-- #### An alternative model and model comparison -->
<!-- Finally we implement a very simple model that posits that RTs and accuracy both depend on the difficulty of the task and ignores the emphasis manipulation. -->
<!-- ```{r,  echo = FALSE, eval = !file.exists("dataR/fit_mv_data.RDS"), echo = FALSE} -->
<!--  fit_mv_data <- stan(file = 'stan_models_remove/multiv.stan', -->
<!--                      data = ls_dots_data_short, -->
<!--                      control = list(adapt_delta =.9, max_treedepth = 12)) -->
<!-- ``` -->
</div>
</div>
</div>
<div id="summary-15" class="section level2 hasAnchor" number="19.2">
<h2><span class="header-section-number">19.2</span> Summary<a href="ch-mixture.html#summary-15" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned to fit increasingly complex two-component mixture models using Stan, starting with a simple model and ending with a fully hierarchical model. We saw how to evaluate model fit using the usual prior and posterior predictive checks, and to investigate parameter recovery. Such mixture models are notoriously difficult to fit, but they have a lot of potential in cognitive science applications, especially in developing computational models of different kinds of cognitive processes.</p>
</div>
<div id="further-reading-16" class="section level2 hasAnchor" number="19.3">
<h2><span class="header-section-number">19.3</span> Further reading<a href="ch-mixture.html#further-reading-16" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The reader interested in a deeper understanding of marginalization is referred to <span class="citation">Pullin, Gurrin, and Vukcevic (<a href="#ref-pullin2021statistical" role="doc-biblioref">2021</a>)</span>. Betancourt discusses problems of identification in Bayesian mixture models in a case study ( <a href="https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html" class="uri">https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html</a>). An in depth treatment of the fast-guess model and other mixture models of response times is provided in Chapter 7 of <span class="citation">Luce et al. (<a href="#ref-luce1986response" role="doc-biblioref">1986</a>)</span>.</p>
</div>
<div id="exercises-5" class="section level2 hasAnchor" number="19.4">
<h2><span class="header-section-number">19.4</span> Exercises<a href="ch-mixture.html#exercises-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- change the true value of `p_correct` to `.5` and to `.1` . -->
<!-- what happens if we weaken the prior of `p_correct`? -->
<!-- change the model by assuming that p_correct is exactly one, tip, you cannot use `bernoulli_lpdf( x | 1)`, because unless x is 1, the loglikelihood is -infinity and the sampler cannot find a way out of this space. The solution is to use an if-else statement. -->
<!-- prior predictive checks where alpha and gamma 6 , 2, do they make snse? -->
<div class="exercise">
<p><span id="exr:pcorrect" class="exercise"><strong>Exercise 19.1  </strong></span>Changes in the true values.</p>
</div>
<p>Change the true value of <code>p_correct</code> to .5 and .1, and generate data for the non-hierarchical model. Can you recover the value of this parameter without changing the model <code>mixture_rtacc2.stan</code>? Perform posterior predictive checks.</p>
<div class="exercise">
<p><span id="exr:mixhier" class="exercise"><strong>Exercise 19.2  </strong></span>RTs in schizophrenic patients and control.</p>
</div>
<p>Response times for schizophrenic patients in a simple visual tracking experiment show more variability than for non-schizophrenic controls; see Figure <a href="ch-mixture.html#fig:schiz">19.20</a>. It has been argued that at least some of this extra variability arises from an attentional lapse that delays some responses. We’ll use the data examined in <span class="citation">Belin and Rubin (<a href="#ref-belin1990" role="doc-biblioref">1990</a>)</span> (<code>df_schizophrenia</code> in the <code>bcogsci</code> package) analysis to investigate some potential models:</p>
<ul>
<li><span class="math inline">\(M_1\)</span>. Both schizophrenic and controls show attentional lapses, but the lapses are more common in schizophrenics. Other than that there is no difference in the latent response times and the lapses of attention.</li>
<li><span class="math inline">\(M_2\)</span>. Only schizophrenic patients show attentional lapses. Other than that there is no difference in the latent response times.</li>
<li><span class="math inline">\(M_3\)</span>. There no (meaningful number of) lapses of attention in neither group.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Fit the three models.</li>
<li>Do posterior predictive checks for each model; can they account for the data?</li>
<li>Do model comparison (with Bayes factor and cross-validation)</li>
</ol>

<div class="sourceCode" id="cb1159"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb1159-1"><a href="ch-mixture.html#cb1159-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1159-2"><a href="ch-mixture.html#cb1159-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_schizophrenia, <span class="fu">aes</span>(rt)) <span class="sc">+</span></span>
<span id="cb1159-3"><a href="ch-mixture.html#cb1159-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb1159-4"><a href="ch-mixture.html#cb1159-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="at">rows =</span> <span class="fu">vars</span>(<span class="fu">factor</span>(patient, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;control&quot;</span>, <span class="st">&quot;schizophrenic&quot;</span>))))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:schiz"></span>
<img src="bookdown_files/figure-html/schiz-1.svg" alt="Distribution of response times for control and schizophrenic patients in df_schizophrenia." width="672" />
<p class="caption">
FIGURE 19.20: Distribution of response times for control and schizophrenic patients in <code>df_schizophrenia</code>.
</p>
</div>
<div class="exercise">
<p><span id="exr:mixbias" class="exercise"><strong>Exercise 19.3  </strong></span><strong>Advanced:</strong> Guessing bias in the model.</p>
</div>
<p>In the original model, it was assumed that subjects might have a bias (a preference) to one of the two answers when they were in the guessing mode. To fit this model we need to change the dependent variable and add more information, now we not only care if the participant answered correctly or not, but also which answer they gave (left or right).</p>
<ul>
<li>Implement a unique bias for all the subjects. Fit the new model to (a subset of) the data.</li>
<li>Implement a hierarchical bias, that is there is a common bias, but every subject has its adjustment. Fit the new model to (a subset of) the data.</li>
</ul>
<!-- https://discourse.mc-stan.org/t/mixture-model-fails-on-cmdstanr-when-conditioning-theta-on-subject-but-not-on-rstan/22018 -->

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-belin1990" class="csl-entry">
Belin, TR, and DB Rubin. 1990. <span>“Analysis of a Finite Mixture Model with Variance Components.”</span> In <em>Proceedings of the Social Statistics Section</em>, 211–15.
</div>
<div id="ref-britten_shadlen_newsome_movshon_1993" class="csl-entry">
Britten, Kenneth H., Michael N. Shadlen, William T. Newsome, and J. Anthony Movshon. 1993. <span>“Responses of Neurons in Macaque MT to Stochastic Motion Signals.”</span> <em>Visual Neuroscience</em> 10 (6): 1157–69. <a href="https://doi.org/10.1017/S0952523800010269">https://doi.org/10.1017/S0952523800010269</a>.
</div>
<div id="ref-brownSimplestCompleteModel2008" class="csl-entry">
Brown, Scott D., and Andrew Heathcote. 2008. <span>“The Simplest Complete Model of Choice Response Time: <span>Linear</span> Ballistic Accumulation.”</span> <em>Cognitive Psychology</em> 57 (3): 153–78. <a href="https://doi.org/10.1016/j.cogpsych.2007.12.002">https://doi.org/10.1016/j.cogpsych.2007.12.002</a>.
</div>
<div id="ref-Dutilh2019quality" class="csl-entry">
Dutilh, Gilles, Jeffrey Annis, Scott D Brown, Peter Cassey, Nathan J Evans, Raoul PPP Grasman, Guy E Hawkins, et al. 2019. <span>“The Quality of Response Time Data Inference: <span>A</span> Blinded, Collaborative Assessment of the Validity of Cognitive Models.”</span> <em>Psychonomic Bulletin &amp; Review</em> 26 (4): 1051–69.
</div>
<div id="ref-DutilhEtAl2011" class="csl-entry">
Dutilh, Gilles, Eric-Jan Wagenmakers, Ingmar Visser, and Han L. J. van der Maas. 2011. <span>“A Phase Transition Model for the Speed-Accuracy Trade-Off in Response Time Experiments.”</span> <em>Cognitive Science</em> 35 (2): 211–50. <a href="https://doi.org/10.1111/j.1551-6709.2010.01147.x">https://doi.org/10.1111/j.1551-6709.2010.01147.x</a>.
</div>
<div id="ref-DingEtAl" class="csl-entry">
Han, Ding, Jana Wegrzyn, Hua Bi, Ruihua Wei, Bin Zhang, and Xiaorong Li. 2018. <span>“Practice Makes the Deficiency of Global Motion Detection in People with Pattern-Related Visual Stress More Apparent.”</span> <em>PLOS ONE</em> 13 (2): 1–13. <a href="https://doi.org/10.1371/journal.pone.0193215">https://doi.org/10.1371/journal.pone.0193215</a>.
</div>
<div id="ref-HeathcoteLove2012" class="csl-entry">
Heathcote, Andrew, and Jonathon Love. 2012. <span>“Linear Deterministic Accumulator Models of Simple Choice.”</span> <em>Frontiers in Psychology</em> 3: 292. <a href="https://doi.org/10.3389/fpsyg.2012.00292">https://doi.org/10.3389/fpsyg.2012.00292</a>.
</div>
<div id="ref-levy1993eye" class="csl-entry">
Levy, Deborah L, Philip S Holzman, Steven Matthysse, and Nancy R Mendell. 1993. <span>“Eye Tracking Dysfunction and Schizophrenia: <span>A</span> Critical Perspective.”</span> <em>Schizophrenia Bulletin</em> 19 (3): 461–536.
</div>
<div id="ref-luce1986response" class="csl-entry">
Luce, R Duncan et al. 1986. <em>Response Times: <span>Their</span> Role in Inferring Elementary Mental Organization</em>. 8. Oxford University Press on Demand.
</div>
<div id="ref-lunn2012bugs" class="csl-entry">
Lunn, David, Chris Jackson, David J Spiegelhalter, Nicky Best, and Andrew Thomas. 2012. <em>The <span>BUGS</span> Book: <span>A</span> Practical Introduction to <span>B</span>ayesian Analysis</em>. Vol. 98. CRC Press.
</div>
<div id="ref-Mcelree2000" class="csl-entry">
McElree, Brian. 2000. <span>“Sentence Comprehension Is Mediated by Content-Addressable Memory Structures.”</span> <em>Journal of Psycholinguistic Research</em> 29 (2): 111–23.
</div>
<div id="ref-nicenboimModelsRetrievalSentence2018" class="csl-entry">
———. 2018. <span>“Models of Retrieval in Sentence Comprehension: <span>A</span> Computational Evaluation Using <span>Bayesian</span> Hierarchical Modeling.”</span> <em>Journal of Memory and Language</em> 99: 1–34. <a href="https://doi.org/10.1016/j.jml.2017.08.004">https://doi.org/10.1016/j.jml.2017.08.004</a>.
</div>
<div id="ref-Ollman1966" class="csl-entry">
Ollman, Robert. 1966. <span>“Fast Guesses in Choice Reaction Time.”</span> <em>Psychonomic Science</em> 6 (4): 155–56.
</div>
<div id="ref-plummer2016jags" class="csl-entry">
Plummer, Martin. 2016. <span>“JAGS Version 4.2.0 User Manual.”</span>
</div>
<div id="ref-pullin2021statistical" class="csl-entry">
Pullin, Jeffrey, Lyle Gurrin, and Damjan Vukcevic. 2021. <span>“Statistical Models of Repeated Categorical Ratings: The r Package Rater.”</span> <a href="https://arxiv.org/abs/2010.09335">https://arxiv.org/abs/2010.09335</a>.
</div>
<div id="ref-Ratcliff1978" class="csl-entry">
Ratcliff, Roger. 1978. <span>“A Theory of Memory Retrieval.”</span> <em>Psychological Review</em> 85 (2): 59.
</div>
<div id="ref-Ratcliff2016" class="csl-entry">
Ratcliff, Roger, Philip L. Smith, Scott D. Brown, and Gail McKoon. 2016. <span>“Diffusion Decision Model: Current Issues and History.”</span> <em>Trends in Cognitive Sciences</em> 20 (4): 260–81. https://doi.org/<a href="https://doi.org/10.1016/j.tics.2016.01.007">https://doi.org/10.1016/j.tics.2016.01.007</a>.
</div>
<div id="ref-RouderEtAl2015" class="csl-entry">
Rouder, Jeffrey N., Jordan M. Province, Richard D. Morey, Pablo Gomez, and Andrew Heathcote. 2015. <span>“The Lognormal Race: <span>A</span> Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties.”</span> <em>Psychometrika</em> 80 (2): 491–513. <a href="https://doi.org/10.1007/s11336-013-9396-3">https://doi.org/10.1007/s11336-013-9396-3</a>.
</div>
<div id="ref-schad2020toward" class="csl-entry">
Schad, Daniel J., Michael J. Betancourt, and Shravan Vasishth. 2020. <span>“Toward a Principled <span>Bayesian</span> Workflow in Cognitive Science.”</span> <em>Psychological Methods</em> 26 (1): 103–26.
</div>
<div id="ref-talts2018validating" class="csl-entry">
Talts, Sean, Michael J. Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2018. <span>“Validating Bayesian Inference Algorithms with Simulation-Based Calibration.”</span> <em>arXiv Preprint arXiv:1804.06788</em>.
</div>
<div id="ref-wickelgren1977speed" class="csl-entry">
Wickelgren, Wayne A. 1977. <span>“Speed-Accuracy Tradeoff and Information Processing Dynamics.”</span> <em>Acta Psychologica</em> 41 (1): 67–85.
</div>
<div id="ref-YackulicEtAl2020" class="csl-entry">
Yackulic, Charles B., Michael Dodrill, Maria Dzul, Jamie S. Sanderlin, and Janice A. Reid. 2020. <span>“A Need for Speed in Bayesian Population Models: A Practical Guide to Marginalizing and Recovering Discrete Latent States.”</span> <em>Ecological Applications</em> 30 (5): e02112. https://doi.org/<a href="https://doi.org/10.1002/eap.2112">https://doi.org/10.1002/eap.2112</a>.
</div>
<div id="ref-Yellott1967" class="csl-entry">
Yellott, John I. 1967. <span>“Correction for Guessing in Choice Reaction Time.”</span> <em>Psychonomic Science</em> 8 (8): 321–22. <a href="https://doi.org/10.3758/BF03331682">https://doi.org/10.3758/BF03331682</a>.
</div>
<div id="ref-Yellott1971" class="csl-entry">
———. 1971. <span>“Correction for Fast Guessing and the Speed-Accuracy Tradeoff in Choice Reaction Time.”</span> <em>Journal of Mathematical Psychology</em> 8 (2): 159–99. <a href="https://doi.org/10.1016/0022-2496(71)90011-3">https://doi.org/10.1016/0022-2496(71)90011-3</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="51">
<li id="fn51"><p>See section <a href="ch-intro.html#sec-marginalizing">1.6.1.1</a> in chapter <a href="ch-intro.html#ch-intro">1</a> for a review on the concept of marginalization.<a href="ch-mixture.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p>As mentioned above, other probabilistic languages that do not rely on Hamiltonian dynamics (exclusively) are able to deal with this. However, even when sampling discrete parameters is possible, marginalization is more efficient <span class="citation">(<a href="#ref-YackulicEtAl2020" role="doc-biblioref">Yackulic et al. 2020</a>)</span>: when <span class="math inline">\(z_n\)</span> is omitted we are fitting a model with <span class="math inline">\(n\)</span> parameters fewer.<a href="ch-mixture.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p>Ollman original model was meant to be relevant for only means, and Yellott <span class="citation">(<a href="#ref-Yellott1967" role="doc-biblioref">1967</a>, <a href="#ref-Yellott1971" role="doc-biblioref">1971</a>)</span> generalized it to a distributional form.<a href="ch-mixture.html#fnref53" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-MPT.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-lognormalrace.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
