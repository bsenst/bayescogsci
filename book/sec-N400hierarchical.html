<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.1 A hierarchical normal model: The N400 effect | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.22.8 and GitBook 2.6.7" />

  <meta property="og:title" content="5.1 A hierarchical normal model: The N400 effect | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.1 A hierarchical normal model: The N400 effect | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci//images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2021-07-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-hierarchical.html"/>
<link rel="next" href="sec-stroop.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>



<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="the-law-of-total-probability.html"><a href="the-law-of-total-probability.html"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs.Â density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-2-continuous-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#sec:generatebivariatedata"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sec-marginal.html"><a href="sec-marginal.html"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.9</b> Summary</a></li>
<li class="chapter" data-level="1.10" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html"><i class="fa fa-check"></i><b>1.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-introBDA.html"><a href="ch-introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>2.1</b> Bayesâ rule</a></li>
<li class="chapter" data-level="2.2" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.2</b> Deriving the posterior using Bayesâ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.2.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.2.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.2.2" data-path="sec-analytical.html"><a href="sec-analytical.html#sec:choosepriortheta"><i class="fa fa-check"></i><b>2.2.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.2.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.2.3</b> Using Bayesâ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.2.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.2.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.2.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.2.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.2.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.2.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.2.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.2.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
<li class="chapter" data-level="2.4" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.4</b> Further reading</a></li>
<li class="chapter" data-level="2.5" data-path="sec-BDAexercises.html"><a href="sec-BDAexercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Regression models with brms</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="sec-sampling.html"><a href="sec-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec-sampling.html"><a href="sec-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using âStanâ: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect response times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#sec:comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="sec-LMexercises.html"><a href="sec-LMexercises.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal modelsâDistributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort.html"><a href="why-fitting-a-bayesian-hierarchical-model-is-worth-the-effort.html"><i class="fa fa-check"></i><b>5.3</b> Why fitting a Bayesian hierarchical model is worth the effort</a></li>
<li class="chapter" data-level="5.4" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>5.4</b> Summary</a></li>
<li class="chapter" data-level="5.5" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.5</b> Further reading</a></li>
<li class="chapter" data-level="5.6" data-path="sec-HLMexercises.html"><a href="sec-HLMexercises.html"><i class="fa fa-check"></i><b>5.6</b> Exercises</a><ul>
<li class="chapter" data-level="5.6.1" data-path="sec-HLMexercises.html"><a href="sec-HLMexercises.html#exercises-with-a-normal-likelihood"><i class="fa fa-check"></i><b>5.6.1</b> Exercises with a normal likelihood</a></li>
<li class="chapter" data-level="5.6.2" data-path="sec-HLMexercises.html"><a href="sec-HLMexercises.html#exercises-with-a-log-normal-likelihood"><i class="fa fa-check"></i><b>5.6.2</b> Exercises with a log-normal likelihood</a></li>
<li class="chapter" data-level="5.6.3" data-path="sec-HLMexercises.html"><a href="sec-HLMexercises.html#exercises-with-a-logistic-regression-bernoulli-likelihood."><i class="fa fa-check"></i><b>5.6.3</b> Exercises with a logistic regression (Bernoulli likelihood).</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-priors.html"><a href="ch-priors.html"><i class="fa fa-check"></i><b>6</b> The Art and Science of Prior Elicitation</a><ul>
<li class="chapter" data-level="6.1" data-path="sec-simpleexamplepriors.html"><a href="sec-simpleexamplepriors.html"><i class="fa fa-check"></i><b>6.1</b> Eliciting priors from oneself for a self-paced reading study: A simple example</a></li>
<li class="chapter" data-level="6.2" data-path="eliciting-priors-from-experts.html"><a href="eliciting-priors-from-experts.html"><i class="fa fa-check"></i><b>6.2</b> Eliciting priors from experts</a></li>
<li class="chapter" data-level="6.3" data-path="deriving-priors-from-meta-analyses.html"><a href="deriving-priors-from-meta-analyses.html"><i class="fa fa-check"></i><b>6.3</b> Deriving priors from meta-analyses</a></li>
<li class="chapter" data-level="6.4" data-path="using-previous-experiments-posteriors-as-priors-for-a-new-study.html"><a href="using-previous-experiments-posteriors-as-priors-for-a-new-study.html"><i class="fa fa-check"></i><b>6.4</b> Using previous experimentsâ posteriors as priors for a new study</a></li>
<li class="chapter" data-level="6.5" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-workflow.html"><a href="ch-workflow.html"><i class="fa fa-check"></i><b>7</b> Workflow</a><ul>
<li class="chapter" data-level="7.1" data-path="model-building.html"><a href="model-building.html"><i class="fa fa-check"></i><b>7.1</b> Model building</a></li>
<li class="chapter" data-level="7.2" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html"><i class="fa fa-check"></i><b>7.2</b> Principled questions on a model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#prior-predictive-checks-checking-consistency-with-domain-expertise"><i class="fa fa-check"></i><b>7.2.1</b> Prior predictive checks: Checking consistency with domain expertise</a></li>
<li class="chapter" data-level="7.2.2" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#computational-faithfulness-testing-for-correct-posterior-approximations"><i class="fa fa-check"></i><b>7.2.2</b> Computational faithfulness: Testing for correct posterior approximations</a></li>
<li class="chapter" data-level="7.2.3" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#model-sensitivity"><i class="fa fa-check"></i><b>7.2.3</b> Model sensitivity</a></li>
<li class="chapter" data-level="7.2.4" data-path="principled-questions-on-a-model.html"><a href="principled-questions-on-a-model.html#posterior-predictive-checks-does-the-model-adequately-capture-the-data"><i class="fa fa-check"></i><b>7.2.4</b> Posterior predictive checks: Does the model adequately capture the data?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html"><i class="fa fa-check"></i><b>7.3</b> Exemplary data analysis</a><ul>
<li class="chapter" data-level="7.3.1" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#prior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Prior predictive checks</a></li>
<li class="chapter" data-level="7.3.2" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#adjusting-priors"><i class="fa fa-check"></i><b>7.3.2</b> Adjusting priors</a></li>
<li class="chapter" data-level="7.3.3" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#computational-faithfulness-and-model-sensitivity"><i class="fa fa-check"></i><b>7.3.3</b> Computational faithfulness and model sensitivity</a></li>
<li class="chapter" data-level="7.3.4" data-path="exemplary-data-analysis.html"><a href="exemplary-data-analysis.html#posterior-predictive-checks-model-adequacy"><i class="fa fa-check"></i><b>7.3.4</b> Posterior predictive checks: Model adequacy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>7.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>8</b> Contrast coding</a><ul>
<li class="chapter" data-level="8.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html"><i class="fa fa-check"></i><b>8.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="8.1.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#treatmentcontrasts"><i class="fa fa-check"></i><b>8.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="8.1.2" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#inverseMatrix"><i class="fa fa-check"></i><b>8.1.2</b> Defining comparisons</a></li>
<li class="chapter" data-level="8.1.3" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#effectcoding"><i class="fa fa-check"></i><b>8.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.1.4" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#sec:cellMeans"><i class="fa fa-check"></i><b>8.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><i class="fa fa-check"></i><b>8.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="8.2.1" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#sumcontrasts"><i class="fa fa-check"></i><b>8.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="8.2.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>8.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="8.2.3" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>8.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html"><i class="fa fa-check"></i><b>8.3</b> Other types of contrasts: illustration with a factor with four levels</a><ul>
<li class="chapter" data-level="8.3.1" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#repeatedcontrasts"><i class="fa fa-check"></i><b>8.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="8.3.2" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#helmertcontrasts"><i class="fa fa-check"></i><b>8.3.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="8.3.3" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>8.3.3</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="8.3.4" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#polynomialContrasts"><i class="fa fa-check"></i><b>8.3.4</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="8.3.5" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#an-alternative-to-contrasts-monotonic-effects"><i class="fa fa-check"></i><b>8.3.5</b> An alternative to contrasts: monotonic effects</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html"><i class="fa fa-check"></i><b>8.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="8.4.1" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#centered-contrasts"><i class="fa fa-check"></i><b>8.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="8.4.2" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>8.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="8.4.3" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>8.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="computing-condition-means-from-estimated-contrasts.html"><a href="computing-condition-means-from-estimated-contrasts.html"><i class="fa fa-check"></i><b>8.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="8.6" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>8.7</b> Further reading</a></li>
<li class="chapter" data-level="8.8" data-path="sec-Contrastsexercises.html"><a href="sec-Contrastsexercises.html"><i class="fa fa-check"></i><b>8.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>9</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="9.1" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html"><i class="fa fa-check"></i><b>9.1</b> Contrast coding in a factorial <span class="math inline">\(2 \times 2\)</span> design</a><ul>
<li class="chapter" data-level="9.1.1" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html#nestedEffects"><i class="fa fa-check"></i><b>9.1.1</b> Nested effects</a></li>
<li class="chapter" data-level="9.1.2" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>9.1.2</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html"><i class="fa fa-check"></i><b>9.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>9.2.1</b> Estimating a group difference and controlling for a covariate</a></li>
<li class="chapter" data-level="9.2.2" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>9.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sec-interactions-NLM.html"><a href="sec-interactions-NLM.html"><i class="fa fa-check"></i><b>9.3</b> Interactions in generalized linear models (with non-linear link functions) and non-linear models</a></li>
<li class="chapter" data-level="9.4" data-path="summary-8.html"><a href="summary-8.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="further-readings.html"><a href="further-readings.html"><i class="fa fa-check"></i><b>9.5</b> Further readings</a></li>
<li class="chapter" data-level="9.6" data-path="sec-Contrasts2x2exercises.html"><a href="sec-Contrasts2x2exercises.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced models with Stan</b></span></li>
<li class="chapter" data-level="10" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>10</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="10.1" data-path="stan-syntax.html"><a href="stan-syntax.html"><i class="fa fa-check"></i><b>10.1</b> Stan syntax</a></li>
<li class="chapter" data-level="10.2" data-path="sec-firststan.html"><a href="sec-firststan.html"><i class="fa fa-check"></i><b>10.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="10.3" data-path="sec-clozestan.html"><a href="sec-clozestan.html"><i class="fa fa-check"></i><b>10.3</b> Another simple example: Cloze probability with Stan with the Binomial likelihood</a></li>
<li class="chapter" data-level="10.4" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html"><i class="fa fa-check"></i><b>10.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="10.4.1" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:pupilstan"><i class="fa fa-check"></i><b>10.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="10.4.2" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:interstan"><i class="fa fa-check"></i><b>10.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="10.4.3" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:logisticstan"><i class="fa fa-check"></i><b>10.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="summary-9.html"><a href="summary-9.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="further-reading-8.html"><a href="further-reading-8.html"><i class="fa fa-check"></i><b>10.6</b> Further reading</a></li>
<li class="chapter" data-level="10.7" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>11</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-hierstan.html"><a href="sec-hierstan.html"><i class="fa fa-check"></i><b>11.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="11.1.1" data-path="sec-hierstan.html"><a href="sec-hierstan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>11.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec-hierstan.html"><a href="sec-hierstan.html#sec:uncorrstan"><i class="fa fa-check"></i><b>11.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="11.1.3" data-path="sec-hierstan.html"><a href="sec-hierstan.html#sec:corrstan"><i class="fa fa-check"></i><b>11.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="11.1.4" data-path="sec-hierstan.html"><a href="sec-hierstan.html#sec:crosscorrstan"><i class="fa fa-check"></i><b>11.1.4</b> By-subject and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="summary-10.html"><a href="summary-10.html"><i class="fa fa-check"></i><b>11.2</b> Summary</a></li>
<li class="chapter" data-level="11.3" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
<li class="chapter" data-level="11.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-custom.html"><a href="ch-custom.html"><i class="fa fa-check"></i><b>12</b> Custom distributions in Stan</a></li>
<li class="part"><span><b>IV Other useful models</b></span></li>
<li class="chapter" data-level="13" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>13</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="13.1" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>13.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="13.1.1" data-path="meta-analysis.html"><a href="meta-analysis.html#a-meta-analysis-of-similarity-based-interference-in-sentence-comprehension"><i class="fa fa-check"></i><b>13.1.1</b> A meta-analysis of similarity-based interference in sentence comprehension</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>13.2</b> Measurement-error models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="measurement-error-models.html"><a href="measurement-error-models.html#accounting-for-measurement-error-in-a-voice-onset-time-model"><i class="fa fa-check"></i><b>13.2.1</b> Accounting for measurement error in a voice onset time model</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="summary-11.html"><a href="summary-11.html"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>13.4</b> Further reading</a></li>
<li class="chapter" data-level="13.5" data-path="sec-REMAMEexercises.html"><a href="sec-REMAMEexercises.html"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-sat.html"><a href="ch-sat.html"><i class="fa fa-check"></i><b>14</b> SAT</a></li>
<li class="part"><span><b>V Model comparison and hypothesis testing</b></span></li>
<li class="chapter" data-level="15" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>15</b> Introduction to model comparison</a><ul>
<li class="chapter" data-level="15.1" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>15.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>16</b> Bayes factors</a><ul>
<li class="chapter" data-level="16.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html"><i class="fa fa-check"></i><b>16.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="16.1.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#marginal-likelihood"><i class="fa fa-check"></i><b>16.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="16.1.2" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#bayes-factor"><i class="fa fa-check"></i><b>16.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html"><i class="fa fa-check"></i><b>16.2</b> Examining the N400 effect with Bayes factor</a><ul>
<li class="chapter" data-level="16.2.1" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>16.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="16.2.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sec:BFnonnested"><i class="fa fa-check"></i><b>16.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest.html"><a href="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest.html"><i class="fa fa-check"></i><b>16.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="16.4" data-path="bayes-factor-in-stan.html"><a href="bayes-factor-in-stan.html"><i class="fa fa-check"></i><b>16.4</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="16.5" data-path="bayes-factors-in-theory-and-in-practice.html"><a href="bayes-factors-in-theory-and-in-practice.html"><i class="fa fa-check"></i><b>16.5</b> Bayes factors in theory and in practice</a><ul>
<li class="chapter" data-level="16.5.1" data-path="bayes-factors-in-theory-and-in-practice.html"><a href="bayes-factors-in-theory-and-in-practice.html#bayes-factors-in-theory-stability-and-accuracy"><i class="fa fa-check"></i><b>16.5.1</b> Bayes factors in theory: Stability and accuracy</a></li>
<li class="chapter" data-level="16.5.2" data-path="bayes-factors-in-theory-and-in-practice.html"><a href="bayes-factors-in-theory-and-in-practice.html#bayes-factors-in-practice-variability-with-the-data"><i class="fa fa-check"></i><b>16.5.2</b> Bayes factors in practice: Variability with the data</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="summary-12.html"><a href="summary-12.html"><i class="fa fa-check"></i><b>16.6</b> Summary</a></li>
<li class="chapter" data-level="16.7" data-path="further-reading-12.html"><a href="further-reading-12.html"><i class="fa fa-check"></i><b>16.7</b> Further reading</a></li>
<li class="chapter" data-level="16.8" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>16.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>17</b> Cross-validation</a><ul>
<li class="chapter" data-level="17.1" data-path="expected-log-predictive-density-of-a-model.html"><a href="expected-log-predictive-density-of-a-model.html"><i class="fa fa-check"></i><b>17.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="17.2" data-path="k-fold-and-leave-one-out-cross-validation.html"><a href="k-fold-and-leave-one-out-cross-validation.html"><i class="fa fa-check"></i><b>17.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="17.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html"><i class="fa fa-check"></i><b>17.3</b> Testing the N400 effect using cross-validation</a><ul>
<li class="chapter" data-level="17.3.1" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html#cross-validation-with-psis-loo"><i class="fa fa-check"></i><b>17.3.1</b> Cross-validation with PSIS-LOO</a></li>
<li class="chapter" data-level="17.3.2" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html#cross-validation-with-k-fold"><i class="fa fa-check"></i><b>17.3.2</b> Cross-validation with K-fold</a></li>
<li class="chapter" data-level="17.3.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html#leave-one-group-out-cross-validation"><i class="fa fa-check"></i><b>17.3.3</b> Leave-one-group-out cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="sec-logcv.html"><a href="sec-logcv.html"><i class="fa fa-check"></i><b>17.4</b> Comparing different likelihoods with cross-validation</a></li>
<li class="chapter" data-level="17.5" data-path="issues-with-cross-validation.html"><a href="issues-with-cross-validation.html"><i class="fa fa-check"></i><b>17.5</b> Issues with cross-validation</a></li>
<li class="chapter" data-level="17.6" data-path="cross-validation-in-stan.html"><a href="cross-validation-in-stan.html"><i class="fa fa-check"></i><b>17.6</b> Cross-validation in Stan</a><ul>
<li class="chapter" data-level="17.6.1" data-path="cross-validation-in-stan.html"><a href="cross-validation-in-stan.html#psis-loo-cv-in-stan"><i class="fa fa-check"></i><b>17.6.1</b> PSIS-LOO-CV in Stan</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="summary-13.html"><a href="summary-13.html"><i class="fa fa-check"></i><b>17.7</b> Summary</a></li>
<li class="chapter" data-level="17.8" data-path="further-reading-13.html"><a href="further-reading-13.html"><i class="fa fa-check"></i><b>17.8</b> Further reading</a></li>
<li class="chapter" data-level="17.9" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>17.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Computational cognitive modeling with Stan</b></span></li>
<li class="chapter" data-level="18" data-path="ch-cogmod.html"><a href="ch-cogmod.html"><i class="fa fa-check"></i><b>18</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="18.1" data-path="further-reading-14.html"><a href="further-reading-14.html"><i class="fa fa-check"></i><b>18.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>19</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="19.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>19.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="19.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>19.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="19.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>19.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>19.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="19.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>19.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="further-reading-15.html"><a href="further-reading-15.html"><i class="fa fa-check"></i><b>19.3</b> Further reading</a></li>
<li class="chapter" data-level="19.4" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>19.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>20</b> Mixture models</a><ul>
<li class="chapter" data-level="20.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><i class="fa fa-check"></i><b>20.1</b> A mixture model of the speed-accuracy trade-off: The fast-guess model account</a><ul>
<li class="chapter" data-level="20.1.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#the-global-motion-detection-task"><i class="fa fa-check"></i><b>20.1.1</b> The global motion detection task</a></li>
<li class="chapter" data-level="20.1.2" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#a-very-simple-implementation-of-the-fast-guess-model"><i class="fa fa-check"></i><b>20.1.2</b> A very simple implementation of the fast-guess model</a></li>
<li class="chapter" data-level="20.1.3" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:multmix"><i class="fa fa-check"></i><b>20.1.3</b> A multivariate implementation of the fast-guess model</a></li>
<li class="chapter" data-level="20.1.4" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#an-implementation-of-the-fast-guess-model-that-takes-instructions-into-account"><i class="fa fa-check"></i><b>20.1.4</b> An implementation of the fast-guess model that takes instructions into account</a></li>
<li class="chapter" data-level="20.1.5" data-path="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off-the-fast-guess-model-account.html#sec:fastguessh"><i class="fa fa-check"></i><b>20.1.5</b> A hierarchical implementation of the fast-guess model</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="summary-14.html"><a href="summary-14.html"><i class="fa fa-check"></i><b>20.2</b> Summary</a></li>
<li class="chapter" data-level="20.3" data-path="further-reading-16.html"><a href="further-reading-16.html"><i class="fa fa-check"></i><b>20.3</b> Further reading</a></li>
<li class="chapter" data-level="20.4" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>20.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="ch-lognormalrace.html"><a href="ch-lognormalrace.html"><i class="fa fa-check"></i><b>21</b> A simple accumulator model to account for choice response time</a></li>
<li class="part"><span><b>VII Appendix</b></span></li>
<li class="chapter" data-level="22" data-path="ch-distr.html"><a href="ch-distr.html"><i class="fa fa-check"></i><b>22</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:N400hierarchical" class="section level2">
<h2><span class="header-section-number">5.1</span> A hierarchical normal model: The N400 effect</h2>
<p>Event-related potentials (ERPs) allow scientists to observe electrophysiological responses in the brain measured by means of electroencephalography (EEG) that are time-locked to a specific event (i.e., the presentation of the stimuli). A very robust ERP effect in the study of language is the N400. It has been shown that words with low predictability are accompanied by an <em>N400 effect</em> in comparison with high-predictable words, this is a relative negativity that peaks around 300-500 after word onset over central parietal scalp sites <span class="citation">(first reported in Kutas and Hillyard <a href="#ref-kutasReadingSenselessSentences1980">1980</a>, for semantic anomalies and in <a href="#ref-kutasBrainPotentialsReading1984">1984</a> for low predictable word; for a review: Kutas and Federmeier <a href="#ref-kutasThirtyYearsCounting2011">2011</a>)</span>; see Figure <a href="sec-N400hierarchical.html#fig:N400noun">5.4</a>.</p>

<div class="figure"><span style="display:block;" id="fig:N400noun"></span>
<img src="bookdown_files/figure-html/N400noun-1.svg" alt="Typical ERP for the grand average across the N400 spatial window (central parietal electrodes: Cz, CP1, CP2, P3, Pz, P4, POz) for high and low predictability nouns (specifically from the constraining context of the experiment reported in Nicenboim, Vasishth, and RÃ¶sler 2020a). The x-axis indicates time in seconds and the y-axis indicates voltage in microvolts (unlike many EEG/ERP plots, the negative polarity is plotted downwards)." width="672" />
<p class="caption">
FIGURE 5.4: Typical ERP for the grand average across the N400 spatial window (central parietal electrodes: Cz, CP1, CP2, P3, Pz, P4, POz) for high and low predictability nouns <span class="citation">(specifically from the constraining context of the experiment reported in Nicenboim, Vasishth, and RÃ¶sler <a href="#ref-nicenboim_vasishth_rosler_2020">2020</a><a href="#ref-nicenboim_vasishth_rosler_2020">a</a>)</span>. The x-axis indicates time in seconds and the y-axis indicates voltage in microvolts (unlike many EEG/ERP plots, the negative polarity is plotted downwards).
</p>
</div>
<p>In 1, for example, the continuation <em>âpaintâ</em> has higher predictability than the continuation <em>âdogâ</em>, and thus we would expect a more negative signal, that is, an N400 effect, in <em>âdogâ</em> in (b) in comparison with <em>âpaintâ</em> in (a). It is often the case that predictability is measured with a cloze task (see section <a href="sec-binomialcloze.html#sec:binomialcloze">1.4</a>).</p>
<ol style="list-style-type: decimal">
<li>Example from <span class="citation">Kutas and Hillyard (<a href="#ref-kutasBrainPotentialsReading1984">1984</a>)</span>
<ol style="list-style-type: lower-alpha">
<li>Donât touch the wet paint.</li>
<li>Donât touch the wet dog.</li>
</ol></li>
</ol>
<p>The EEG data are typically recorded in tens of electrodes every couple of milliseconds, but for our purposes (i.e., for learning about Bayesian hierarchical models), we can safely ignore the complexity of the data. A common way to simplify the high-dimensional EEG data when we are dealing with the N400 is to focus on the average amplitude of the EEG signal at the typical spatio-temporal window of the N400 <span class="citation">(see for example Frank et al. <a href="#ref-frankERPResponseAmount2015">2015</a>)</span>.</p>
<p>For this example, we are going to focus on the N400 effect for critical nouns from a subset of the data of <span class="citation">Nieuwland et al. (<a href="#ref-nieuwlandLargescaleReplicationStudy2018">2018</a>)</span>. <span class="citation">Nieuwland et al. (<a href="#ref-nieuwlandLargescaleReplicationStudy2018">2018</a>)</span> presented a replication attempt of an original experiment of <span class="citation">DeLong, Urbach, and Kutas (<a href="#ref-delongProbabilisticWordPreactivation2005">2005</a>)</span> with sentences like (2)</p>
<ol start="2" style="list-style-type: decimal">
<li>Example from <span class="citation">DeLong, Urbach, and Kutas (<a href="#ref-delongProbabilisticWordPreactivation2005">2005</a>)</span>
<ol style="list-style-type: lower-alpha">
<li>The day was breezy so the boy went outside to fly a kite.</li>
<li>The day was breezy so the boy went outside to fly an airplane.</li>
</ol></li>
</ol>
<p>Weâll ignore the goal of original experiment <span class="citation">(DeLong, Urbach, and Kutas <a href="#ref-delongProbabilisticWordPreactivation2005">2005</a>)</span>, and its replication <span class="citation">(Nieuwland et al. <a href="#ref-nieuwlandLargescaleReplicationStudy2018">2018</a>)</span>. We are going to focus on the N400 at the final nouns in the experimental stimuli. In example (2), for example, the final noun <em>âkiteâ</em> has higher predictability than <em>âairplaneâ</em>, and thus we would expect a more negative signal in <em>âairplaneâ</em> in (b) in comparison with <em>âkiteâ</em> in (a).</p>
<p>To speed-up computation, we restrict the dataset to the participants from the Edinburgh lab using <code>df_eeg</code> from <code>bcogsci</code> package. Weâll center the cloze probability before using it as a predictor.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_eeg&quot;</span>)</a>
<a class="sourceLine" id="cb212-2" data-line-number="2">(df_eeg &lt;-<span class="st"> </span>df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb212-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cloze =</span> cloze <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cloze)))</a></code></pre></div>
<pre><code>## # A tibble: 2,863 x 7
##    subj cloze  item  n400 cloze_ans     N c_cloze
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1     1  0        1  7.08         0    44  -0.476
## 2     1  0.03     2 -0.68         1    44  -0.446
## 3     1  1        3  1.39        44    44   0.524
## # â¦ with 2,860 more rows</code></pre>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" data-line-number="1"><span class="co"># Number of subjects</span></a>
<a class="sourceLine" id="cb214-2" data-line-number="2">df_eeg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb214-3" data-line-number="3"><span class="st">  </span><span class="kw">distinct</span>(subj) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb214-4" data-line-number="4"><span class="st">  </span><span class="kw">count</span>()</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1    37</code></pre>
<p>One nice aspect of using averages of EEG data is that they are roughly normally distributed. This allows us to use the Normal likelihood. Figure <a href="sec-N400hierarchical.html#fig:histn400">5.5</a> shows the distribution of the data.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" data-line-number="1">df_eeg <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(n400)) <span class="op">+</span></a>
<a class="sourceLine" id="cb216-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_histogram</span>(</a>
<a class="sourceLine" id="cb216-3" data-line-number="3">    <span class="dt">binwidth =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb216-4" data-line-number="4">    <span class="dt">colour =</span> <span class="st">&quot;gray&quot;</span>,</a>
<a class="sourceLine" id="cb216-5" data-line-number="5">    <span class="dt">alpha =</span> <span class="fl">.5</span>,</a>
<a class="sourceLine" id="cb216-6" data-line-number="6">    <span class="kw">aes</span>(<span class="dt">y =</span> ..density..)</a>
<a class="sourceLine" id="cb216-7" data-line-number="7">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb216-8" data-line-number="8"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb216-9" data-line-number="9">    <span class="dt">mean =</span> <span class="kw">mean</span>(df_eeg<span class="op">$</span>n400),</a>
<a class="sourceLine" id="cb216-10" data-line-number="10">    <span class="dt">sd =</span> <span class="kw">sd</span>(df_eeg<span class="op">$</span>n400)</a>
<a class="sourceLine" id="cb216-11" data-line-number="11">  )) <span class="op">+</span></a>
<a class="sourceLine" id="cb216-12" data-line-number="12"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Average voltage in microvolts for the N400 spatiotemporal window&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:histn400"></span>
<img src="bookdown_files/figure-html/histn400-1.svg" alt="Histogram of the N400 averages for every trial in gray; density plot of a normal distribution in black." width="672" />
<p class="caption">
FIGURE 5.5: Histogram of the N400 averages for every trial in gray; density plot of a normal distribution in black.
</p>
</div>
<div id="complete-pooling-model-m_cp" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</h3>
<p>Weâll start from the simplest model which is basically the linear regression we encountered in the preceding chapter.</p>
<div id="model-assumptions" class="section level4">
<h4><span class="header-section-number">5.1.1.1</span> Model assumptions</h4>
<p>This model, call it <span class="math inline">\(M_{cp}\)</span>, makes the following assumptions.</p>
<ol style="list-style-type: decimal">
<li>The EEG averages for the N400 spatiotemporal window are normally distributed.</li>
<li>Observations are <em>independent</em>.</li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<p><strong>This model is incorrect for these data due to assumption (2) being violated.</strong></p>
<p>With the last assumption, we are saying that the difference in the average signal when we compare nouns with cloze probability of 0 and 0.1 is the same as the difference in the signal when we compare nouns with cloze values of 0.1 and 0.2 (or 0.9 and 1). This is just an assumption, and it may not necessarily be the case in the actual data. This means that we are going to get a posterior for <span class="math inline">\(\beta\)</span> <em>conditional</em> on the assumption that the linear relationship holds. Even if it <em>approximately</em> holds, we still donât know how much we deviate from this assumption.</p>
<!-- BN: we never really come back to that, I think -->
<!-- We'll come back to this issue in chapters \@ref(ch:bf)-\@ref(ch:cv) when we deal with model comparison. -->
<p>We can now decide on a likelihood and priors:</p>
</div>
<div id="likelihood-and-priors-1" class="section level4">
<h4><span class="header-section-number">5.1.1.2</span> Likelihood and priors</h4>
<p>A normal likelihood seems reasonable for these data:</p>
<p><span class="math display" id="eq:Mcp">\[\begin{equation}
   signal_n \sim Normal( \alpha + c\_cloze_n \cdot \beta,\sigma)
  \tag{5.1}
 \end{equation}\]</span></p>
<p>where <span class="math inline">\(n =1, \ldots, N\)</span>, and <span class="math inline">\(signal\)</span> is the dependent variable (average signal in the N400 spatiotemporal window in microvolts). The variable <span class="math inline">\(N\)</span> represents the total number of data points.</p>
<p>As always we need to rely on our previous knowledge and domain expertise to decide on priors. We know that ERPs (signals time-locked to a stimulus) have mean amplitudes of a couple of microvolts: This is easy to see in any plot of the EEG literature. This means that we donât expect the effect of our manipulation to exceed, say, 10 <span class="math inline">\(\mu V\)</span>. As before weâll assume that effects can be negative or positive. We can quantify our prior knowledge regarding plausible values of <span class="math inline">\(\beta\)</span> as normally distributed centered at zero with a standard deviation of 10 <span class="math inline">\(\mu V\)</span>. (5 <span class="math inline">\(\mu V\)</span> would have been more informative and also reasonable, since it would entail that 95% of the prior mass probability is between -10 and 10 <span class="math inline">\(\mu V\)</span>.)</p>
<p>If the signal for each ERP is <em>baselined</em>, that is, the mean signal of a time window before the time window of interest is subtracted from the time window of interest, then the mean signal would be relatively close to 0. Since we know that the ERPs were baselined in this study, we expect that the grand mean of our signal should be relatively close to zero. Our prior for <span class="math inline">\(\alpha\)</span> is then normally distributed centered in zero with a standard deviation of 10 <span class="math inline">\(\mu V\)</span>.</p>
<p>The standard deviation of our signal distribution is harder to guess. We know that EEG signals are quite noisy, and that the standard deviation must be higher than zero. Our prior for <span class="math inline">\(\sigma\)</span> is a truncated normal distribution with location zero and scale as 50. Recall that since we truncate the distribution, the parameters location and scale do not correspond to the mean and standard deviation of the new distribution; see Box <a href="sec-pupil.html#thm:truncation">4.1</a>.</p>
<p>We can draw random samples from this distribution and calculate their mean and standard deviation:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" data-line-number="1">samples &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(<span class="dv">20000</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">50</span>, <span class="dt">a =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb217-2" data-line-number="2"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(samples), <span class="kw">sd</span>(samples))</a></code></pre></div>
<pre><code>## mean      
##   40   30</code></pre>
<p>So we are essentially saying that we assume a priori that we will find the true standard deviation of the signal in the following interval with 95% probability:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" data-line-number="1"><span class="kw">quantile</span>(samples, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">.975</span>))</a></code></pre></div>
<pre><code>##   2.5%  97.5% 
##   1.53 111.52</code></pre>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" data-line-number="1"><span class="co"># or c(qtnorm(.025, 0, 50, a = 0), qtnorm(.975, 0, 50, a = 0))</span></a></code></pre></div>
<p>To sum up, we are going to use the following priors:</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim Normal(0,10)\\
 \beta  &amp;\sim Normal(0,10)\\
 \sigma  &amp;\sim Normal_{+}(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>A model such as <span class="math inline">\(M_{cp}\)</span> is sometimes called a <em>fixed-effects</em> model: all the parameters are fixed and do not vary from subject to subject or from item to item. A similar frequentist model would correspond to fitting a simple linear model using the <code>lm</code> function: <code>lm(n400 ~ 1 + cloze, data = df_eeg)</code>.</p>
<p>We fit this model in <code>brms</code> as follows (the default family is <code>gaussian()</code> so we can omit it). As with <code>lm</code>, by default an intercept is fitted and thus <code>n400 ~ c_cloze</code> is equivalent to <code>n400 ~ 1 + c_cloze</code>:</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" data-line-number="1">fit_N400_cp &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze,</a>
<a class="sourceLine" id="cb222-2" data-line-number="2">  <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb222-3" data-line-number="3">    <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb222-4" data-line-number="4">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb222-5" data-line-number="5">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb222-6" data-line-number="6">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma)</a>
<a class="sourceLine" id="cb222-7" data-line-number="7">    ),</a>
<a class="sourceLine" id="cb222-8" data-line-number="8">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb222-9" data-line-number="9">)</a></code></pre></div>
<p>For now, weâll check the summary and plot the posterior of the model.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" data-line-number="1">fit_N400_cp</a></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     3.66      0.22     3.23     4.10 1.00     4208     3215
## c_cloze       2.24      0.55     1.17     3.32 1.00     4879     3022
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    11.82      0.15    11.53    12.13 1.00     4271     2550
## 
## ...</code></pre>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" data-line-number="1"><span class="kw">plot</span>(fit_N400_cp)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-119-1.svg" width="672" /></p>
</div>
</div>
<div id="no-pooling-model-m_np" class="section level3">
<h3><span class="header-section-number">5.1.2</span> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</h3>
<p>One of the assumptions of the previous model is clearly wrong: observations are not independent, they are clustered by participant (and also by the specific item, but weâll ignore this until section <a href="sec-N400hierarchical.html#sec:mcvivs">5.1.4</a>). It is reasonable to assume that EEG signals are more similar within participants than between them. The following model assumes that each participant is completely independent from each other.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
<div id="model-assumptions-1" class="section level4">
<h4><span class="header-section-number">5.1.2.1</span> Model assumptions</h4>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.</li>
<li>Every participantâs model is fit independently of the other participants; the participants have no parameters in common (an exception is the standard deviation; this is the same for all participants).</li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<p>What likelihood and priors can we choose here?</p>
</div>
<div id="likelihood-and-priors-2" class="section level4">
<h4><span class="header-section-number">5.1.2.2</span> Likelihood and priors</h4>
<p>The likelihood is a normal distribution as before:</p>
<p><span class="math display">\[\begin{equation}
 signal_n \sim Normal( \alpha_{subj[n]} + c\_cloze_n \cdot \beta_{subj[n]},\sigma)
 \end{equation}\]</span></p>
<p>This model is actually fitting one linear model for each participant, with a single standard deviation <span class="math inline">\(\sigma\)</span> across all participants.</p>
<p>As before, <span class="math inline">\(n\)</span> represents each observation, that is, the <span class="math inline">\(n\)</span>th row in the data frame, which has <span class="math inline">\(N\)</span> rows, and now <span class="math inline">\(i\)</span> identifies the participant. The notation <span class="math inline">\(subj[n]\)</span>, which roughly follows <span class="citation">Gelman and Hill (<a href="#ref-GelmanHill2007">2007</a>)</span>, identifies the participant index; for example, if <span class="math inline">\(subj[10]=3\)</span>, then the <span class="math inline">\(10\)</span>th row of the data-frame is from participant <span class="math inline">\(3\)</span>.</p>
<p>We define the priors as follows:</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha_{i} &amp;\sim Normal(0,10)\\
 \beta_{i}  &amp;\sim Normal(0,10)\\
 \sigma  &amp;\sim Normal_+(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>In <code>brms</code>, such a model can be fit by removing the common intercept with <code>0 +</code>. Instead, we force the model to estimate one intercept and one slope for <em>each</em> level of <code>subj</code>. The by-subject intercepts are indicated with <code>factor(subj)</code> and the by-subject slopes with <code>c_cloze:factor(subj)</code>. Itâs very important to specify that <code>subject</code> should be treated as a factor and not as a number; we donât assume that subject number 3 will show 3 times more positive (or negative) average signal than subject number 1! The model fits 37 independent intercepts and 37 independent slopes. By setting a prior to <code>class = b</code> and omitting <code>coef</code>, we are essentially setting identical priors to all the intercepts and slopes of the model. The parameters are independent from each other, itâs only our previous knowledge about their possible values (encoded in the priors) that is identical. We can set different priors to each intercept and slope, but that will mean to set 74 priors!</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" data-line-number="1">fit_N400_np &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(subj) <span class="op">+</span><span class="st"> </span>c_cloze<span class="op">:</span><span class="kw">factor</span>(subj),</a>
<a class="sourceLine" id="cb226-2" data-line-number="2">  <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb226-3" data-line-number="3">    <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb226-4" data-line-number="4">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb226-5" data-line-number="5">      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma)</a>
<a class="sourceLine" id="cb226-6" data-line-number="6">    ),</a>
<a class="sourceLine" id="cb226-7" data-line-number="7">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb226-8" data-line-number="8">)</a></code></pre></div>
<p>For this model, printing a summary means printing the 75 parameters (<span class="math inline">\(\alpha_{1,...,37}\)</span>, <span class="math inline">\(\beta_{1,...,37}\)</span>, and <span class="math inline">\(\sigma\)</span>). We could do this as always by printing out the model results: just type <code>fit_N400_np</code>. Instead, one can plot <span class="math inline">\(\beta_{1,..,37}\)</span> using <code>bayesplot</code>. (<code>brms</code> also includes a wrapper to this function called <code>stanplot</code>). We can peek at the internal names that <code>brms</code> gives to the parameters with <code>parnames(fit_N400_np)</code>; they are <code>b_factorsubj</code>, then the subject index and then <code>:c_cloze</code>. The code below changes the subject labels back to their original numerical indices and plots them in Figure <a href="sec-N400hierarchical.html#fig:nopooling">5.6</a>. The subjects are ordered by the magnitude of their mean effects.</p>
<p>The model <span class="math inline">\(M_{np}\)</span> does not estimate a unique population-level effect; instead, there is a different effect estimated for each subject. However, given the posterior means from each subject, it is still possible to calculate the average of these estimates <span class="math inline">\(\hat\beta_{1,...,n}\)</span>:</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" data-line-number="1"><span class="co"># parameter name of beta by subject:</span></a>
<a class="sourceLine" id="cb227-2" data-line-number="2">ind_effects_np &lt;-<span class="st"> </span><span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb227-3" data-line-number="3">  <span class="st">&quot;b_factorsubj&quot;</span>,</a>
<a class="sourceLine" id="cb227-4" data-line-number="4">  <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj), <span class="st">&quot;:c_cloze&quot;</span></a>
<a class="sourceLine" id="cb227-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb227-6" data-line-number="6">beta_across_subj &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit_N400_np,</a>
<a class="sourceLine" id="cb227-7" data-line-number="7">  <span class="dt">pars =</span> ind_effects_np</a>
<a class="sourceLine" id="cb227-8" data-line-number="8">) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb227-9" data-line-number="9"><span class="st">  </span><span class="kw">rowMeans</span>()</a>
<a class="sourceLine" id="cb227-10" data-line-number="10"><span class="co"># We calculate the average of these estimates</span></a>
<a class="sourceLine" id="cb227-11" data-line-number="11">(grand_av_beta &lt;-<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb227-12" data-line-number="12">  <span class="dt">mean =</span> <span class="kw">mean</span>(beta_across_subj),</a>
<a class="sourceLine" id="cb227-13" data-line-number="13">  <span class="dt">lq =</span> <span class="kw">quantile</span>(beta_across_subj, <span class="kw">c</span>(.<span class="dv">025</span>)),</a>
<a class="sourceLine" id="cb227-14" data-line-number="14">  <span class="dt">hq =</span> <span class="kw">quantile</span>(beta_across_subj, <span class="kw">c</span>(.<span class="dv">975</span>))</a>
<a class="sourceLine" id="cb227-15" data-line-number="15">))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##    mean    lq    hq
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  2.18  1.19  3.21</code></pre>
<p>The 95% credible interval of this overall mean effect is plotted in Figure <a href="sec-N400hierarchical.html#fig:nopooling">5.6</a> as two vertical lines together with the effect of cloze probability for each subject (ordered by effect size). Rather than using a plotting function from <code>brms</code>, we extract the summary, we arrange it and we plot it with <code>ggplot2</code>.</p>

<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" data-line-number="1"><span class="co"># We make a table of beta by subject</span></a>
<a class="sourceLine" id="cb229-2" data-line-number="2">beta_by_subj &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_N400_np,</a>
<a class="sourceLine" id="cb229-3" data-line-number="3">  <span class="dt">pars =</span> ind_effects_np</a>
<a class="sourceLine" id="cb229-4" data-line-number="4">) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-5" data-line-number="5"><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subject =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-7" data-line-number="7"><span class="st">  </span><span class="co">## reorder plot by magnitude of mean:</span></a>
<a class="sourceLine" id="cb229-8" data-line-number="8"><span class="st">  </span><span class="kw">arrange</span>(Estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subject =</span> <span class="kw">factor</span>(subject, <span class="dt">levels =</span> subject))</a>
<a class="sourceLine" id="cb229-10" data-line-number="10"><span class="co"># We plot:</span></a>
<a class="sourceLine" id="cb229-11" data-line-number="11"><span class="kw">ggplot</span>(</a>
<a class="sourceLine" id="cb229-12" data-line-number="12">  beta_by_subj,</a>
<a class="sourceLine" id="cb229-13" data-line-number="13">  <span class="kw">aes</span>(<span class="dt">x =</span> Estimate, <span class="dt">xmin =</span> Q2<span class="fl">.5</span>, <span class="dt">xmax =</span> Q97<span class="fl">.5</span>, <span class="dt">y =</span> subject)</a>
<a class="sourceLine" id="cb229-14" data-line-number="14">) <span class="op">+</span></a>
<a class="sourceLine" id="cb229-15" data-line-number="15"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb229-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_errorbarh</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb229-17" data-line-number="17"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> grand_av_beta<span class="op">$</span>mean) <span class="op">+</span></a>
<a class="sourceLine" id="cb229-18" data-line-number="18"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> grand_av_beta<span class="op">$</span>lq, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb229-19" data-line-number="19"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> grand_av_beta<span class="op">$</span>hq, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb229-20" data-line-number="20"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;By-subject effect of cloze probability in microvolts&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:nopooling"></span>
<img src="bookdown_files/figure-html/nopooling-1.svg" alt="95% credible intervals of the effect of cloze probability for each subject according to the no pooling model." width="672" />
<p class="caption">
FIGURE 5.6: 95% credible intervals of the effect of cloze probability for each subject according to the no pooling model.
</p>
</div>
</div>
</div>
<div id="sec:uncorrelated" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</h3>
<p>One major problem with the no-pooling model is that we ignore completely that the subjects were after all doing the same experiment. We fit each subjectâs data ignoring the information available in the other subjectsâ data.
The no-pooling model is very likely to <em>overfit</em> the individual subjectsâ data; we are likely to ignore the generalities of the data and we may end up overinterpreting the noise. The model can be modified to explicitly assume that the subjects have an overall effect common to all the subjects, with the individual subjects deviating from this common effect.</p>
<p>Assuming that there is an overall effect that is common to the subjects, and, importantly, assuming that all subjectsâ parameters originate from one common (normal) distribution will result in the estimation of posteriors for each participant being also influenced by what we know about all the subjects together. Weâll first fit a hierarchical model with uncorrelated varying intercept and slope.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<div id="model-assumptions-2" class="section level4">
<h4><span class="header-section-number">5.1.3.1</span> Model assumptions</h4>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.
<!-- BN: I don't understand this, commenting it for now:
2. All subjects share a grand mean signal voltage and of the mean effect of predictability.
--></li>
<li>Each subject deviates to some extent (this is made precise below) from the grand mean and from the mean effect of predictability. This implies that there is some between-subject variability in the individual-level intercept and slope adjustments by subject.</li>
<li>There is a linear relationship between cloze and the EEG signal.</li>
</ol>
</div>
<div id="likelihood-and-priors-3" class="section level4">
<h4><span class="header-section-number">5.1.3.2</span> Likelihood and priors</h4>
<p>The likelihood now incorporates the assumption that both the intercept and slope are adjusted by participant.</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim Normal(\alpha + u_{subj[n],1} + c\_cloze_n \cdot (\beta+ u_{subj[n],2}),\sigma)
 \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp;\sim Normal(0,10)\\
 \beta  &amp;\sim Normal(0,10)\\
 u_1 &amp;\sim Normal(0,\tau_{u_1})\\
 u_2 &amp;\sim Normal(0,\tau_{u_2})\\
 \tau_{u_1} &amp;\sim Normal_+(0,20) \\
 \tau_{u_2} &amp;\sim Normal_+(0,20) \\
 \sigma  &amp;\sim Normal_+(0,50)
 \end{aligned}
 \end{equation}\]</span></p>
<p>In this model each subject has their own intercept adjustment, <span class="math inline">\(u_{1,subj}\)</span>, and slope adjustment, <span class="math inline">\(u_{2,subj}\)</span>.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> If <span class="math inline">\(u_{1,subj}\)</span> is positive, the subject will have a more positive EEG signal than the grand mean average. If <span class="math inline">\(u_{2,subj}\)</span> is positive, the subject will have a more positive EEG response to a change of one unit in <code>c_cloze</code> than the overall mean effect (i.e., there will be a more positive effect of cloze probability on the N400). The parameters <span class="math inline">\(u\)</span> are sometimes called random effects and thus a model with fixed effects (<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) and random effects is called a mixed model. However, random effects have different meanings in different contexts. To avoid ambiguity, <code>brms</code> calls these parameters <em>group-level</em> effects. Since we are estimating <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(u\)</span> at the same time and we assume that the average of the <span class="math inline">\(u\)</span>âs is 0 (since it is assumed to be normally distributed with mean 0), what is common between the subjects, the grand mean, is estimated as the intercept <span class="math inline">\(\alpha\)</span>, and the deviations of individual subjectsâ means from this grand mean are the adjustments <span class="math inline">\(u_1\)</span>. Similarly, the mean effect of cloze is estimated as <span class="math inline">\(\beta\)</span>, and the deviations of individual subjectsâ mean effects of cloze from <span class="math inline">\(\beta\)</span> are the adjustment <span class="math inline">\(u_2\)</span>. The standard deviations of these two adjustment terms, <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>, respectively, represent between participant variability; see also Box <a href="sec-N400hierarchical.html#thm:hierarchical">5.2</a>.</p>
<p>Thus, the model <span class="math inline">\(M_{v}\)</span> has three <em>standard deviations</em>: <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>. In statistics, it is conventional to talk about variances (the square of these standard deviations); for this reason, these standard deviations are also (confusingly) called <em>variance components</em>. The variance components <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span> characterize between-subject variability, and the variance component <span class="math inline">\(\sigma\)</span> characterizes within-subject variability.</p>
<p>The by-subject adjustments <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are parameters in the model, and therefore have priors defined on them. Parameters that appear in the prior specifications for parameters, such as <span class="math inline">\(\tau_u\)</span>, are often called <em>hyperparameters</em>,<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> and the priors on such hyperparameters are called <em>hyperpriors</em>. Thus, the parameter <span class="math inline">\(u_1\)</span> has <span class="math inline">\(Normal(0,\tau_{u_1})\)</span> as a prior; <span class="math inline">\(\tau_{u_1}\)</span> is a hyperparameter, and the hyperprior on <span class="math inline">\(\tau_{u_1}\)</span> is <span class="math inline">\(Normal(0,20)\)</span>.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></p>
<p>We know that in general, in EEG experiments, the standard deviations for the by-subject adjustments are smaller than the standard deviation of the observations (which is the within-subjects standard deviation). That is, usually the between-subject variability in the intercepts and slopes is smaller than the within-subjects variability in the data. For this reason, reducing the scale of the truncated normal distribution to <span class="math inline">\(20\)</span> (in comparison to <span class="math inline">\(50\)</span>) seems reasonable for the priors of the <span class="math inline">\(\tau\)</span> parameters. As always, we can do a sensitivity analysis to verify that our priors are reasonably uninformative (if we intended them to be uninformative).</p>

<div class="extra">

<div class="theorem">
<span id="thm:hierarchical" class="theorem"><strong>Box 5.2  </strong></span><strong>Some important (and sometimes confusing) points:</strong>
</div>
<ul>
<li><p>Why does <span class="math inline">\(u\)</span> have a mean of 0?</p>
<p>Because we want <span class="math inline">\(u\)</span> to capture only differences between subjects, we could achieve the same by assuming the following relationship between the likelihood and the intercept and slope:</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
 signal_n &amp;\sim Normal(\alpha_{subj[n]} + \beta_{subj[n]} \cdot c\_cloze_n, \sigma)  \\
 \alpha_i &amp;\sim Normal(\alpha,\tau_{u_1})\\
 \beta_i &amp;\sim Normal(\beta,\tau_{u_2})\\
 \end{aligned}
 \end{equation}\]</span></p>
<p>And in fact, thatâs another common way to write the model.</p></li>
<li><p>Why do the adjustments <span class="math inline">\(u\)</span> have a normal distribution?</p></li>
</ul>
<p>Mostly because of convention, thatâs the way itâs implemented in most frequentist mixed models. But also because if we donât know anything about the distribution besides its mean and variance, the normal distribution is the most conservative assumption <span class="citation">(see also chapter 9 of McElreath <a href="#ref-mcelreath2015statistical">2015</a>)</span>.</p>
</div>

<p>For now, we are assuming that there is no relationship (no correlation) between the by-subject intercept and slope adjustments <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span>; as in <code>lmer</code>, this lack of correlation is indicated using in <code>brms</code> using the double pipe <code>||</code>. In <code>brms</code>, we need to specify hyperpriors for <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>; these are called <code>sd</code> in <code>brms</code>, to distinguish these standard deviations from <span class="math inline">\(\sigma\)</span>. As with the population-level effects, the by-subjects intercept adjustments are implicitly fit for the group-level effects and thus <code>(c_cloze || subj)</code> is equivalent to <code>(1 + c_cloze || subj)</code>. If we donât want an intercept we need to explicitly indicate it with <code>(0 + c_cloze || subj)</code> or <code>(-1 + c_cloze || subj)</code>. Such a removal of the intercept is not normally done.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" data-line-number="1">prior_v &lt;-</a>
<a class="sourceLine" id="cb230-2" data-line-number="2"><span class="st">  </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb230-3" data-line-number="3">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb230-4" data-line-number="4">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb230-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb230-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept, <span class="dt">group =</span> subj),</a>
<a class="sourceLine" id="cb230-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze, <span class="dt">group =</span> subj)</a>
<a class="sourceLine" id="cb230-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb230-9" data-line-number="9">fit_N400_v &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">||</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb230-10" data-line-number="10">  <span class="dt">prior =</span> prior_v,</a>
<a class="sourceLine" id="cb230-11" data-line-number="11">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb230-12" data-line-number="12">)</a></code></pre></div>
<p>When we print a <code>brms</code> fit, we first see the summaries of the posteriors of the standard deviation of the by-group intercept and slopes, <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span> as <code>sd(Intercept)</code> and <code>sd(c_cloze)</code>, and then, as with previous models, the population-level effects, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> as <code>Intercept</code> and <code>c_cloze</code>, and the scale of the likelihood, <span class="math inline">\(\sigma\)</span>, as <code>sigma</code>.</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb231-1" data-line-number="1">fit_N400_v</a></code></pre></div>
<p>Because the above command will result in pages of output, it is easier to understand the summary graphically:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1"><span class="kw">plot</span>(fit_N400_v, <span class="dt">N =</span> <span class="dv">6</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-124-1.svg" width="672" /></p>
<p>Because we estimated how the population-level effect of cloze is adjusted for each subject, we could examine how each subject is being affected by the manipulation. For this we do the following, and we plot it in Figure <a href="sec-N400hierarchical.html#fig:partialpooling">5.7</a>. These are adjustments, <span class="math inline">\(u_{1,1},u_{1,...},u_{1,37}\)</span>, and not the effect of the manipulation by subject, <span class="math inline">\(\beta + [u_{1,1},u_{1,...},u_{1,37}]\)</span>.</p>

<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb233-1" data-line-number="1"><span class="co"># We make a table of u_2s</span></a>
<a class="sourceLine" id="cb233-2" data-line-number="2">ind_effects_v &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;r_subj[&quot;</span>, <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj), <span class="st">&quot;,c_cloze]&quot;</span>)</a>
<a class="sourceLine" id="cb233-3" data-line-number="3">u_<span class="dv">2</span>_v &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_N400_v)[ind_effects_v, ] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb233-4" data-line-number="4"><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb233-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subj =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb233-6" data-line-number="6"><span class="st">  </span><span class="co">## reorder plot by magnitude of mean:</span></a>
<a class="sourceLine" id="cb233-7" data-line-number="7"><span class="st">  </span><span class="kw">arrange</span>(Estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb233-8" data-line-number="8"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subj =</span> <span class="kw">factor</span>(subj, <span class="dt">levels =</span> subj))</a>
<a class="sourceLine" id="cb233-9" data-line-number="9"><span class="co"># We plot:</span></a>
<a class="sourceLine" id="cb233-10" data-line-number="10"><span class="kw">ggplot</span>(</a>
<a class="sourceLine" id="cb233-11" data-line-number="11">  u_<span class="dv">2</span>_v,</a>
<a class="sourceLine" id="cb233-12" data-line-number="12">  <span class="kw">aes</span>(<span class="dt">x =</span> Estimate, <span class="dt">xmin =</span> Q2<span class="fl">.5</span>, <span class="dt">xmax =</span> Q97<span class="fl">.5</span>, <span class="dt">y =</span> subj)</a>
<a class="sourceLine" id="cb233-13" data-line-number="13">) <span class="op">+</span></a>
<a class="sourceLine" id="cb233-14" data-line-number="14"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb233-15" data-line-number="15"><span class="st">  </span><span class="kw">geom_errorbarh</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb233-16" data-line-number="16"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;By-subject adjustment to the slope in microvolts&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:partialpooling"></span>
<img src="bookdown_files/figure-html/partialpooling-1.svg" alt="95% credible intervals of adjustments to the effect of cloze probability for each subject (\(u_{1,1..37}\)) according to the varying intercept and varying slopes model." width="672" />
<p class="caption">
FIGURE 5.7: 95% credible intervals of adjustments to the effect of cloze probability for each subject (<span class="math inline">\(u_{1,1..37}\)</span>) according to the varying intercept and varying slopes model.
</p>
</div>
<p>There is an important difference between the no-pooling model and the varying intercepts and slopes model we just fit. The no-pooling model fits each individual subjectâs intercept and slope independently for each subject. By contrast, the varying intercepts and slopes model takes <em>all</em> the subjectsâ data into account in order to compute the fixed effects <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>; and the model shrinks the by-subject intercept and slope adjustments towards the fixed effects estimates. We can see the shrinkage of the estimates in the varying intercepts model when we compare them with the estimates of the no pooling model (<span class="math inline">\(M_{np}\)</span>) in Figure <a href="sec-N400hierarchical.html#fig:comparison">5.8</a>. The code is here more involved since it require us to build a data frame with the by-subject effects.</p>

<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" data-line-number="1"><span class="co"># No pooling model</span></a>
<a class="sourceLine" id="cb234-2" data-line-number="2">par_np &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(fit_N400_np)[ind_effects_np, ] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb234-3" data-line-number="3"><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb234-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb234-5" data-line-number="5">    <span class="dt">model =</span> <span class="st">&quot;No pooling&quot;</span>,</a>
<a class="sourceLine" id="cb234-6" data-line-number="6">    <span class="dt">subj =</span> <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj)</a>
<a class="sourceLine" id="cb234-7" data-line-number="7">  )</a>
<a class="sourceLine" id="cb234-8" data-line-number="8"></a>
<a class="sourceLine" id="cb234-9" data-line-number="9"><span class="co"># For the hierarchical model is more complicated,</span></a>
<a class="sourceLine" id="cb234-10" data-line-number="10"><span class="co"># because we want the effect (beta) + adjustment:</span></a>
<a class="sourceLine" id="cb234-11" data-line-number="11"><span class="co"># we extract the overall group level effect:</span></a>
<a class="sourceLine" id="cb234-12" data-line-number="12">beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">posterior_samples</span>(fit_N400_v)<span class="op">$</span>b_c_cloze)</a>
<a class="sourceLine" id="cb234-13" data-line-number="13"><span class="co"># We extract the individual adjustments</span></a>
<a class="sourceLine" id="cb234-14" data-line-number="14">ind_effects_v &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;r_subj[&quot;</span>, <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj), <span class="st">&quot;,c_cloze]&quot;</span>)</a>
<a class="sourceLine" id="cb234-15" data-line-number="15">adjustment &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">posterior_samples</span>(fit_N400_v)[ind_effects_v])</a>
<a class="sourceLine" id="cb234-16" data-line-number="16"><span class="co"># We get the by subject effects in a data frame where each adjustment</span></a>
<a class="sourceLine" id="cb234-17" data-line-number="17"><span class="co"># is in each column.</span></a>
<a class="sourceLine" id="cb234-18" data-line-number="18">by_subj_effect &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(beta <span class="op">+</span><span class="st"> </span>adjustment)</a>
<a class="sourceLine" id="cb234-19" data-line-number="19"><span class="co"># We summarize them by getting a table with the mean and the</span></a>
<a class="sourceLine" id="cb234-20" data-line-number="20"><span class="co"># quantiles for each column and then binding them.</span></a>
<a class="sourceLine" id="cb234-21" data-line-number="21">par_h &lt;-<span class="st"> </span><span class="kw">lapply</span>(by_subj_effect, <span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb234-22" data-line-number="22">  <span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb234-23" data-line-number="23">    <span class="dt">Estimate =</span> <span class="kw">mean</span>(x),</a>
<a class="sourceLine" id="cb234-24" data-line-number="24">    <span class="dt">Q2.5 =</span> <span class="kw">quantile</span>(x, <span class="fl">.025</span>),</a>
<a class="sourceLine" id="cb234-25" data-line-number="25">    <span class="dt">Q97.5 =</span> <span class="kw">quantile</span>(x, <span class="fl">.975</span>)</a>
<a class="sourceLine" id="cb234-26" data-line-number="26">  )</a>
<a class="sourceLine" id="cb234-27" data-line-number="27">}) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb234-28" data-line-number="28"><span class="st">  </span><span class="kw">bind_rows</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb234-29" data-line-number="29"><span class="st">  </span><span class="co"># We add a column to identify that the model,</span></a>
<a class="sourceLine" id="cb234-30" data-line-number="30"><span class="st">  </span><span class="co"># and one with the subject labels:</span></a>
<a class="sourceLine" id="cb234-31" data-line-number="31"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb234-32" data-line-number="32">    <span class="dt">model =</span> <span class="st">&quot;Hierarchical&quot;</span>,</a>
<a class="sourceLine" id="cb234-33" data-line-number="33">    <span class="dt">subj =</span> <span class="kw">unique</span>(df_eeg<span class="op">$</span>subj)</a>
<a class="sourceLine" id="cb234-34" data-line-number="34">  )</a>
<a class="sourceLine" id="cb234-35" data-line-number="35"></a>
<a class="sourceLine" id="cb234-36" data-line-number="36"><span class="co"># The mean and 95% CI of both models in one dataframe:</span></a>
<a class="sourceLine" id="cb234-37" data-line-number="37">by_subj_df &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(par_h, par_np) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb234-38" data-line-number="38"><span class="st">  </span><span class="kw">arrange</span>(Estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb234-39" data-line-number="39"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subj =</span> <span class="kw">factor</span>(subj, <span class="dt">levels =</span> <span class="kw">unique</span>(.data<span class="op">$</span>subj)))</a>
<a class="sourceLine" id="cb234-40" data-line-number="40"></a>
<a class="sourceLine" id="cb234-41" data-line-number="41"><span class="kw">ggplot</span>(</a>
<a class="sourceLine" id="cb234-42" data-line-number="42">  by_subj_df,</a>
<a class="sourceLine" id="cb234-43" data-line-number="43">  <span class="kw">aes</span>(</a>
<a class="sourceLine" id="cb234-44" data-line-number="44">    <span class="dt">ymin =</span> Q2<span class="fl">.5</span>, <span class="dt">ymax =</span> Q97<span class="fl">.5</span>, <span class="dt">x =</span> subj, <span class="dt">y =</span> Estimate, <span class="dt">color =</span> model,</a>
<a class="sourceLine" id="cb234-45" data-line-number="45">    <span class="dt">shape =</span> model</a>
<a class="sourceLine" id="cb234-46" data-line-number="46">  )</a>
<a class="sourceLine" id="cb234-47" data-line-number="47">) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-48" data-line-number="48"><span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="dt">position =</span> <span class="kw">position_dodge</span>(<span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-49" data-line-number="49"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position =</span> <span class="kw">position_dodge</span>(<span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-50" data-line-number="50"><span class="st">  </span><span class="co"># We&#39;ll also add the mean and 95% CrI of the overall difference</span></a>
<a class="sourceLine" id="cb234-51" data-line-number="51"><span class="st">  </span><span class="co"># to the plot:</span></a>
<a class="sourceLine" id="cb234-52" data-line-number="52"><span class="st">  </span><span class="kw">geom_hline</span>(</a>
<a class="sourceLine" id="cb234-53" data-line-number="53">    <span class="dt">yintercept =</span></a>
<a class="sourceLine" id="cb234-54" data-line-number="54">      <span class="kw">posterior_summary</span>(fit_N400_v)[<span class="st">&quot;b_c_cloze&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</a>
<a class="sourceLine" id="cb234-55" data-line-number="55">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-56" data-line-number="56"><span class="st">  </span><span class="kw">geom_hline</span>(</a>
<a class="sourceLine" id="cb234-57" data-line-number="57">    <span class="dt">yintercept =</span></a>
<a class="sourceLine" id="cb234-58" data-line-number="58">      <span class="kw">posterior_summary</span>(fit_N400_v)[<span class="st">&quot;b_c_cloze&quot;</span>, <span class="st">&quot;Q2.5&quot;</span>],</a>
<a class="sourceLine" id="cb234-59" data-line-number="59">    <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="dt">size =</span> <span class="fl">.5</span></a>
<a class="sourceLine" id="cb234-60" data-line-number="60">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-61" data-line-number="61"><span class="st">  </span><span class="kw">geom_hline</span>(</a>
<a class="sourceLine" id="cb234-62" data-line-number="62">    <span class="dt">yintercept =</span></a>
<a class="sourceLine" id="cb234-63" data-line-number="63">      <span class="kw">posterior_summary</span>(fit_N400_v)[<span class="st">&quot;b_c_cloze&quot;</span>, <span class="st">&quot;Q97.5&quot;</span>],</a>
<a class="sourceLine" id="cb234-64" data-line-number="64">    <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="dt">size =</span> <span class="fl">.5</span></a>
<a class="sourceLine" id="cb234-65" data-line-number="65">  ) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-66" data-line-number="66"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;N400 effect of predictability&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-67" data-line-number="67"><span class="st">  </span><span class="kw">coord_flip</span>()</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:comparison"></span>
<img src="bookdown_files/figure-html/comparison-1.svg" alt="Comparison of the estimates of effect of cloze probability for each subject between the no pooling and the varying intercept and varying slopes, hierarchical, model." width="672" />
<p class="caption">
FIGURE 5.8: Comparison of the estimates of effect of cloze probability for each subject between the no pooling and the varying intercept and varying slopes, hierarchical, model.
</p>
</div>
</div>
</div>
<div id="sec:mcvivs" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</h3>
<p>The model <span class="math inline">\(M_{v}\)</span> allowed for differences in intercept (mean voltage) and slopes (effects of cloze) across subjects, but it has the implicit assumption that these are independent. It is in principle possible that subjects showing more negative voltage may also show stronger effects (or weaker effects). Next, we fit a model that assumes a correlation between the intercepts and slopes. We model the correlation between varying intercepts and slopes, by defining a variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> between the by-subject varying intercepts and slopes, and by assuming that both adjustments (intercept and slope) come from a multivariate (in this case, a bivariate) normal distribution. <!-- See Box \@ref(thm:vcovmatrix) for a short overview of the essential details regarding the variance-covariance matrix. --></p>
<!-- \Begin{extra} -->
<!-- <div class="extra"> -->
<!-- ```{theorem, vcovmatrix} -->
<!-- **The variance-covariance matrix and the corresponding correlation matrix:** -->
<!-- ``` -->
<!-- The variances in a multivariate distribution will be composed of -->
<!--   - variances for each random variable -->
<!--   - covariances between pairs of random variables, which includes some correlation $\rho$ between pairs of random variables -->
<!-- E.g., for a bivariate distribution with random variables $u_1$ and $u_2$, this information is expressed in a variance-covariance matrix. -->
<!-- \begin{equation} -->
<!-- \Sigma_u -->
<!-- = -->
<!-- \begin{pmatrix} -->
<!-- \tau _{u_1}^2  & \rho_u \tau _{u_1}\tau _{u1}\\ -->
<!-- \rho \tau _{u_1}\tau _{u_2}    & \tau _{u_2}^2\\ -->
<!-- \end{pmatrix} -->
<!-- \end{equation} -->
<!-- the covariance $Cov(u_1,u_2)$ between two variables $X$ and $Y$ is -->
<!-- defined as the product of their correlation $\rho_u$ and their standard -->
<!-- deviations $\tau_{u_1}$ and $\tau_{u_2}$, such that, $Cov(u_1,u_2) = \rho_u -->
<!-- \tau_{u_1} \tau_{u_2}$. -->
<!-- The covariance matrix can be decomposed into a matrix of standard deviations and a correlation matrix. For our example, the correlation matrix looks like this: -->
<!-- \begin{equation} -->
<!-- \mathbf{\rho}_u =  -->
<!-- {\begin{pmatrix}  -->
<!-- 1 & \rho_u  \\  -->
<!-- \rho_u  & 1 -->
<!-- \end{pmatrix}} -->
<!-- \end{equation} -->
<!-- This means that we can decompose the covariance matrix into three parts: -->
<!-- \begin{equation} -->
<!-- \begin{aligned} -->
<!-- \boldsymbol{\Sigma_u}  -->
<!-- &= -->
<!-- {\begin{pmatrix}  -->
<!-- \tau_{u_1} & 0 \\  -->
<!-- 0  & \tau_{u_2} -->
<!-- \end{pmatrix}} -->
<!-- {\begin{pmatrix}  -->
<!-- 1 & \rho_u  \\  -->
<!-- \rho_u  & 1 -->
<!-- \end{pmatrix}} -->
<!-- {\begin{pmatrix}  -->
<!-- \tau_{u_1} & 0 \\  -->
<!-- 0  & \tau_{u_2} -->
<!-- \end{pmatrix}} -->
<!-- \end{aligned} -->
<!-- \end{equation} -->
<!-- The importance of the correlation matrix is that a prior will be defined on the correlation matrix rather than on the individual correlation parameter. One reason for this is generality: in more complex designs, such as $2\times 2\times 2$  factorial experiments, the variance covariance matrix is much larger than in our example in the text, but the proliferation of correlations that result is no problem for `brms` or Stan because we define the prior on the correlation matrix. -->
<!-- </div> -->
<!-- \End{extra} -->
<ul>
<li>In <span class="math inline">\(M_h\)</span>, we model the EEG data with the following assumptions:</li>
</ul>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.</li>
<li>Some aspects of the mean signal voltage and of the effect of predictability depend on the participant, and these two might be correlated, i.e., we assume group-level intercepts, and slopes, and a correlation between them by-subject.
<!-- 3. The variation in the signal is independent from the participant. --></li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<p>The likelihood remains identical to the model without a correlation between group-level intercepts and slopes (section <a href="sec-N400hierarchical.html#sec:uncorrelated">5.1.3</a>):</p>
<p><span class="math display">\[\begin{equation}
  signal_n \sim Normal(\alpha + u_{subj[n],1} + c\_cloze_n \cdot  (\beta + u_{subj[n],2}),\sigma)
  \end{equation}\]</span></p>
<p>The correlation is indicated in the priors on the adjustments for intercept <span class="math inline">\(u_{1}\)</span> and slopes <span class="math inline">\(u_{2}\)</span>.</p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp; \sim Normal(0,10) \\
 \beta  &amp; \sim Normal(0,10) \\
  \sigma  &amp;\sim Normal_+(0,50)\\
  {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right)
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p>In this model, we define an <span class="math inline">\(n\times 2\)</span> matrix <span class="math inline">\(\mathbf{u}\)</span> as coming from a bivariate normal distribution with a variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma_u}\)</span>. This matrix has the variances of the adjustment to the intercept and to the slope respectively along the diagonal, and the covariances on the off-diagonal (lower and upper triangles). The covariance <span class="math inline">\(Cov(u_1,u_2)\)</span> between two variables <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> is defined as the product of their correlation <span class="math inline">\(\rho\)</span> and their standard deviations <span class="math inline">\(\tau_{u_1}\)</span> and <span class="math inline">\(\tau_{u_2}\)</span>, such that, <span class="math inline">\(Cov(u_1,u_2) = \rho_u \tau_{u_1} \tau_{u_2}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Sigma_u} = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{equation}\]</span></p>
<p>In order to specify a prior for <span class="math inline">\(\Sigma_u\)</span>, we need priors for the standard deviations, <span class="math inline">\(\tau_{u_1}\)</span>, and <span class="math inline">\(\tau_{u_2}\)</span>, and also for their correlation, <span class="math inline">\(\rho_u\)</span>. We can use the same priors for <span class="math inline">\(\tau\)</span> as before. For the correlation parameter <span class="math inline">\(\rho_u\)</span> (and the correlation matrix more generally), we use the LKJ prior. The basic idea of the LKJ correlation distribution is that as its parameter, <span class="math inline">\(\eta\)</span> (<em>eta</em>), increases, it will favor a correlation closer to zero.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> At <span class="math inline">\(\eta = 1\)</span>, the LKJ correlation distribution is uninformative (similar to <span class="math inline">\(Beta(1,1)\)</span>), at <span class="math inline">\(\eta &lt; 1\)</span>, it favors extreme correlations (similar to <span class="math inline">\(Beta(a&lt;1,b&lt;1)\)</span>). We set <span class="math inline">\(\eta = 2\)</span> so that we donât favor extreme correlations and we still represent our lack of knowledge. Figure <a href="sec-N400hierarchical.html#fig:lkjviz">5.9</a> shows a visualization of different parametrizations of the LKJ prior.</p>

<div class="figure"><span style="display:block;" id="fig:lkjviz"></span>
<img src="bookdown_files/figure-html/lkjviz-1.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" /><img src="bookdown_files/figure-html/lkjviz-2.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" /><img src="bookdown_files/figure-html/lkjviz-3.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" /><img src="bookdown_files/figure-html/lkjviz-4.svg" alt="Visualization of the LKJ correlation distribution prior with four different values of the \(\eta\) parameter." width="48%" />
<p class="caption">
FIGURE 5.9: Visualization of the LKJ correlation distribution prior with four different values of the <span class="math inline">\(\eta\)</span> parameter.
</p>
</div>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim Normal_+(0,20)\\
\tau_{u_2} &amp;\sim Normal_+(0,20)\\
\rho_u &amp;\sim LKJcorr(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>We indicate in our <code>brms</code> model that we assume a possible correlation between the by-subject intercept and slope with the single pipe <code>|</code>. As before the intercept is implicitly fit. This means that we need to add a new prior for the correlation, <span class="math inline">\(\rho_{u}\)</span>, <code>cor</code> in <code>brms</code>.</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" data-line-number="1">prior_h &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb235-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb235-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb235-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb235-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb235-6" data-line-number="6">    <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept,</a>
<a class="sourceLine" id="cb235-7" data-line-number="7">    <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb235-8" data-line-number="8">  ),</a>
<a class="sourceLine" id="cb235-9" data-line-number="9">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb235-10" data-line-number="10">    <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze,</a>
<a class="sourceLine" id="cb235-11" data-line-number="11">    <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb235-12" data-line-number="12">  ),</a>
<a class="sourceLine" id="cb235-13" data-line-number="13">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor, <span class="dt">group =</span> subj)</a>
<a class="sourceLine" id="cb235-14" data-line-number="14">)</a>
<a class="sourceLine" id="cb235-15" data-line-number="15">fit_N400_h &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb235-16" data-line-number="16">  <span class="dt">prior =</span> prior_h,</a>
<a class="sourceLine" id="cb235-17" data-line-number="17">  <span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb235-18" data-line-number="18">)</a></code></pre></div>
<p>The estimates do not change much in comparison with the varying intercept/slope model, probably because the estimation of the correlation is quite poor (i.e., there is a lot of uncertainty). As before we show the estimates graphically, one can access the complete summary as always with <code>fit_N400_h</code>.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1"><span class="kw">plot</span>(fit_N400_h, <span class="dt">N =</span> <span class="dv">6</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-126-1.svg" width="672" /></p>
<p>We are now half-way to what is called the maximal hierarchical model <span class="citation">(Barr et al. <a href="#ref-barr2013">2013</a>)</span>, because everything that we said about subjects is also relevant for items. The next section spells out this type of model.</p>
</div>
<div id="sec:sih" class="section level3">
<h3><span class="header-section-number">5.1.5</span> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</h3>
<p>Our new model, <span class="math inline">\(M_{sih}\)</span> will allow for differences in intercept (mean voltage) and slopes (effects of predictability) across subjects <em>and</em> across items. Here
we assume a possible correlation between varying intercepts and slopes by subjects, and another one by items.</p>
<ul>
<li>In <span class="math inline">\(M_{sih}\)</span>, we model the EEG data with the following assumptions:</li>
</ul>
<ol style="list-style-type: decimal">
<li>EEG averages for the N400 spatio-temporal window are normally distributed.</li>
<li>Some aspects of the mean signal voltage and of the effect of predictability depend on the participant, i.e., we assume group-level intercepts, and slopes, and a correlation between them by-subject.</li>
<li>Some aspects of the mean signal voltage and of the effect of predictability depend on the item, i.e., we assume group-level intercepts, and slopes, and a correlation between them by-item.
<!-- 3. The variation in the signal is independent from the participant. --></li>
<li>There is a linear relationship between cloze and the EEG signal for the trial.</li>
</ol>
<ul>
<li>Likelihood:</li>
</ul>
<p><span class="math display">\[\begin{equation}
  signal_n \sim Normal(\alpha + u_{subj[n],1} + w_{item[n],1} + c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma)
  \end{equation}\]</span></p>
<ul>
<li>Priors:
<span class="math display">\[\begin{equation}
 \begin{aligned}
 \alpha &amp; \sim Normal(0,10) \\
 \beta  &amp; \sim Normal(0,10) \\
  \sigma  &amp;\sim Normal_+(0,50)\\
  {\begin{pmatrix}
  u_{i,1} \\
  u_{i,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) \\
   {\begin{pmatrix}
  w_{j,1} \\
  w_{j,2}
  \end{pmatrix}}
 &amp;\sim {\mathcal {N}}
  \left(
 {\begin{pmatrix} 
  0\\
  0
 \end{pmatrix}}
 ,\boldsymbol{\Sigma_w} \right) 
 \end{aligned}
 \end{equation}\]</span></li>
</ul>
<p>We have added the index <span class="math inline">\(j\)</span>, which represents each item, as we did with subjects; <span class="math inline">\(item[n]\)</span> indicates the item that corresponds to the observation <span class="math inline">\(n\)</span>.</p>
<p>We have hyperpriors as before:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
 \boldsymbol{\Sigma_u} &amp; = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}\\
 \boldsymbol{\Sigma_w} &amp; = 
{\begin{pmatrix} 
\tau_{w_1}^2 &amp; \rho_w \tau_{w_1} \tau_{w_2} \\ 
\rho_w \tau_{w_1} \tau_{w_2} &amp; \tau_{w_2}^2
\end{pmatrix}}
 \end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim Normal_+(0,20)\\
\tau_{u_2} &amp;\sim Normal_+(0,20)\\
\rho_u &amp;\sim LKJcorr(2) \\
\tau_{w_1} &amp;\sim Normal_+(0,20)\\
\tau_{w_2} &amp;\sim Normal_+(0,20)\\
\rho_w &amp;\sim LKJcorr(2) \\
\end{aligned}
\end{equation}\]</span></p>
<p>We set identical priors to by-items group-level effects as to the by-subject ones, because we donât have different prior information about them. However, bear in mind that the estimation for items is completely independent from the estimation for subjects. Although we wrote many more equations than before, the <code>brms</code> model is quite straightforward to extend:</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb237-1" data-line-number="1">prior_sih_full &lt;-</a>
<a class="sourceLine" id="cb237-2" data-line-number="2"><span class="st">  </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb237-3" data-line-number="3">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb237-4" data-line-number="4">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_cloze),</a>
<a class="sourceLine" id="cb237-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb237-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb237-7" data-line-number="7">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept,</a>
<a class="sourceLine" id="cb237-8" data-line-number="8">      <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb237-9" data-line-number="9">    ),</a>
<a class="sourceLine" id="cb237-10" data-line-number="10">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb237-11" data-line-number="11">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze,</a>
<a class="sourceLine" id="cb237-12" data-line-number="12">      <span class="dt">group =</span> subj</a>
<a class="sourceLine" id="cb237-13" data-line-number="13">    ),</a>
<a class="sourceLine" id="cb237-14" data-line-number="14">    <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor, <span class="dt">group =</span> subject),</a>
<a class="sourceLine" id="cb237-15" data-line-number="15">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb237-16" data-line-number="16">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> Intercept,</a>
<a class="sourceLine" id="cb237-17" data-line-number="17">      <span class="dt">group =</span> item</a>
<a class="sourceLine" id="cb237-18" data-line-number="18">    ),</a>
<a class="sourceLine" id="cb237-19" data-line-number="19">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb237-20" data-line-number="20">      <span class="dt">class =</span> sd, <span class="dt">coef =</span> c_cloze,</a>
<a class="sourceLine" id="cb237-21" data-line-number="21">      <span class="dt">group =</span> item</a>
<a class="sourceLine" id="cb237-22" data-line-number="22">    ),</a>
<a class="sourceLine" id="cb237-23" data-line-number="23">    <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor, <span class="dt">group =</span> item)</a>
<a class="sourceLine" id="cb237-24" data-line-number="24">  )</a>
<a class="sourceLine" id="cb237-25" data-line-number="25"></a>
<a class="sourceLine" id="cb237-26" data-line-number="26">fit_N400_sih &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb237-27" data-line-number="27"><span class="st">  </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb237-28" data-line-number="28"><span class="dt">prior =</span> prior_sih_full,</a>
<a class="sourceLine" id="cb237-29" data-line-number="29"><span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb237-30" data-line-number="30">)</a></code></pre></div>
<p>We can also simplify the call to <code>brms</code>, when we assign the same priors to the by-subject and by-item parameters:</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" data-line-number="1">prior_sih &lt;-</a>
<a class="sourceLine" id="cb238-2" data-line-number="2"><span class="st">  </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb238-3" data-line-number="3">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb238-4" data-line-number="4">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb238-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb238-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb238-7" data-line-number="7">    <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)</a>
<a class="sourceLine" id="cb238-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb238-9" data-line-number="9"></a>
<a class="sourceLine" id="cb238-10" data-line-number="10">fit_N400_sih &lt;-<span class="st"> </span><span class="kw">brm</span>(n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span></a>
<a class="sourceLine" id="cb238-11" data-line-number="11"><span class="st">  </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb238-12" data-line-number="12"><span class="dt">prior =</span> prior_sih,</a>
<a class="sourceLine" id="cb238-13" data-line-number="13"><span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb238-14" data-line-number="14">)</a></code></pre></div>
<p>We have new group-level effects in the summary, but again the estimate of the effect of cloze remains virtually unchanged.</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb239-1" data-line-number="1">fit_N400_sih</a></code></pre></div>
<pre><code>## ...
## Group-Level Effects: 
## ~item (Number of levels: 80) 
##                        Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)              1.52      0.34     0.86     2.16 1.00
## sd(c_cloze)                2.25      1.06     0.20     4.21 1.00
## cor(Intercept,c_cloze)    -0.41      0.33    -0.90     0.33 1.00
##                        Bulk_ESS Tail_ESS
## sd(Intercept)              1484     1620
## sd(c_cloze)                1046     1503
## cor(Intercept,c_cloze)     2450     2413
## 
## ~subj (Number of levels: 37) 
##                        Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)              2.21      0.38     1.55     3.04 1.00
## sd(c_cloze)                1.49      0.89     0.08     3.31 1.00
## cor(Intercept,c_cloze)     0.13      0.36    -0.62     0.78 1.00
##                        Bulk_ESS Tail_ESS
## sd(Intercept)              1553     2738
## sd(c_cloze)                1121     1807
## cor(Intercept,c_cloze)     4168     2719
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     3.66      0.45     2.79     4.55 1.00     2167     2649
## c_cloze       2.32      0.69     0.95     3.67 1.00     4418     3353
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    11.49      0.16    11.18    11.82 1.00     6720     3096
## 
## ...</code></pre>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb241-1" data-line-number="1"><span class="kw">plot</span>(fit_N400_sih, <span class="dt">N =</span> <span class="dv">9</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-131-1.svg" width="672" /></p>

<div class="extra">

<div class="theorem">
<span id="thm:matrixHierachicalModel" class="theorem"><strong>Box 5.3  </strong></span><strong>The Matrix Formulation of Hierarchical Models (the Laird-Ware form)</strong>
</div>
<p>We have been writing linear models as follows; where <span class="math inline">\(n\)</span> refers to the row id in the data-frame.</p>
<p><span class="math display">\[\begin{equation}
y_n \sim Normal(\alpha + \beta\cdot x_n)
\end{equation}\]</span></p>
<p>This simple linear model can be re-written as follows:</p>
<p><span class="math display">\[\begin{equation}
y_n = \alpha + \beta\cdot x_n + \varepsilon_n
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\varepsilon_n \sim Normal(0, \sigma)\)</span>.</p>
<p>If we consider that the model does not change if <span class="math inline">\(\alpha\)</span> is being multiplied by <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[\begin{equation}
y_n = \alpha\cdot 1 + \beta\cdot x_n + \varepsilon_n
\end{equation}\]</span></p>
<p>The above is actually <span class="math inline">\(n\)</span> linear equations, and can be written compactly in matrix form:</p>
<p><span class="math display">\[\begin{equation}
{\begin{pmatrix} 
    y_1\\
    y_2\\
    \vdots \\
    y_n\\
   \end{pmatrix}}
 = 
 {\begin{pmatrix} 
    1 &amp; x_1\\
    1 &amp; x_2\\
    \vdots &amp; \vdots \\
    1 &amp; x_n\\
   \end{pmatrix}}
{\begin{pmatrix} 
    \alpha\\
    \beta \\
   \end{pmatrix}}
+
 {\begin{pmatrix} 
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
     \varepsilon_n \\
   \end{pmatrix}}   
\end{equation}\]</span></p>
<p>Consider this matrix in the above equation:</p>
<p><span class="math display">\[\begin{equation}
{\begin{pmatrix} 
    1 &amp; x_1\\
    1 &amp; x_2\\
    \vdots &amp; \vdots \\
    1 &amp; x_n\\
   \end{pmatrix}}
\end{equation}\]</span></p>
<p>This matrix is called the model matrix or the design matrix; we will encounter it again in the contrast coding chapters, where it plays a crucial role. If we write the dependent variable <span class="math inline">\(y\)</span> as a <span class="math inline">\(n\times 1\)</span> vector, the above matrix as the matrix <span class="math inline">\(X\)</span> (which has dimensions <span class="math inline">\(n\times 2\)</span>, the intercept and slope parameters as a <span class="math inline">\(2\times 1\)</span> matrix <span class="math inline">\(\zeta\)</span>, and the residual errors as an <span class="math inline">\(n\times 1\)</span> matrix, we can write the linear model very compactly:</p>
<p><span class="math display">\[\begin{equation}
y =  X \zeta + \varepsilon
\end{equation}\]</span></p>
<p>The above matrix formulation of the linear model extends to the hierarchical model very straightforwardly. For example,
consider the model <span class="math inline">\(M_{sih}\)</span> that we just saw above. This model has the following likelihood:</p>
<p><span class="math display">\[\begin{multline}
  signal_n \sim Normal(\alpha + u_{subj[n],1} + w_{item[n],1} \\
  + c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma)
  \end{multline}\]</span></p>
<p>The terms in the location parameter in the Normal likelihood can be re-written in matrix form, just like the linear model above. To see this, consider the fact that the location term</p>
<p><span class="math display">\[\begin{equation}
\alpha + u_{subj[n],1} + w_{item[n],1} + c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2})
\end{equation}\]</span></p>
<p>can be re-written as</p>
<p><span class="math display">\[\begin{multline}
\alpha\cdot 1 + u_{subj[n],1}\cdot 1 + w_{item[n],1}\cdot 1 +\\
\beta \cdot c\_cloze_n + u_{subj[n],2}\cdot c\_cloze_n+ w_{item[n],2}\cdot c\_cloze_n
\end{multline}\]</span></p>
<p>The above equation can in turn be written in matrix form:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp;
{\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
{\begin{pmatrix} 
    \alpha\\
    \beta \\
   \end{pmatrix}}
+ \\
&amp; {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
   {\begin{pmatrix} 
    u_{subj[1],1}  &amp; u_{subj[2],1} &amp; \dots &amp; u_{subj[n],1}\\
    u_{subj[1],2} &amp; u_{subj[2],2} &amp;  \dots &amp; u_{subj[n],2}\\
   \end{pmatrix}}
   +\\
&amp; {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}
   {\begin{pmatrix} 
    w_{item[1],1} &amp; w_{item[2],1} &amp; \dots &amp; w_{item[n],1} \\
    w_{item[1],2} &amp; w_{item[2],2} &amp; \dots &amp; w_{item[n],2} \\
   \end{pmatrix}}
 \end{aligned}
\end{equation}\]</span></p>
<p>In this hierarchical model, there are three model matrices:</p>
<ul>
<li>the matrix associated with the intercept <span class="math inline">\(\alpha\)</span> and the slope <span class="math inline">\(\beta\)</span><br />
</li>
<li>the matrix associated with the by-subject varying intercepts and slopes</li>
<li>the matrix associated with the by-item varying intercepts and slopes</li>
</ul>
<p>The model can now be written very compactly in matrix form by writing these three matrices as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
X = &amp; {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}}\\ 
   Z_u = &amp;
   {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}} \\
   Z_w = &amp;
   {\begin{pmatrix} 
    1 &amp; c\_cloze_1\\
    1 &amp; c\_cloze_2\\
    \vdots &amp; \vdots \\
    1 &amp; c\_cloze_n\\
   \end{pmatrix}} \\
\end{aligned}
\end{equation}\]</span></p>
<p>The location part of the model <span class="math inline">\(M_{sih}\)</span> can now be written very compactly:</p>
<p><span class="math display">\[\begin{equation}
X \zeta
+
Z_u z_u
+ 
Z_w z_w
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\zeta\)</span> is a <span class="math inline">\(2\times 1\)</span> matrix containing the intercept <span class="math inline">\(\alpha\)</span> and the slope <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(z_u\)</span> and <span class="math inline">\(z_w\)</span> are the intercept and slope adjustments by subject and by item:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
z_u = &amp;
 {\begin{pmatrix} 
    u_{subj[1],1}  &amp; u_{subj[2],1} &amp; \dots &amp; u_{subj[n],1}\\
    u_{subj[1],2} &amp; u_{subj[2],2} &amp;  \dots &amp; u_{subj[n],2}\\
   \end{pmatrix}}\\
 z_w = &amp; 
    {\begin{pmatrix} 
    w_{item[1],1} &amp; w_{item[2],1} &amp; \dots &amp; w_{item[n],1} \\
    w_{item[1],2} &amp; w_{item[2],2} &amp; \dots &amp; w_{item[n],2} \\
   \end{pmatrix}}\\
\end{aligned}
\end{equation}\]</span></p>
<p>In summary, the hierarchical model has a very general matrix formulation, called the Laird-Ware form <span class="citation">(Laird and Ware <a href="#ref-laird1982random">1982</a>)</span>:</p>
<p><span class="math display">\[\begin{equation}
 signal = X \zeta
+
Z_u z_u
+ 
Z_w z_w +
\varepsilon
 \end{equation}\]</span></p>
<p>The practical relevance of this matrix formulation is that we can define hierarchical models very compactly and efficiently in Stan by expressing the model in terms of the model matrices <span class="citation">(Sorensen, Hohenstein, and Vasishth <a href="#ref-SorensenVasishthTutorial">2016</a>)</span>.</p>
</div>
</div>
<div id="sec:distrmodel" class="section level3">
<h3><span class="header-section-number">5.1.6</span> Beyond the so-called maximal modelsâDistributional regression models</h3>
<p>We can use posterior predictive checks to verify that our last model can capture the entire signal distribution.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" data-line-number="1"><span class="kw">pp_check</span>(fit_N400_sih, <span class="dt">nsamples =</span> <span class="dv">50</span>, <span class="dt">type =</span> <span class="st">&quot;dens_overlay&quot;</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-132-1.svg" width="672" /></p>
<p>However, we know that in ERP studies, large levels of impedance between the recording electrodes and the skin tissue increase the noise in the recordings <span class="citation">(Picton et al. <a href="#ref-picton_etal_2000">2000</a>)</span>. Given that skin tissue is different between subjects, it could be the case that the level of noise varies by participant.
It might be a good idea to verify that our model is good enough for capturing the by-subject data pattern. We plot it in Figure <a href="sec-N400hierarchical.html#fig:postpreddensbysubj">5.10</a>.</p>

<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb243-1" data-line-number="1"><span class="kw">ppc_dens_overlay_grouped</span>(df_eeg<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb243-2" data-line-number="2">  <span class="dt">yrep =</span></a>
<a class="sourceLine" id="cb243-3" data-line-number="3">    <span class="kw">posterior_predict</span>(fit_N400_sih,</a>
<a class="sourceLine" id="cb243-4" data-line-number="4">      <span class="dt">nsamples =</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb243-5" data-line-number="5">    ),</a>
<a class="sourceLine" id="cb243-6" data-line-number="6">  <span class="dt">group =</span> df_eeg<span class="op">$</span>subj</a>
<a class="sourceLine" id="cb243-7" data-line-number="7">) <span class="op">+</span></a>
<a class="sourceLine" id="cb243-8" data-line-number="8"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Signal in the N400 spatiotemporal window&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpreddensbysubj"></span>
<img src="bookdown_files/figure-html/postpreddensbysubj-1.svg" alt="The plot shows 100 predicted distributions in blue density plots and the distribution of the average signal data in black density plots for the 37 subjects that participated in the experiment." width="672" />
<p class="caption">
FIGURE 5.10: The plot shows 100 predicted distributions in blue density plots and the distribution of the average signal data in black density plots for the 37 subjects that participated in the experiment.
</p>
</div>
<p>Figure <a href="sec-N400hierarchical.html#fig:postpreddensbysubj">5.10</a> hints that we might be misfitting some subjects: Some of the by-participant observed distributions of the EEG signal averages look much tighter than their corresponding posterior predictive distributions (e.g., subjects 3, 5, 9, 10, 14), whereas some other by-participant observed distributions look wider (e.g., subjects 25, 26, 27). Another approach to examine whether we misfit the by-subject noise level is to plot posterior distributions of the standard deviations and compared them with the observed standard deviation. This is achieved in the following code that groups by subject and shows the distribution of standard deviations; the result is shown in Figure <a href="sec-N400hierarchical.html#fig:postpredsumbysubj">5.11</a>. It is clear now that, for some subjects, the observed standard deviation lies outside the distribution of predictive standard deviations.</p>

<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" data-line-number="1"><span class="kw">pp_check</span>(fit_N400_sih,</a>
<a class="sourceLine" id="cb244-2" data-line-number="2">  <span class="dt">type =</span> <span class="st">&quot;stat_grouped&quot;</span>,</a>
<a class="sourceLine" id="cb244-3" data-line-number="3">  <span class="dt">nsamples =</span> <span class="dv">1000</span>,</a>
<a class="sourceLine" id="cb244-4" data-line-number="4">  <span class="dt">group =</span> <span class="st">&quot;subj&quot;</span>,</a>
<a class="sourceLine" id="cb244-5" data-line-number="5">  <span class="dt">stat =</span> <span class="st">&quot;sd&quot;</span></a>
<a class="sourceLine" id="cb244-6" data-line-number="6">)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpredsumbysubj"></span>
<img src="bookdown_files/figure-html/postpredsumbysubj-1.svg" alt="Distribution of posterior predicted standard deviations in gray and observed standard deviation in black lines by subject." width="672" />
<p class="caption">
FIGURE 5.11: Distribution of posterior predicted standard deviations in gray and observed standard deviation in black lines by subject.
</p>
</div>
<p>Why is our âmaximalâ hierarchical model misfitting the by-subject distribution of data? This is because, the maximal models are, in general and implicitly, models with the maximal group-level effect structure for the location parameter (e.g., the mean, <span class="math inline">\(\mu\)</span>, in a normal model). Other parameters (e.g., scale or shape parameters) are estimated as auxiliary parameters assuming them to be constant across observations and clusters. This assumption is so common that researchers may not be aware that it is just an assumption, which (in the Bayesian framework) can be changed. Changing this assumption leads to distributional regression models. These can be fit in <code>brms</code>.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
<p>We are going to change our previous likelihood, so that the scale, <span class="math inline">\(\sigma\)</span> has also a group-level effect structure. We exponentiate <span class="math inline">\(\sigma\)</span> to make sure that the negative adjustments do not cause <span class="math inline">\(\sigma\)</span> to become negative.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  signal_n &amp;\sim Normal(\alpha + u_{subj[n],1} + w_{item[n],1} + \\             &amp;  \hspace{2cm} c\_cloze_n \cdot  (\beta + u_{subj[n],2}+ w_{item[n],2}), \sigma_n)\\
  \sigma_n &amp;= \exp(\sigma_\alpha + \sigma_{u_{subj[n]}})
\end{aligned}
\end{equation}\]</span></p>
<p>We just need to add priors to our new parameters (that replace the old prior for <span class="math inline">\(\sigma\)</span>). We set the prior to the intercept of the standard deviation, <span class="math inline">\(\sigma_\alpha\)</span>, to be similar to our previous <span class="math inline">\(\sigma\)</span>. For the variance component of <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\tau_{\sigma_u}\)</span>, we set quite vague hyperpriors. Recall that everything is exponentiated when it goes inside the likelihood, and it way we use <span class="math inline">\(\log(50)\)</span> rather than 50 in <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  \sigma_\alpha &amp;\sim Normal(0,log(50))\\
  \sigma_u &amp;\sim Normal(0, \tau_{\sigma_u}) \\
  \tau_{\sigma_u} &amp;\sim Normal_+(0, 5)
\end{aligned}
\end{equation}\]</span></p>
<p>This model can be fit in <code>brms</code> using the internal function <code>bf()</code>. This will allow us to set a hierarchical structure (and any regression) to the parameter <span class="math inline">\(\sigma\)</span>. We also need to set new priors; these priors are identified by <code>dpar = sigma</code>.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb245-1" data-line-number="1">prior_s &lt;-<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb245-2" data-line-number="2">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb245-3" data-line-number="3">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb245-4" data-line-number="4">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb245-5" data-line-number="5">  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor),</a>
<a class="sourceLine" id="cb245-6" data-line-number="6">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="kw">log</span>(<span class="dv">50</span>)), <span class="dt">class =</span> Intercept, <span class="dt">dpar =</span> sigma),</a>
<a class="sourceLine" id="cb245-7" data-line-number="7">  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>),</a>
<a class="sourceLine" id="cb245-8" data-line-number="8">    <span class="dt">class =</span> sd, <span class="dt">group =</span> subj,</a>
<a class="sourceLine" id="cb245-9" data-line-number="9">    <span class="dt">dpar =</span> sigma</a>
<a class="sourceLine" id="cb245-10" data-line-number="10">  )</a>
<a class="sourceLine" id="cb245-11" data-line-number="11">)</a>
<a class="sourceLine" id="cb245-12" data-line-number="12">fit_N400_s &lt;-<span class="st"> </span><span class="kw">brm</span>(<span class="kw">bf</span>(</a>
<a class="sourceLine" id="cb245-13" data-line-number="13">  n400 <span class="op">~</span><span class="st"> </span>c_cloze <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>subj) <span class="op">+</span><span class="st"> </span>(c_cloze <span class="op">|</span><span class="st"> </span>item),</a>
<a class="sourceLine" id="cb245-14" data-line-number="14">  sigma <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>subj)</a>
<a class="sourceLine" id="cb245-15" data-line-number="15">),</a>
<a class="sourceLine" id="cb245-16" data-line-number="16"><span class="dt">prior =</span> prior_s,</a>
<a class="sourceLine" id="cb245-17" data-line-number="17"><span class="dt">data =</span> df_eeg</a>
<a class="sourceLine" id="cb245-18" data-line-number="18">)</a></code></pre></div>
<p>We inspect the output below, and we see that our estimate for the effect of cloze
remains very similar to our previous one.</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_N400_s)[<span class="st">&quot;b_c_cloze&quot;</span>, ]</a></code></pre></div>
<pre><code>##  Estimate Est.Error      Q2.5     Q97.5 
##     2.299     0.664     0.976     3.624</code></pre>
<p>Nonetheless, Figure <a href="sec-N400hierarchical.html#fig:postpreddensbysubj2">5.12</a> shows that the fit of the model with respect to the by-subject variability is much better than before. Furthermore, Figure <a href="sec-N400hierarchical.html#fig:postpredsumbysubj2">5.13</a> shows that the observed standard deviations for each subject are well inside the posterior predictive distributions.</p>

<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" data-line-number="1"><span class="kw">ppc_dens_overlay_grouped</span>(df_eeg<span class="op">$</span>n400,</a>
<a class="sourceLine" id="cb248-2" data-line-number="2">  <span class="dt">yrep =</span></a>
<a class="sourceLine" id="cb248-3" data-line-number="3">    <span class="kw">posterior_predict</span>(fit_N400_s,</a>
<a class="sourceLine" id="cb248-4" data-line-number="4">      <span class="dt">nsamples =</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb248-5" data-line-number="5">    ),</a>
<a class="sourceLine" id="cb248-6" data-line-number="6">  <span class="dt">group =</span> df_eeg<span class="op">$</span>subj</a>
<a class="sourceLine" id="cb248-7" data-line-number="7">) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-8" data-line-number="8"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Signal in the N400 spatiotemporal window&quot;</span>)</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:postpreddensbysubj2"></span>
<img src="bookdown_files/figure-html/postpreddensbysubj2-1.svg" alt="The plot shows 100 predicted distributions for the model that includes a hierarchical structure for \(\sigma\) in blue density plots and the distribution of the average signal data in black density plots for the 37 subjects that participated in the experiment." width="672" />
<p class="caption">
FIGURE 5.12: The plot shows 100 predicted distributions for the model that includes a hierarchical structure for <span class="math inline">\(\sigma\)</span> in blue density plots and the distribution of the average signal data in black density plots for the 37 subjects that participated in the experiment.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:postpredsumbysubj2"></span>
<img src="bookdown_files/figure-html/postpredsumbysubj2-1.svg" alt="Distribution of posterior predicted standard deviations for the model that includes a hierarchical structure for \(\sigma\) in gray and observed standard deviation in black lines by subject." width="672" />
<p class="caption">
FIGURE 5.13: Distribution of posterior predicted standard deviations for the model that includes a hierarchical structure for <span class="math inline">\(\sigma\)</span> in gray and observed standard deviation in black lines by subject.
</p>
</div>
<p>This raises the question of how much structure should we add to our statistical model. Should we assume that <span class="math inline">\(\sigma\)</span> can also vary by items, and also by our experimental manipulation? Should we have a maximal model also for <span class="math inline">\(\sigma\)</span>? Unfortunately, there are no clear answers that apply to every situation. The amount of complexity that we can introduce in a statistical model depends on (i) the answers we are looking for, that is, we should have the parameters that represent what we want to estimate, (ii) the size of the data at hand (more complex models require more data), (iii) our computing power; as the complexity increases models take increasingly long to converge and require more computer power to finish in a feasible time frame, and (iv) our domain and experimental knowledge.</p>
<p>Ultimately, all models are approximations (in the best case, when they are not plainly wrong) and we need to think carefully about which aspects of our data we have to account and which aspects we can abstract away from.</p>
<p>In the context of cognitive modeling, <span class="citation">McClelland (<a href="#ref-mcclellandPlaceModelingCognitive2009">2009</a>)</span> argues that models should not focus on a every single detail of the process they intend to explain. In order to understand a model, it needs to be simple enough. However, <span class="citation">McClelland (<a href="#ref-mcclellandPlaceModelingCognitive2009">2009</a>)</span> warns us that one must bear in mind that simplification does impact on what we can conclude from our analysis: A simplification can limit the phenomena that a model addresses, or can even lead to incorrect predictions. There is a continuum between purely statistical models (e.g., a linear regression) and computational cognitive models, that includes âhybridâ models such as the linear ballistic accumulator, where a great deal of cognitive detail is sacrificed for tractability. The conclusions of <span class="citation">McClelland (<a href="#ref-mcclellandPlaceModelingCognitive2009">2009</a>)</span> apply to any type of model in cognitive science: âSimplification is essential, but it comes at a cost, and real understanding depends in part on understanding the effects of the simplificationâ.</p>
<!-- ## Posterior predictive checks -->
<!-- But is the fit with the effect of cloze good?  -->
<!-- ```{r} -->
<!-- yrep <- posterior_predict(fit_N400_sih, nsamples = 50) -->
<!-- data_sims <- array_branch(yrep,1) %>% map_dfr(~ mutate(df_eeg, yrep = ., -->
<!--                                           ints = cut(cloze ,breaks = 10)) %>% -->
<!--                                      group_by(ints) %>% -->
<!--                                 summarize(yrep = mean(yrep)),.id= "sim") -->
<!-- df_eeg_sum <- df_eeg_data %>% -->
<!--     mutate(ints = cut(cloze ,breaks = 10)) %>% -->
<!--                group_by(ints) %>% -->
<!--                summarize(n400 = mean(n400)) -->
<!-- ggplot(data_sims, aes(x=ints, y =yrep)) + geom_violin() + -->
<!--     geom_point(data=df_eeg_sum, aes(x=ints, y=n400), inherit.aes = FALSE) -->
<!-- ``` -->
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-barr2013">
<p>Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. âRandom Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.â <em>Journal of Memory and Language</em> 68 (3). Elsevier: 255â78.</p>
</div>
<div id="ref-delongProbabilisticWordPreactivation2005">
<p>DeLong, Katherine A, Thomas P Urbach, and Marta Kutas. 2005. âProbabilistic Word Pre-Activation During Language Comprehension Inferred from Electrical Brain Activity.â <em>Nature Neuroscience</em> 8 (8): 1117â21. <a href="https://doi.org/10.1038/nn1504" class="uri">https://doi.org/10.1038/nn1504</a>.</p>
</div>
<div id="ref-frankERPResponseAmount2015">
<p>Frank, Stefan L., Leun J. Otten, Giulia Galli, and Gabriella Vigliocco. 2015. âThe ERP Response to the Amount of Information Conveyed by Words in Sentences.â <em>Brain and Language</em> 140: 1â11. <a href="https://doi.org/10.1016/j.bandl.2014.10.006" class="uri">https://doi.org/10.1016/j.bandl.2014.10.006</a>.</p>
</div>
<div id="ref-GelmanHill2007">
<p>Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge University Press.</p>
</div>
<div id="ref-kutasThirtyYearsCounting2011">
<p>Kutas, Marta, and Kara D. Federmeier. 2011. âThirty Years and Counting: Finding Meaning in the N400 Componentof the Event-Related Brain Potential (ERP).â <em>Annual Review of Psychology</em> 62 (1): 621â47. <a href="https://doi.org/10.1146/annurev.psych.093008.131123" class="uri">https://doi.org/10.1146/annurev.psych.093008.131123</a>.</p>
</div>
<div id="ref-kutasReadingSenselessSentences1980">
<p>Kutas, Marta, and Steven A Hillyard. 1980. âReading Senseless Sentences: Brain Potentials Reflect Semantic Incongruity.â <em>Science</em> 207 (4427): 203â5. <a href="https://doi.org/10.1126/science.7350657" class="uri">https://doi.org/10.1126/science.7350657</a>.</p>
</div>
<div id="ref-kutasBrainPotentialsReading1984">
<p>Kutas, Marta, and Steven A Hillyard. 1984. âBrain Potentials During Reading Reflect Word Expectancy and Semantic Association.â <em>Nature</em> 307 (5947): 161â63. <a href="https://doi.org/10.1038/307161a0" class="uri">https://doi.org/10.1038/307161a0</a>.</p>
</div>
<div id="ref-laird1982random">
<p>Laird, Nan M, and James H Ware. 1982. âRandom-Effects Models for Longitudinal Data.â <em>Biometrics</em>. JSTOR, 963â74.</p>
</div>
<div id="ref-mcclellandPlaceModelingCognitive2009">
<p>McClelland, James L. 2009. âThe Place of Modeling in Cognitive Science.â <em>Topics in Cognitive Science</em> 1 (1): 11â38. <a href="https://doi.org/10.1111/j.1756-8765.2008.01003.x" class="uri">https://doi.org/10.1111/j.1756-8765.2008.01003.x</a>.</p>
</div>
<div id="ref-mcelreath2015statistical">
<p>McElreath, Richard. 2015. <em>Statistical Rethinking: A Bayesian Course with R Examples</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-nicenboim_vasishth_rosler_2020">
<p>Nicenboim, Bruno, Shravan Vasishth, and Frank RÃ¶sler. 2020a. âAre Words Pre-Activated Probabilistically During Sentence Comprehension? Evidence from New Data and a Bayesian Random-Effects Meta-Analysis Using Publicly Available Data.â <em>Neuropsychologia</em> 142. <a href="https://doi.org/10.1016/j.neuropsychologia.2020.107427" class="uri">https://doi.org/10.1016/j.neuropsychologia.2020.107427</a>.</p>
</div>
<div id="ref-nieuwlandLargescaleReplicationStudy2018">
<p>Nieuwland, Mante S, Stephen Politzer-Ahles, Evelien Heyselaar, Katrien Segaert, Emily Darley, Nina Kazanina, Sarah Von Grebmer Zu Wolfsthurn, et al. 2018. âLarge-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension.â <em>eLife</em> 7. <a href="https://doi.org/10.7554/eLife.33468" class="uri">https://doi.org/10.7554/eLife.33468</a>.</p>
</div>
<div id="ref-picton_etal_2000">
<p>Picton, T.W., S. Bentin, P. Berg, E. Donchin, S.A. Hillyard, R. Johnson JR., G.A. Miller, et al. 2000. âGuidelines for Using Human Event-Related Potentials to Study Cognition: Recording Standards and Publication Criteria.â <em>Psychophysiology</em> 37 (2): 127â52. <a href="https://doi.org/10.1111/1469-8986.3720127" class="uri">https://doi.org/10.1111/1469-8986.3720127</a>.</p>
</div>
<div id="ref-SorensenVasishthTutorial">
<p>Sorensen, Tanner, Sven Hohenstein, and Shravan Vasishth. 2016. âBayesian Linear Mixed Models Using Stan: A Tutorial for Psychologists, Linguists, and Cognitive Scientists.â <em>Quantitative Methods for Psychology</em> 12 (3): 175â200. <a href="http://www.ling.uni-potsdam.de/~vasishth/statistics/BayesLMMs.html" class="uri">http://www.ling.uni-potsdam.de/~vasishth/statistics/BayesLMMs.html</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>For simplicity, we assume that they share the same standard deviation.<a href="sec-N400hierarchical.html#fnref13" class="footnote-back">â©</a></p></li>
<li id="fn14"><p>An analogous frequentist model can be fit with <code>lmer</code> from the package <code>lme4</code>, using <code>(c_cloze||subj)</code> for the random effects.<a href="sec-N400hierarchical.html#fnref14" class="footnote-back">â©</a></p></li>
<li id="fn15"><p>The intercept adjustment is often called <span class="math inline">\(u_0\)</span> in statistics books, where the intercept might be called <span class="math inline">\(\alpha\)</span> or (sometimes also <span class="math inline">\(\beta_0\)</span>), and thus <span class="math inline">\(u_1\)</span> refers to the adjustment to the slope. However, in this book, we start the indexing with 1 to be consistent with the Stan language.<a href="sec-N400hierarchical.html#fnref15" class="footnote-back">â©</a></p></li>
<li id="fn16"><p>Another source of confusion here is that <em>hyperparameters</em> is also used in the machine learning literature with a different meaning.<a href="sec-N400hierarchical.html#fnref16" class="footnote-back">â©</a></p></li>
<li id="fn17"><p>One could in theory keep going deeper and deeper, defining hyper-hyperpriors etc., but the model would quickly become impossible to fit.<a href="sec-N400hierarchical.html#fnref17" class="footnote-back">â©</a></p></li>
<li id="fn18"><p>This is because an LKJ correlation distribution with a large <span class="math inline">\(\eta\)</span> corresponds to a correlation matrix with values close to zero in the lower and upper triangles<a href="sec-N400hierarchical.html#fnref18" class="footnote-back">â©</a></p></li>
<li id="fn19"><p><a href="https://web.archive.org/web/20191206093021/https://paul-buerkner.github.io/brms/articles/brms_distreg.html" class="uri">https://web.archive.org/web/20191206093021/https://paul-buerkner.github.io/brms/articles/brms_distreg.html</a><a href="sec-N400hierarchical.html#fnref19" class="footnote-back">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-hierarchical.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-stroop.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/05-hierarchical.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
