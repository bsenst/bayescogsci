<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.2 A hierarchical log-normal model: The Stroop effect | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="5.2 A hierarchical log-normal model: The Stroop effect | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.2 A hierarchical log-normal model: The Stroop effect | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2021-02-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-N400hierarchical.html"/>
<link rel="next" href="summary-2.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="the-law-of-total-probability.html"><a href="the-law-of-total-probability.html"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs.Â density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-2-continuous-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#generate-simulated-bivariate-multivariate-data"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sec-marginal.html"><a href="sec-marginal.html"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.9</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.10" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.11</b> Exercises</a><ul>
<li class="chapter" data-level="1.11.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.11.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.11.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.11.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.11.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.11.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.11.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.11.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.11.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.11.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayesâ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#sec:choosepriortheta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayesâ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayesâ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Regression models</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="sec-sampling.html"><a href="sec-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec-sampling.html"><a href="sec-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using âStanâ: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#sec:comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix.html"><a href="appendix.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal modelsâDistributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="6.1.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining hypotheses</a></li>
<li class="chapter" data-level="6.1.3" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#sec:cellMeans"><i class="fa fa-check"></i><b>6.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="6.2.1" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html"><i class="fa fa-check"></i><b>6.3</b> Further examples of contrasts illustrated with a factor with four levels</a><ul>
<li class="chapter" data-level="6.3.1" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.2</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="6.3.3" data-path="sec-4levelFactor.html"><a href="sec-4levelFactor.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.3</b> Polynomial contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="6.4.1" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="computing-condition-means-from-estimated-contrasts.html"><a href="computing-condition-means-from-estimated-contrasts.html"><i class="fa fa-check"></i><b>6.5</b> Computing condition means from estimated contrasts</a></li>
<li class="chapter" data-level="6.6" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-coding2x2.html"><a href="ch-coding2x2.html"><i class="fa fa-check"></i><b>7</b> Contrast coding for designs with two predictor variables</a><ul>
<li class="chapter" data-level="7.1" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html"><i class="fa fa-check"></i><b>7.1</b> Contrast coding in a factorial 2 x 2 design</a><ul>
<li class="chapter" data-level="7.1.1" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html#the-difference-between-an-anova-and-a-multiple-regression"><i class="fa fa-check"></i><b>7.1.1</b> The difference between an ANOVA and a multiple regression</a></li>
<li class="chapter" data-level="7.1.2" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html#nestedEffects"><i class="fa fa-check"></i><b>7.1.2</b> Nested effects</a></li>
<li class="chapter" data-level="7.1.3" data-path="sec-MR-ANOVA.html"><a href="sec-MR-ANOVA.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>7.1.3</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html"><i class="fa fa-check"></i><b>7.2</b> One factor and one covariate</a><ul>
<li class="chapter" data-level="7.2.1" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>7.2.1</b> Estimating a group-difference and controlling for a covariate</a></li>
<li class="chapter" data-level="7.2.2" data-path="sec-contrast-covariate.html"><a href="sec-contrast-covariate.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>7.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="sec-interactions-NLM.html"><a href="sec-interactions-NLM.html"><i class="fa fa-check"></i><b>7.3</b> Interactions in generalized linear models (with non-linear link functions)</a></li>
<li class="chapter" data-level="7.4" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>8</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="8.1" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>8.1</b> Meta-analysis</a></li>
<li class="chapter" data-level="8.2" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>8.2</b> Measurement-error models</a></li>
<li class="chapter" data-level="8.3" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>8.3</b> Further reading</a></li>
<li class="chapter" data-level="8.4" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>8.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Model comparison</b></span></li>
<li class="chapter" data-level="9" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>9</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="10" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>10</b> Bayes factors</a><ul>
<li class="chapter" data-level="10.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html"><i class="fa fa-check"></i><b>10.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="10.1.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#marginal-likelihood"><i class="fa fa-check"></i><b>10.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="10.1.2" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#bayes-factor"><i class="fa fa-check"></i><b>10.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html"><i class="fa fa-check"></i><b>10.2</b> Testing the N400 effect using null hypothesis testing</a><ul>
<li class="chapter" data-level="10.2.1" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sensitivity-analysis"><i class="fa fa-check"></i><b>10.2.1</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="10.2.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sec:BFnonnested"><i class="fa fa-check"></i><b>10.2.2</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest.html"><a href="the-influence-of-the-priors-on-bayes-factors-beyond-the-effect-of-interest.html"><i class="fa fa-check"></i><b>10.3</b> The influence of the priors on Bayes factors: beyond the effect of interest</a></li>
<li class="chapter" data-level="10.4" data-path="bayes-factors-in-theory-stability-and-accuracy.html"><a href="bayes-factors-in-theory-stability-and-accuracy.html"><i class="fa fa-check"></i><b>10.4</b> Bayes factors in theory: Stability and accuracy</a><ul>
<li class="chapter" data-level="10.4.1" data-path="bayes-factors-in-theory-stability-and-accuracy.html"><a href="bayes-factors-in-theory-stability-and-accuracy.html#instability-due-to-the-effective-number-of-posterior-samples"><i class="fa fa-check"></i><b>10.4.1</b> Instability due to the effective number of posterior samples</a></li>
<li class="chapter" data-level="10.4.2" data-path="bayes-factors-in-theory-stability-and-accuracy.html"><a href="bayes-factors-in-theory-stability-and-accuracy.html#inaccuracy-of-bayes-factors-does-the-estimate-approximate-the-true-bayes-factor-well"><i class="fa fa-check"></i><b>10.4.2</b> Inaccuracy of Bayes factors: Does the estimate approximate the true Bayes factor well?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="bayes-factors-in-practice-variability-with-the-data.html"><a href="bayes-factors-in-practice-variability-with-the-data.html"><i class="fa fa-check"></i><b>10.5</b> Bayes factors in practice: Variability with the data</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayes-factors-in-practice-variability-with-the-data.html"><a href="bayes-factors-in-practice-variability-with-the-data.html#variation-associated-with-the-data-subjects-items-and-residual-noise"><i class="fa fa-check"></i><b>10.5.1</b> Variation associated with the data (subjects, items, and residual noise)</a></li>
<li class="chapter" data-level="10.5.2" data-path="bayes-factors-in-practice-variability-with-the-data.html"><a href="bayes-factors-in-practice-variability-with-the-data.html#example-2-inhibitory-and-facilitatory-interference-effects"><i class="fa fa-check"></i><b>10.5.2</b> Example 2: Inhibitory and facilitatory interference effects</a></li>
<li class="chapter" data-level="10.5.3" data-path="bayes-factors-in-practice-variability-with-the-data.html"><a href="bayes-factors-in-practice-variability-with-the-data.html#variability-of-the-bayes-factor-posterior-simulations"><i class="fa fa-check"></i><b>10.5.3</b> Variability of the Bayes factor: Posterior simulations</a></li>
<li class="chapter" data-level="10.5.4" data-path="bayes-factors-in-practice-variability-with-the-data.html"><a href="bayes-factors-in-practice-variability-with-the-data.html#visualize-distribution-of-bayes-factors"><i class="fa fa-check"></i><b>10.5.4</b> Visualize distribution of Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
<li class="chapter" data-level="10.7" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>10.7</b> Further reading</a></li>
<li class="chapter" data-level="10.8" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>10.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>11</b> Cross validation</a><ul>
<li class="chapter" data-level="11.1" data-path="expected-log-predictive-density-of-a-model.html"><a href="expected-log-predictive-density-of-a-model.html"><i class="fa fa-check"></i><b>11.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="11.2" data-path="k-fold-and-leave-one-out-cross-validation.html"><a href="k-fold-and-leave-one-out-cross-validation.html"><i class="fa fa-check"></i><b>11.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="11.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html"><i class="fa fa-check"></i><b>11.3</b> Testing the N400 effect using cross-validation</a></li>
<li class="chapter" data-level="11.4" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
<li class="chapter" data-level="11.5" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>11.5</b> Further reading</a></li>
<li class="chapter" data-level="11.6" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Advanced models with Stan</b></span></li>
<li class="chapter" data-level="12" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>12</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="12.1" data-path="stan-syntax.html"><a href="stan-syntax.html"><i class="fa fa-check"></i><b>12.1</b> Stan syntax</a></li>
<li class="chapter" data-level="12.2" data-path="sec-firststan.html"><a href="sec-firststan.html"><i class="fa fa-check"></i><b>12.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="12.3" data-path="sec-clozestan.html"><a href="sec-clozestan.html"><i class="fa fa-check"></i><b>12.3</b> Another simple example: Cloze probability with Stan: Binomial likelihood</a></li>
<li class="chapter" data-level="12.4" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html"><i class="fa fa-check"></i><b>12.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="12.4.1" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:pupilstan"><i class="fa fa-check"></i><b>12.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="12.4.2" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:interstan"><i class="fa fa-check"></i><b>12.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="12.4.3" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:logisticstan"><i class="fa fa-check"></i><b>12.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html"><i class="fa fa-check"></i><b>12.5</b> Model comparison in Stan</a><ul>
<li class="chapter" data-level="12.5.1" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#bayes-factor-in-stan"><i class="fa fa-check"></i><b>12.5.1</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="12.5.2" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>12.5.2</b> Cross validation in Stan</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="further-reading-8.html"><a href="further-reading-8.html"><i class="fa fa-check"></i><b>12.7</b> Further reading</a></li>
<li class="chapter" data-level="12.8" data-path="exercises-7.html"><a href="exercises-7.html"><i class="fa fa-check"></i><b>12.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>13</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="13.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html"><i class="fa fa-check"></i><b>13.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="13.1.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>13.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="13.1.2" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:uncorrstan"><i class="fa fa-check"></i><b>13.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="13.1.3" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:corrstan"><i class="fa fa-check"></i><b>13.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="13.1.4" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:crosscorrstan"><i class="fa fa-check"></i><b>13.1.4</b> By-participant and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="summary-8.html"><a href="summary-8.html"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
<li class="chapter" data-level="13.3" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>13.3</b> Further reading</a></li>
<li class="chapter" data-level="13.4" data-path="exercises-8.html"><a href="exercises-8.html"><i class="fa fa-check"></i><b>13.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Computational cognitive modeling</b></span></li>
<li class="chapter" data-level="14" data-path="introduction-to-computational-cognitive-modeling-chcogmod.html"><a href="introduction-to-computational-cognitive-modeling-chcogmod.html"><i class="fa fa-check"></i><b>14</b> Introduction to computational cognitive modeling {ch:cogmod}</a><ul>
<li class="chapter" data-level="14.1" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>14.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>15</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="15.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>15.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="15.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>15.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="15.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>15.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>15.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="15.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>15.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>15.3</b> Further reading</a></li>
<li class="chapter" data-level="15.4" data-path="exercises-9.html"><a href="exercises-9.html"><i class="fa fa-check"></i><b>15.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>16</b> Mixture models</a><ul>
<li class="chapter" data-level="16.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html"><i class="fa fa-check"></i><b>16.1</b> A mixture model of the speed-accuracy trade-off</a><ul>
<li class="chapter" data-level="16.1.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#a-fast-guess-model-account-of-the-global-motion-detection-task"><i class="fa fa-check"></i><b>16.1.1</b> A fast guess model account of the global motion detection task</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="summary-9.html"><a href="summary-9.html"><i class="fa fa-check"></i><b>16.2</b> Summary</a></li>
<li class="chapter" data-level="16.3" data-path="further-reading-12.html"><a href="further-reading-12.html"><i class="fa fa-check"></i><b>16.3</b> Further reading</a></li>
<li class="chapter" data-level="16.4" data-path="exercises-10.html"><a href="exercises-10.html"><i class="fa fa-check"></i><b>16.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Appendix</b></span></li>
<li class="chapter" data-level="17" data-path="ch-distr.html"><a href="ch-distr.html"><i class="fa fa-check"></i><b>17</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:stroop" class="section level2">
<h2><span class="header-section-number">5.2</span> A hierarchical log-normal model: The Stroop effect</h2>
<p>We will illustrate the issues that arise with a log-normal likelihood in a hierarchical model using data from a Stroop task <span class="citation">(Stroop <a href="#ref-stroop1935studies">1935</a>; for a review, see MacLeod <a href="#ref-macleod1991half">1991</a>)</span>. We will analyze a subset of the data of 3337 participants that undertook one variant of the Stroop task as part of the battery of tasks run in <span class="citation">Ebersole et al. (<a href="#ref-ManyLabs3">2016</a>)</span>.</p>
<p>For this variant of the Stroop task, participants were presented with one word at the center of the screen, which was either âredâ, âblueâ, and âgreenâ (<em>word</em>) written in either red, blue, or green font (<em>color</em>). In one third of the trials the <em>word</em> matched the <em>color</em> of the text (âcongruentâ condition) and in the rest of the trials it did not match (âincongruentâ condition). Participants were instructed to only pay attention to the color, and press <code>1</code> if the color of the word was red, <code>2</code> if it was blue, and <code>3</code> if it was green. The complete procedure can be found in <a href="https://osf.io/5ykuj/" class="uri">https://osf.io/5ykuj/</a> and the dataset can be found in <a href="https://osf.io/n8xa7/" class="uri">https://osf.io/n8xa7/</a>. The Stroop effect, that is, the difficulty in identifying the color when it mismatches the word in the incongruent condition (e.g., green in color blue) in comparison to a baseline condition, here, the congruent condition (e.g., green in color green) is extremely robust across variations of the task.</p>
<p>While this task yields two measures: the accuracy of the decision made and the time it took to respond. For the Stroop task, accuracy is usually almost at ceiling level, and to simplify the model, we will ignore it. <span class="citation">(See Nicenboim <a href="#ref-Nicenboim2018StanCon">2018</a> for a cognitive model that incorporates accuracy and reaction times in the same model to analyze the same Stroop data)</span>.</p>
<div id="a-correlated-varying-intercept-varying-slopes-log-normal-model" class="section level3">
<h3><span class="header-section-number">5.2.1</span> A correlated varying intercept varying slopes log-normal model</h3>
<p>If our theory only focuses on the difference between the reaction times for the âcongruentâ vs. âincongruentâ condition, we can ignore the actual color presented and what was written and focus in whether there was a match or not between the two. We will need a predictor that indicates whether each trial is congruent or incongruent (<code>c_cond</code>). For simplicity, we will also assume that all participants share the same variance (as we saw in section <a href="sec-N400hierarchical.html#sec:distrmodel">5.1.6</a> changing this assumption leads to distributional regression models). This means that we are going to fit the data with the following likelihood (identical to the likelihood that we fit in section <a href="sec-N400hierarchical.html#sec:mcvivs">5.1.4</a> except that here the location and scale are embedded in a log-normal likelihood rather than a normal one). This likelihood indicates that we are dealing with a hierarchical model with by-subjects varying intercept and varying slopes model:</p>
<p><span class="math display">\[\begin{equation}
  rt_n \sim LogNormal(\alpha + u_{1,subj[n]}  + c\_cond_n \cdot  (\beta + u_{2,subj[n]}), \sigma)
\end{equation}\]</span></p>
<p>We will discuss how to deal with the coding of conditions, such as <code>c_cond</code>, with more details in chapter <a href="ch-contr.html#ch:contr">6</a> <span class="citation">(and see also D. J. Schad et al. <a href="#ref-schadHowCapitalizePriori2020">2020</a><a href="#ref-schadHowCapitalizePriori2020">b</a> for the mathematics underlying different kinds of contrasts)</span>, but for now it will suffice to say that we assign a <code>1</code> to <code>c_cond</code> for the âincongruentâ condition and a <code>-1</code> for the âcongruentâ one (i.e., a sum coding contrast). This will mean that if <span class="math inline">\(\beta\)</span> turns out to be positive, the incongruent condition will be slower than the congruent one. This is because on average the location of the log-normal likelihood for each condition will be as follows. (We could have chosen to do the opposite assignments, and get to the opposite conclusion without any change in the underlying model).</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  \mu_{incongruent} &amp;= \alpha + 1 \cdot  \beta \\
  \mu_{congruent} &amp;= \alpha + -1 \cdot  \beta
  \end{aligned}
\end{equation}\]</span></p>
<p>As always, we need priors for all the parameters in our model. For the population-level parameters (or fixed effects), we use the same priors as we did when we were fitting a regression with a log-normal likelihood in section <a href="sec-ppd.html#sec:lognormal">3.5.3</a>.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
   \alpha &amp; \sim Normal(6, 1.5) \\
   \beta  &amp; \sim Normal(0, .01) \\
    \sigma  &amp;\sim Normal_+(0, 1)
 \end{aligned}
 \end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\beta\)</span> represents the effect of the experimental manipulation in log-scale: how much we increase or decrease the location of the log-normal in comparison to the intercept, <span class="math inline">\(\alpha\)</span>. For this model, <span class="math inline">\(\beta\)</span> will probably be longer than for the model that examined the difference in pressing the spacebar for two consecutive trials in section <a href="sec-ppd.html#sec:lognormal">3.5.3</a>. We might need to examine the prior for <span class="math inline">\(\beta\)</span> with predictive distributions, but we will delay this for now.</p>
<p>In contrast to our previous models, the intercept, <span class="math inline">\(\alpha\)</span>, is not the grand mean of the location because the conditions were not balanced in the experiment (one third of the conditions were congruent and two thirds incongruent). The intercept could be interpreted here as the time (in log-scale) it takes to answer if we cancel out the experimental manipulation.</p>
<p>We focus now on the priors for the group-level parameters (or random effects). If we assume a possible correlation between by-participant intercept and slope, our model will have the following structure, which requires us to assign priors to <span class="math inline">\(\Sigma_u\)</span>.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
    {\begin{pmatrix}
    u_{i,1} \\
    u_{i,2}
    \end{pmatrix}}
   &amp;\sim {\mathcal {N}}
    \left(
   {\begin{pmatrix} 
    0\\
    0
   \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) 
 \end{aligned}
 \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
 \boldsymbol{\Sigma_u} &amp; = 
{\begin{pmatrix} 
\tau_{u_1}^2 &amp; \rho_u \tau_{u_1} \tau_{u_2} \\ 
\rho_u \tau_{u_1} \tau_{u_2} &amp; \tau_{u_2}^2
\end{pmatrix}}
\end{aligned}
\end{equation}\]</span></p>
<p>In practice this means that we need priors for the by-participant variances and correlations. For the variance components (which, confusingly enough, are actually standard deviations in our prior specification), we will set a similar prior than for <span class="math inline">\(\sigma\)</span>. We donât expect the by-group adjustments to the intercept and slope to have more variance than the overall observations, so this prior will be quite conservative (keeping a big deal of uncertainty). We assign the same prior for the correlations as we did in <a href="sec-N400hierarchical.html#sec:sih">5.1.5</a>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_1} &amp;\sim Normal_+(0,1)\\
\tau_{u_2} &amp;\sim Normal_+(0,1)\\
\rho_u &amp;\sim LKJcorr(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>We are ready to fit a model now. To speed up computation, we subset 50 participants of the original dataset. (In a real setting, we would obviously not subset the participants.)
We restrict ourselves to the correct trials only and we add a <code>c_cond</code> predictor with the sum coded variable.</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;df_stroop&quot;</span>)</a>
<a class="sourceLine" id="cb248-2" data-line-number="2">(df_stroop &lt;-<span class="st"> </span>df_stroop <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb248-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;Incongruent&quot;</span>, <span class="dv">1</span>, <span class="dv">-1</span>)))</a></code></pre></div>
<pre><code>## # A tibble: 3,058 x 5
##    subj trial condition      RT c_cond
##   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;
## 1     1     0 Congruent    1484     -1
## 2     1     1 Incongruent  1316      1
## 3     1     2 Incongruent   628      1
## 4     1     3 Congruent     511     -1
## 5     1     4 Congruent     509     -1
## # â¦ with 3,053 more rows</code></pre>
<p>We fit the model with 4000 iterations rather than with the default of 2000 iterations by chain. This is because if we run the model with the default number of iterations, the following warning that appears: <code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and media Running the chains for more iterations may help. See http://mc-stan.org/misc/warnings.html#bulk-ess</code>.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" data-line-number="1">fit_stroop &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb250-2" data-line-number="2">                   <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb250-3" data-line-number="3">                  <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb250-4" data-line-number="4">                      <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb250-5" data-line-number="5">                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">.01</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb250-6" data-line-number="6">                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb250-7" data-line-number="7">                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb250-8" data-line-number="8">                        <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</a>
<a class="sourceLine" id="cb250-9" data-line-number="9">                  <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb250-10" data-line-number="10">                  <span class="dt">data =</span> df_stroop)</a></code></pre></div>
<p>We will focus on <span class="math inline">\(\beta\)</span> (but you can verify that there is nothing surprising in <code>fit_stroop</code> ):</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb251-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_stroop, <span class="dt">pars =</span> <span class="st">&quot;b_c_cond&quot;</span>)</a></code></pre></div>
<pre><code>##          Estimate Est.Error   Q2.5  Q97.5
## b_c_cond   0.0268   0.00547 0.0159 0.0373</code></pre>
<p>After seeing the posterior for <span class="math inline">\(\beta\)</span>, we suspect that the prior might have been too restrictive. If we overlay density plots for prior and posterior distributions this is more evident:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb253-1" data-line-number="1">sample_b_post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit_stroop)<span class="op">$</span>b_c_cond</a>
<a class="sourceLine" id="cb253-2" data-line-number="2"><span class="co"># We generate samples from the prior as well:</span></a>
<a class="sourceLine" id="cb253-3" data-line-number="3">N &lt;-<span class="st"> </span><span class="kw">length</span>(sample_b_post)</a>
<a class="sourceLine" id="cb253-4" data-line-number="4">sample_b_prior &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">.01</span>)</a>
<a class="sourceLine" id="cb253-5" data-line-number="5">samples &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">sample =</span> <span class="kw">c</span>(sample_b_post, sample_b_prior),</a>
<a class="sourceLine" id="cb253-6" data-line-number="6">                  <span class="dt">distribution =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;posterior&quot;</span>,N), <span class="kw">rep</span>(<span class="st">&quot;prior&quot;</span>, N)))</a>
<a class="sourceLine" id="cb253-7" data-line-number="7"><span class="kw">ggplot</span>(samples, <span class="kw">aes</span>(<span class="dt">x =</span> sample, <span class="dt">fill =</span> distribution)) <span class="op">+</span></a>
<a class="sourceLine" id="cb253-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">.5</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-162-1.svg" width="672" /></p>
<div id="a-sensitivity-analysis" class="section level4">
<h4><span class="header-section-number">5.2.1.1</span> A sensitivity analysis</h4>
<p>We see that the posterior distribution covers values much larger than the ones that are in the bulk of the prior distribution. Is our posterior overly biased by the prior distribution? We can investigate this with a sensitivity analysis. We will examine what happens for the following priors for <span class="math inline">\(\beta\)</span>:</p>
<ul>
<li><span class="math inline">\(\beta \sim Normal(0,.05)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0,.1)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0,1)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0,2)\)</span></li>
</ul>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb254-1" data-line-number="1">sds &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>,<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb254-2" data-line-number="2">df_beta_stroop &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(sds, <span class="cf">function</span>(sd){</a>
<a class="sourceLine" id="cb254-3" data-line-number="3">  priorb &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;normal(0, &quot;</span>,sd ,<span class="st">&quot;)&quot;</span>)</a>
<a class="sourceLine" id="cb254-4" data-line-number="4">  fit &lt;-<span class="st">  </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb254-5" data-line-number="5">              <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb254-6" data-line-number="6">              <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb254-7" data-line-number="7">                <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb254-8" data-line-number="8">                  <span class="kw">prior_string</span>(priorb, <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb254-9" data-line-number="9">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb254-10" data-line-number="10">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb254-11" data-line-number="11">                  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</a>
<a class="sourceLine" id="cb254-12" data-line-number="12">              <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb254-13" data-line-number="13">              <span class="dt">data =</span> df_stroop)</a>
<a class="sourceLine" id="cb254-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb254-15" data-line-number="15">  <span class="kw">posterior_summary</span>(fit, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b_c_cond&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb254-16" data-line-number="16"><span class="st">    </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb254-17" data-line-number="17"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">prior =</span> priorb)</a>
<a class="sourceLine" id="cb254-18" data-line-number="18">})</a></code></pre></div>
<p>We can summarize the estimates of <span class="math inline">\(\beta\)</span> given different priors in the following way:</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Q2.5
</th>
<th style="text-align:right;">
Q97.5
</th>
<th style="text-align:left;">
prior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
-0.001
</td>
<td style="text-align:right;">
0.003
</td>
<td style="text-align:left;">
normal(0, 0.001)
</td>
</tr>
<tr>
<td style="text-align:right;">
0.027
</td>
<td style="text-align:right;">
0.016
</td>
<td style="text-align:right;">
0.037
</td>
<td style="text-align:left;">
normal(0, 0.01)
</td>
</tr>
<tr>
<td style="text-align:right;">
0.037
</td>
<td style="text-align:right;">
0.025
</td>
<td style="text-align:right;">
0.049
</td>
<td style="text-align:left;">
normal(0, 0.05)
</td>
</tr>
<tr>
<td style="text-align:right;">
0.037
</td>
<td style="text-align:right;">
0.025
</td>
<td style="text-align:right;">
0.049
</td>
<td style="text-align:left;">
normal(0, 0.1)
</td>
</tr>
<tr>
<td style="text-align:right;">
0.037
</td>
<td style="text-align:right;">
0.025
</td>
<td style="text-align:right;">
0.050
</td>
<td style="text-align:left;">
normal(0, 1)
</td>
</tr>
<tr>
<td style="text-align:right;">
0.038
</td>
<td style="text-align:right;">
0.025
</td>
<td style="text-align:right;">
0.050
</td>
<td style="text-align:left;">
normal(0, 2)
</td>
</tr>
</tbody>
</table>
<p>It might be easier to see how much the posterior difference between conditions changes depending on the prior. In order to answer this question, we need to remember that the median difference between conditions can be calculated as the difference between the exponentiation of each condition:</p>
<p><span class="math display" id="eq:medianrt">\[\begin{equation}
\begin{aligned}
MedianRT_{diff} &amp;= MedianRT_{incongruent} - MedianRT_{congruent}\\
MedianRT_{diff} &amp;= exp(\alpha + \beta) - \exp(\alpha - \beta)
\end{aligned}
\tag{5.2}
\end{equation}\]</span></p>
<p>This means that we need to re-run the models to extract samples from the intercept.</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" data-line-number="1">sds &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>,<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb255-2" data-line-number="2">df_diffs_stroop &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(sds, <span class="cf">function</span>(sd){</a>
<a class="sourceLine" id="cb255-3" data-line-number="3">  priorb &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;normal(0, &quot;</span>,sd ,<span class="st">&quot;)&quot;</span>)</a>
<a class="sourceLine" id="cb255-4" data-line-number="4">  fit &lt;-<span class="st">  </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subj),</a>
<a class="sourceLine" id="cb255-5" data-line-number="5">              <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb255-6" data-line-number="6">              <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb255-7" data-line-number="7">                <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb255-8" data-line-number="8">                  <span class="kw">prior_string</span>(priorb, <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb255-9" data-line-number="9">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb255-10" data-line-number="10">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb255-11" data-line-number="11">                  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</a>
<a class="sourceLine" id="cb255-12" data-line-number="12">              <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb255-13" data-line-number="13">              <span class="dt">data =</span> df_stroop)</a>
<a class="sourceLine" id="cb255-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb255-15" data-line-number="15">  sample_a_post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit)<span class="op">$</span>b_Intercept</a>
<a class="sourceLine" id="cb255-16" data-line-number="16">  sample_b_post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit)<span class="op">$</span>b_c_cond</a>
<a class="sourceLine" id="cb255-17" data-line-number="17">  RT_diff =<span class="st"> </span><span class="kw">exp</span>(sample_a_post <span class="op">+</span><span class="st"> </span>sample_b_post) <span class="op">-</span></a>
<a class="sourceLine" id="cb255-18" data-line-number="18"><span class="st">    </span><span class="kw">exp</span>(sample_a_post <span class="op">-</span><span class="st"> </span>sample_b_post)</a>
<a class="sourceLine" id="cb255-19" data-line-number="19">  <span class="kw">tibble</span>(<span class="st">`</span><span class="dt">mean diff (ms)</span><span class="st">`</span> =<span class="st"> </span><span class="kw">mean</span>(RT_diff),</a>
<a class="sourceLine" id="cb255-20" data-line-number="20">         <span class="dt">Q2.5 =</span> <span class="kw">quantile</span>(RT_diff, <span class="fl">.025</span>),</a>
<a class="sourceLine" id="cb255-21" data-line-number="21">         <span class="dt">Q97.5 =</span> <span class="kw">quantile</span>(RT_diff, <span class="fl">.975</span>),</a>
<a class="sourceLine" id="cb255-22" data-line-number="22">         <span class="dt">prior =</span> priorb)</a>
<a class="sourceLine" id="cb255-23" data-line-number="23">})</a></code></pre></div>
<p>We get the <em>posterior distributions</em> of the median difference between conditions for different models by using equation <a href="sec-stroop.html#eq:medianrt">(5.2)</a>. We calculate the median difference rather than the mean difference because the mean depends on the parameter <span class="math inline">\(\sigma\)</span>, but the median doesnât; see <a href="sec-ppd.html#sec:lognormal">3.5.3</a>. In the following table, we use <em>means</em>, and 95% quantiles to summarize these posterior distributions. Itâs important to realize that the use of mean to summarize the posterior distribution is orthogonal to our use of median to summarize the reaction times by condition: In the first case, we use the median to summarize a group of <em>observations</em>, and in the second case, we use the mean to summarize a group of <em>samples</em> from the posteriorâwe could have summarized the samples from the posterior with its median as well.</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
mean diff (ms)
</th>
<th style="text-align:right;">
Q2.5
</th>
<th style="text-align:right;">
Q97.5
</th>
<th style="text-align:left;">
prior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.682
</td>
<td style="text-align:right;">
-1.47
</td>
<td style="text-align:right;">
2.88
</td>
<td style="text-align:left;">
normal(0, 0.001)
</td>
</tr>
<tr>
<td style="text-align:right;">
30.220
</td>
<td style="text-align:right;">
17.86
</td>
<td style="text-align:right;">
42.35
</td>
<td style="text-align:left;">
normal(0, 0.01)
</td>
</tr>
<tr>
<td style="text-align:right;">
41.484
</td>
<td style="text-align:right;">
28.20
</td>
<td style="text-align:right;">
54.97
</td>
<td style="text-align:left;">
normal(0, 0.05)
</td>
</tr>
<tr>
<td style="text-align:right;">
41.944
</td>
<td style="text-align:right;">
27.96
</td>
<td style="text-align:right;">
56.16
</td>
<td style="text-align:left;">
normal(0, 0.1)
</td>
</tr>
<tr>
<td style="text-align:right;">
41.958
</td>
<td style="text-align:right;">
28.18
</td>
<td style="text-align:right;">
55.96
</td>
<td style="text-align:left;">
normal(0, 1)
</td>
</tr>
<tr>
<td style="text-align:right;">
42.159
</td>
<td style="text-align:right;">
28.58
</td>
<td style="text-align:right;">
56.00
</td>
<td style="text-align:left;">
normal(0, 2)
</td>
</tr>
</tbody>
</table>
<p>This shows us that the posterior changes substantially when we include wider priors in our model. It seems that the posterior is relatively unaffected for priors with a standard deviation larger than .05, but if we assume a priori that the effect of the manipulation <em>must</em> be small, we will end up finding that. When we include less information about the possible effect sizes with a more diffuse priorâwe assume that they can be small but also largeâ, we allow the data to influence more the posterior. (We can safely ignore a difference of a couple milliseconds in <span class="math inline">\(\approx 40\)</span> milliseconds.)</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ManyLabs3">
<p>Ebersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. âMany Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.â <em>Journal of Experimental Social Psychology</em> 67: 68â82. <a href="https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012" class="uri">https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012</a>.</p>
</div>
<div id="ref-macleod1991half">
<p>MacLeod, Colin M. 1991. âHalf a Century of Research on the Stroop Effect: An Integrative Review.â <em>Psychological Bulletin</em> 109 (2). American Psychological Association: 163.</p>
</div>
<div id="ref-Nicenboim2018StanCon">
<p>Nicenboim, Bruno. 2018. âThe Implementation of a Model of Choice: The (Truncated) Linear Ballistic Accumulator.â In <em>StanCon</em>. Aalto University, Helsinki, Finland. <a href="https://doi.org/10.5281/zenodo.1465990" class="uri">https://doi.org/10.5281/zenodo.1465990</a>.</p>
</div>
<div id="ref-schadHowCapitalizePriori2020">
<p>Schad, Daniel J., Shravan Vasishth, Sven Hohenstein, and Reinhold Kliegl. 2019. âHow to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.â <em>Journal of Memory and Language</em> 110. <a href="https://doi.org/10.1016/j.jml.2019.104038" class="uri">https://doi.org/10.1016/j.jml.2019.104038</a>.</p> 2020a. âHow to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.â <em>Journal of Memory and Language</em> 110. Elsevier: 104038.</p> 2020b. âHow to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.â <em>Journal of Memory and Language</em> 110 (February): 104038. <a href="https://doi.org/10/gf9tjp" class="uri">https://doi.org/10/gf9tjp</a>.</p>
</div>
<div id="ref-stroop1935studies">
<p>Stroop, J Ridley. 1935. âStudies of Interference in Serial Verbal Reactions.â <em>Journal of Experimental Psychology</em> 18 (6). Psychological Review Company: 643.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-N400hierarchical.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/05-hierarchical.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
